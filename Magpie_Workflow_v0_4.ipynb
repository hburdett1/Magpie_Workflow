{"cells":[{"cell_type":"markdown","metadata":{"id":"XRLIfXS0JCmR"},"source":["# **Magpie Workflow v0.4**\n","\n","Magpie, an open-source Python and R model configuration workflow developed in Google Colab, to assist with geospatial data preparation and model construction for Raven.<br>\n","\n","Users can upload their own data or use the workflows data collection section to access open-source data available for North America, such as topographic, land use, soil, and climatic forcing data. The user-uploaded or workflow-acquired data is then formatted and entered into a series of tools, such as BasinMaker, to discretize the basin into subbasins and hydrologic response units and the RavenR software library, to generate the necessary configuration inputs for a Raven model. The Magpie workflow also runs the Raven executable and generates visualizations of the model outputs. <br>\n","\n","To goal of Magpie is to provide a user-friendly experience with transparent and reproducible scientific outputs. The workflow significantly reduces model configuration time for experienced modellers and allows for open modification and customization, while also providing a welcoming platform for those new to hydrologic modelling.<br>\n","\n","For more information, please review the <font color=#5559AB>\"Overview\"</font> subsection which provides a general overview of each subsection.\n","\n","\n","New to Google Colab? Check out the following [Overview of Google Colab Features video](https://youtu.be/rpr0PAd_0Dc) that demonstrates some of the Google Colab features used in the Magpie Workflow.\n","\n","In the case this is your first time using Magpie, check out the [Overview and Python Library Installation video](https://youtu.be/umPQJJstr8A) to see how to finish setting up the Magpie Workflow\n","\n","If you run into an error, restart your workflow (“Runtime” -> “Restart\n","Runtime”). After the workflow has been restarted, be sure to run “Workflow\n","Set up (Mandatory)” -> “Run to Set up Workflow”.\n","\n","If you run into any issues, please email: hburdett@uwaterloo.ca"]},{"cell_type":"markdown","metadata":{"id":"s7T7RQ9KMFmy"},"source":["## <font color=#5559AB> Overview </font>\n"]},{"cell_type":"markdown","metadata":{"id":"vC3vND72tm89"},"source":["#### <font color=grey>Python Library Installation</font>"]},{"cell_type":"markdown","metadata":{"id":"tlFGKLBEtqdH"},"source":["\n","\n","> The \"Python Libraries Installation\" section of the Magpie Workflow guide streamlines the installation process by saving specific versions of Python libraries directly to the user's Google Drive. This one-time setup significantly reduces future installation times and guarantees consistent library versions, enhancing reproducibility. Users are advised to execute this subsection only **once** during their initial use of the Magpie Workflow. While this step is optional and time-consuming, it is highly recommended for frequent users. Undertaking this longer initial installation eliminates the need for repetitive library installations with each notebook reconnection, saving considerable time in the long run.\n"]},{"cell_type":"markdown","metadata":{"id":"hcJK2E-dbkbC"},"source":["#### <font color=grey>R Library Installation</font>"]},{"cell_type":"markdown","metadata":{"id":"svnfqmFJbkbD"},"source":["\n","\n","> The \"R Libraries Installation\" section is designed to optimize the setup process in the Magpie Workflow by installing specific versions of R libraries onto the user's personal Google Drive. This strategy is aimed at minimizing installation times for subsequent uses of the workflow and ensuring consistent results through reproducibility. The installed R libraries are conveniently accessible directly from the user's drive. It's important to note that this procedure is required only **once**, specifically during the first initiation of the Magpie Workflow. Although this step is optional and initially time-consuming, it is strongly recommended for those who anticipate regular use of the workflow. Completing this one-time, extensive installation process prevents the need for repeated library installations each time the notebook is reconnected, thereby offering significant time savings over repeated uses.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PR63RNeUuiFu"},"source":["#### <font color=grey>Workflow Set up (Mandatory)</font>"]},{"cell_type":"markdown","metadata":{"id":"6KFIIJ2evai9"},"source":["> The \"Workflow Set Up\" is an essential and mandatory section that needs to be executed every time the notebook is disconnected or restarted. This crucial subsection links the Google Colab Notebook to the user's Google Drive, enabling seamless data transfer and storage between the notebook and the drive. It also takes care of loading the required libraries and establishing the working directories. It's important to remember that if there are any changes to the workflow's name, corresponding updates must be made to the paths for `folder_name`, `model_name`, and `final_output_folder`. This ensures that the workflow accurately references and accesses the correct directories and data within the user's drive."]},{"cell_type":"markdown","metadata":{"id":"x2fj5T3ZvyKp"},"source":["#### <font color=grey>1.0 Data Collection</font>"]},{"cell_type":"markdown","metadata":{"id":"3tJCLeG1vySA"},"source":[">\n","\n","In section 1.0 of the Magpie, users have complete flexibility in generating inputs for BasinMaker. Each subsection offers two options: uploading personal data or utilizing the Magpie workflow to acquire data from Google Earth Engine. The workflow can provide all necessary data for a model or supplement missing input layers for BasinMaker.\n","\n","For users opting to upload their data, here are the specific accepted formats for each subsection:\n","- *Shapefile_study_area*: Polygon shapefile (.shp) representing the study area.\n","- *DEM*: Digital Elevation Model as a raster file (.tif).\n","- *Elevation_band*: Polygon shapefile (.shp).\n","- *Land_cover*: Either a polygon shapefile (.shp) for direct use or a raster file (.tif)/\n","- *Soil*: Similar to Land Cover, this can be a polygon shapefile (.shp) for direct use or a raster file (.tif).\n","\n","Please note that it is not mandatory to have all these files; any missing files can be generated within the Magpie Workflow.\n","\n","Each section clearly outlines the necessary inputs, data sources, and guides users on saving the data to their drive. It's important to remember that the land cover, vegetation, and soil classifications used in the RVP and RVH Raven models are produced in subsection 1f) Classification. Users have the flexibility to define these classifications by manually inputting their choices. Please ensure to include a comma between values and do not include spaces.\n"]},{"cell_type":"markdown","metadata":{"id":"5Kxs2qQ-2CFV"},"source":["#### <font color=grey>2.0 Discretize Basin</font>"]},{"cell_type":"markdown","metadata":{"id":"xmguYfIcEo6T"},"source":["> BasinMaker, a tool innovatively created by Han et al. (2021), is designed to construct vector-based hydrological lake-river routing networks with remarkable efficiency and automation. It features versatile discretization options, enabling the representation of any number of lakes within a watershed. To facilitate this process, users have the flexibility to either upload their own files or leverage subsection 1.0 Data Collection in the BasinMaker framework to create the necessary input files. Potential inputs for discretization include DEM, elevation bands, aspect, landcover, and soil layers. Please note, users are not required to have all the layers previously mentioned for the discretization process.\n","\n","> Please note within the Magpie wqorkflow, it uses BasinMaker light which pulls from a pre-existing routing products. Users have the option to upload subbasin and/or HRU layers derived from BasinMaker full which is run through a GIS interface such as ArcGIS or QGIS. Magpie can then be used at any step to finialize the process of discretization and generating RVH and RVP files."]},{"cell_type":"markdown","metadata":{"id":"TKy81j-k2COG"},"source":["#### <font color=grey>3.0 Forcing Data</font>"]},{"cell_type":"markdown","metadata":{"id":"IfefpWkx2CRR"},"source":["> Raven supports gridded or station-based forcing inputs exclusively in NetCDF format (*.nc files). This subsection offers users access to five different forcing input options that include downloading and formatting the forcing data into a Raven RVT data:\n","* *3a) CaSPAr Data* - the Canadian Surface Prediction Archive (CaSPAr) is an archive of numerical weather predictions issued by Environment and Climate Change Canada.\n","* *3b) Gridded Weights Generator* - produces a grid weights file that can be used in the hydrologic modelling framework Raven to handle gridded NetCDF inputs and map them to the subbasins/ HRUs a distributed model is based on\n","* *3c) DayMet Data* - Daymet provides long-term, continuous, gridded estimates of daily weather and climatology variables by interpolating and extrapolating ground-based observations through statistical modelling techniques; includes daily, monthly, and annual\n","* *3d) Environment Canada Climate Data* - gauge data - accesses historical climate data, such as temperature, precipitation, degree days, relative humidity, wind speed and direction; includes hourly, daily, and monthly values\n","* *3e) Format Uploaded Observational Data* - Gauge Data - Users can upload their own meteorological or flow data to be formatted into an RVT file. Each subsection demonstrates how the data should be formatted.\n","* *3f) Hydrometric data (HYDAT*) - gauge data - historical water level and flow (discharge) data collected at over 7700 hydrometric stations across Canada; includes daily and monthly flow levels."]},{"cell_type":"markdown","metadata":{"id":"_mO8mnrG2CUc"},"source":["#### <font color=grey>4.0 Raven Input Files</font>"]},{"cell_type":"markdown","metadata":{"id":"F025IY5YJX6T"},"source":[">This section, written in R, focuses on generating the RVI, RVH, and RVC input files for Raven (RVI, RVH, and RVCfiles) utilizing RavenR, developed by Chlumsky et al. (2022). <br>\n","<br>\n","For the RVI, there are several template options available; for more information, please see the Raven Manual. After creating the RVI, the user must open the RVI file (which can be done directly in Google Colab) and modify the simulation details, such as the “:StartDate”, “:Duration”, “:Timestep”.\n","A .rvp_temp.rvp file generated in BasinMaker is required to produce an RVP file. <br>\n","<br>\n","A blank RVC file is produced and can be modified directly in Google Colab to set the initial storage conditions of the simulation. For more information on the RVC file please visit the Raven Manual."]},{"cell_type":"markdown","metadata":{"id":"q5-PqtpEJX9j"},"source":["#### <font color=grey>5.0 Run Raven</font>"]},{"cell_type":"markdown","metadata":{"id":"wgIaIF5GLFzT"},"source":[">The Raven model is ready to be run! Define the model name and ensure that the RVI, RVH, RVC, RVP, and RVT files all have the same name. Additionally, users are required to specify an output folder name. This name is used to store the model's outputs within the “workflow_outputs” folder, organizing the results for easy access and review. For those who prefer to run the Raven model on their local machine or a different server, the workflow includes a convenient feature: `Download Raven Input Files`. When selected, this option compiles all necessary Raven input files and forcing data into a single compressed (zipped) file. This file can then be easily downloaded, offering users the flexibility to run the model in an environment of their choosing."]},{"cell_type":"markdown","metadata":{"id":"YB9lnpYkvyVk"},"source":["#### <font color=grey>6.0 Visualization of Raven Model Outputs</font>\n"]},{"cell_type":"markdown","metadata":{"id":"b9bIgTPcLUB7"},"source":["> This section will be improved at a later date. Currently, it offers an interactive plot feature for visualizing hydrographs generated by the Raven model. The custom outputs from the model, particularly for each Hydrologic Response Unit (HRU), are formatted to be compatible with Pandas, the Python data analysis library. This compatibility allows for the effective visualization of these outputs as line graphs.\n","\n","**RavenView**\n","> RavenView is an online tool for visualizing Raven model output. This subsection generates a zipped file that can be downloaded and then dragged and dropped into RavenView to examine Raven model inputs and outputs more thoroughly."]},{"cell_type":"markdown","metadata":{"id":"y6j09pBE9hZi"},"source":["#### <font color=grey>Run into an Error within the Workflow?</font>"]},{"cell_type":"markdown","metadata":{"id":"2vdO6sz69pV5"},"source":["First try restarting the workflow, \"Runtime\" -> \"Restart Runtime\"\n","\n","The cells are run in real time, so if the issue presists and you are familiar with coding, the script can be adjusted and run again.\n","\n","If an issue presists please feel free to email me at hburdett@uwaterloo.ca with the subject \"Magpie Issue\""]},{"cell_type":"markdown","metadata":{"id":"lTN57xbsSYu7"},"source":["## <font color=#5559AB>Python Library Installation</font>\n","\n","Only needs to be run the **first time** the Magpie Workflow is being set-up\n","\n","Once the cell is done running, restart the workflow (\"Runtime\" --> \"Restart runtime\") or “Ctrl+M”\n","\n","Check out the [Getting Started with Magpie Workflow: Overview and Library Installations](https://youtu.be/umPQJJstr8A) short video for more information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"aU4X4ecBoCIB"},"outputs":[],"source":["#@markdown <font size=\"+2\"><font color=#5559AB> **Run to Set up Python Librairies for Workflow** </font>\n","\n","#@markdown To avoid long installation times each time the Magpie Workflow is utilized, the “Python Library Installation” subsection installs a several Python libraries onto the users personal Google Drive. The Python libraries can then be loaded directly from the users drive;\n","#@markdown be sure to run each of the cells in this section.\n","\n","#@markdown Estimated execuation time: 12min\n","\n","import os, sys\n","from google.colab import drive\n","\n","#mounting google drive allows us to work with its contents\n","drive.mount('/content/google_drive')\n","\n","# define output directory path\n","main_dir = \"/content\"\n","packages_dir = os.path.join(main_dir, 'google_drive', 'MyDrive', 'Packages')\n","packages_path = os.path.isdir(packages_dir)\n","\n","if not packages_path:\n","  os.makedirs(packages_dir)\n","  print(\"created  folder: \", packages_dir)\n","else:\n","  print(packages_dir, \"folder already exists\")\n","\n","# changes path to files\n","nb_path = '/content/notebooks'\n","os.symlink(packages_dir, nb_path)\n","sys.path.insert(0,nb_path)\n","\n","!pip install --target=$nb_path pandas==1.5.3 &> /dev/null\n","!pip install --target=$nb_path netCDF4==1.6.3 &> /dev/null\n","\n","!pip install --target=$nb_path wget &> /dev/null\n","!pip install --target=$nb_path rasterio==1.3.7 &> /dev/null\n","!pip install --target=$nb_path rioxarray==0.14.1 &> /dev/null\n","\n","!pip install --target=$nb_path fiona==1.9.4.post1 &> /dev/null\n","!pip install --target=$nb_path pytest==7.2.2 &> /dev/null\n","!pip install --target=$nb_path ipyleaflet==0.17.2 &> /dev/null\n","\n","!pip install --target=$nb_path scipy==1.10.1 &> /dev/null\n","!pip install --target=$nb_path joblib==1.2.0 &> /dev/null\n","\n","!pip install --target=$nb_path ipywidgets==7.7.1 &> /dev/null\n","!pip install --target=$nb_path shapely==2.0.1 &> /dev/null\n","!pip install --target=$nb_path pyproj==3.5.0 &> /dev/null\n","\n","!pip install --target=$nb_path rtree==1.0.1 &> /dev/null\n","!pip install --target=$nb_path --upgrade --no-cache-dir gdown &> /dev/null\n","!pip install --target=$nb_path git+https://github.com/python-visualization/folium &> /dev/null\n","\n","print('Please restart the workflow (Runtime -> Restart session (Ctrl+M))')"]},{"cell_type":"markdown","metadata":{"id":"CGNwp4hc8GbL"},"source":["## <font color=#5559AB>R Library Installation</font>\n","\n","Only needs to be run the **first time** the Magpie Workflow is being set-up\n","\n","Once the cell is done running, restart the workflow (\"Runtime\" --> \"Restart runtime\")\n","\n","Check out the [Getting Started with Magpie Workflow: Overview and Library Installations](https://youtu.be/umPQJJstr8A) short video for more information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YSt3vLrO1WPa"},"outputs":[],"source":["#@markdown <font size=\"+2\"><font color=#5559AB> **Run to Set up R Librairies for Workflow** </font>\n","\n","#@markdown Connect to google drive in order to store libraries\n","\n","%load_ext rpy2.ipython\n","\n","from google.colab import drive\n","drive.mount('/content/google_drive')\n","\n","import os\n","\n","# define output directory path\n","main_dir = \"/content\"\n","packages_dir = os.path.join(main_dir, 'google_drive', 'MyDrive', 'R_Packages')\n","packages_path = os.path.isdir(packages_dir)\n","\n","if not packages_path:\n","  os.makedirs(packages_dir)\n","  print(\"created  folder: \", packages_dir)\n","else:\n","  print(packages_dir, \"folder already exists\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"6GdN7oJK-BP3"},"outputs":[],"source":["#@markdown <font color=grey> **RavenR Library Installation** </font>\n","\n","#@markdown To avoid long installation times each time the Magpie Workflow is utilized, the “RavenR Library Installation” subsection installs a several R libraries onto the users personal Google Drive. The R libraries can then be loaded directly from the users drive;\n","#@markdown be sure to run each of the cells in this section.\n","\n","#@markdown Estimated execuation time: 12.5min\n","\n","%%R\n","\n","# Start measuring time\n","start_time <- Sys.time()\n","\n","# Install igraph package with a specific version\n","devtools::install_version(\"igraph\", version = \"1.2.6\", repos = \"https://cloud.r-project.org/\", lib = \"/content/google_drive/MyDrive/R_Packages\")\n","\n","# Install RavenR package\n","install.packages(\"RavenR\", lib = \"/content/google_drive/MyDrive/R_Packages\")\n","\n","# End measuring time\n","end_time <- Sys.time()\n","\n","# Calculate the elapsed time in minutes\n","elapsed_time_minutes <- as.numeric(difftime(end_time, start_time, units = \"mins\"))\n","\n","cat(paste(\"Installation time:\", elapsed_time_minutes, \"minutes\\n\"))\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EXZ1pnwCSSY8"},"outputs":[],"source":["#@markdown <font color=grey> **R Library Installation** </font>\n","\n","#@markdown Only need to install if using section **Generate DEM - R elevatr Package**\n","\n","#@markdown Estimated execuation time: 40min\n","\n","%%R\n","\n","# Start measuring time\n","start_time <- Sys.time()\n","\n","# Define the library path and packages to check\n","library_path <- \"/content/google_drive/MyDrive/R_Packages\"\n","packages <- c(\"terra\", \"sf\", \"slippymath\", \"raster\", \"s2\", \"elevatr\")\n","\n","# Create the library path if it doesn't exist\n","if (!dir.exists(library_path)) {\n","  dir.create(library_path, recursive = TRUE)\n","}\n","\n","# Function to check and install missing packages\n","check_and_install <- function(package_name) {\n","  if (!require(package_name, character.only = TRUE, lib.loc = library_path)) {\n","    install.packages(package_name, lib = library_path, repos = \"http://cran.r-project.org\")\n","    library(package_name, lib.loc = library_path, character.only = TRUE)\n","  }\n","}\n","\n","# Check and install each package\n","for (pkg in packages) {\n","  check_and_install(pkg)\n","}\n","# End measuring time\n","end_time <- Sys.time()\n","\n","# Calculate the elapsed time in minutes\n","elapsed_time_minutes <- as.numeric(difftime(end_time, start_time, units = \"mins\"))\n","\n","cat(paste(\"Installation time:\", elapsed_time_minutes, \"minutes\\n\"))\n"]},{"cell_type":"markdown","metadata":{"id":"jaKMH_Nvb5_a"},"source":["# **Workflow Set up (Mandatory)**"]},{"cell_type":"markdown","metadata":{"id":"-uZugzVVcGUC"},"source":["The \"Workflow Set up\" section is mandatory as it mounts the Magpie workflow to the user's Google Drive. This allows for data to be transferred between the user's Google Drive and workflow so that when the notebook becomes disconnected, the data is not lost. Additionally, this section loads required packages and define working and output directories.\n","\n","For more information visit the [Mandatory Workflow Set up video](https://youtu.be/krakrlHOpkU)\n","\n","**MUST BE RUN EACH TIME THE WORKFLOW BECOMES DISCONNECTED OR IS RESTARTED**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ApreacTJx5Jb"},"outputs":[],"source":["import sys\n","import warnings\n","import importlib\n","\n","#@markdown <font size=\"+3\"><font color=#5559AB> **Run to Set up Workflow** </font> <br>\n","\n","# connect to google drive\n","from google.colab import drive\n","drive.mount('/content/google_drive')\n","\n","sys.path.append('/content/google_drive/MyDrive/Packages')\n","warnings.filterwarnings('ignore')\n","\n","# function to be used throughout workflow to check is packages are installed\n","def check_and_install_libraries(library_list):\n","    for library_name in library_list:\n","        try:\n","            importlib.import_module(library_name)\n","            print(f\"{library_name} is already installed.\")\n","        except ImportError:\n","            print(f\"{library_name} is not installed. Installing...\")\n","            try:\n","                import subprocess\n","                subprocess.check_call([\"pip\", \"install\", library_name])\n","                print(f\"{library_name} has been successfully installed.\")\n","            except Exception as e:\n","                print(f\"Failed to install {library_name}. Error: {str(e)}\")\n","\n","# check libraries\n","libraries_to_check = [\"geopandas==0.13.3\",\"rasterio==1.3.7\",\"rasterstats==0.19.0\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import pandas as pd\n","import numpy as np\n","import os\n","import wget\n","import geopandas as gpd\n","import rioxarray as rxr\n","import xarray as xr\n","import rasterio\n","from glob import glob\n","import json\n","import shutil\n","import sys\n","import subprocess\n","import matplotlib.pyplot as plt\n","\n","#@markdown **Define name of workflow folder:**\n","folder_name = \"Magpie_Workflow\" #@param [\"Magpie_Workflow\"] {allow-input: true}\n","#@markdown the name of the folder where Magpie is saved\n","\n","#@markdown ****\n","\n","#@markdown **Define model name:**\n","model_name = \"petawawa\" #@param [\"Magpie_Workflow\"] {allow-input: true}\n","#@markdown the model name will be use to name the RVI, RVP, RVH, RVT, RVC files to keep naming consistent\n","\n","#@markdown ****\n","\n","#@markdown **Define output  file name:**\n","final_output_folder = \"outputA\" #@param [\"Magpie_Workflow\"] {allow-input: true}\n","#@markdown define the folder name for Raven model outputs to be saved\n","\n","# define main output directory\n","main_dir = os.path.join('/content', 'google_drive', 'MyDrive', folder_name)\n","\n","print(\"\\n-----------------------------------------------------\")\n","print(f\"Main Directory: {main_dir}\")\n","print(\"-----------------------------------------------------\")\n","\n","# temporary directory\n","temporary_dir = os.path.join('/content',\"temporary_data\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"8XdPqkiDHdI-"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file:\n","  data = {\n","      \"_comment\": \"------------SET UP MAIN DIRECTORY---------------\",\n","      \"main_dir\": f\"{main_dir}\",\n","      \"temporary_dir\": f\"{temporary_dir}\",\n","      \"model_name\": f\"{model_name}\",\n","      \"output_folder_name\": f\"{final_output_folder}\",\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path, \"0_main_setup.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"-IB0C9cpaoGC"},"source":["# **1.0 Data Collection**\n","\n","This section allows users to collect and format data needed to run the HRU discretization tool in BasinMaker.\n","\n","Each section is optional, so users can pick and choose which sections best suit their needs.\n","\n","Users have the option to upload to generate the data within each section"]},{"cell_type":"markdown","metadata":{"id":"k8-TLJfja_9j"},"source":["## <font color=#5559AB> 1a) Study Area </font>\n","\n","This section utilizes BasinMaker, developed by Han et al. (2021), to extract/format a shapefile of your study area to be used throughout the Magpie Workflow. The shapefile can be generated by either the gauge name or latitude and longitude. Don't have that information? No problem, the first section of the workbook helps to identify the coordinates and gauge name.\n"]},{"cell_type":"markdown","metadata":{"id":"cTrn3D4maSQB"},"source":["### <font color=grey> **Upload Study Area** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"3N6csURC0J5z"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"folium\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import folium\n","from IPython.display import display\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def format_shapefile(shp_file_path, main_dir):\n","    \"\"\"\n","    Formats a shapefile by dissolving its boundaries and saving the result as 'studyArea_outline.shp'.\n","\n","    Parameters:\n","    - shp_file_path (str): The path to the folder containing the shapefile.\n","    - main_dir (str): The main directory where the formatted shapefile will be saved.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('Format Shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the shapefile in the given path\n","    for shp_file in os.listdir(shp_file_path):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Read the shapefile into a GeoDataFrame\n","    studyArea_bound = gpd.read_file(os.path.join(shp_file_path, shp_file_name))\n","\n","    # Add a temporary column 'dissolve' with a constant value for dissolving boundaries\n","    studyArea_bound[\"dissolve\"] = 1\n","\n","    # Extract relevant columns for boundary dissolve\n","    boundary = studyArea_bound[['dissolve', 'geometry']]\n","\n","    # Dissolve boundaries based on the 'dissolve' column\n","    cont_studyArea = boundary.dissolve(by='dissolve')\n","\n","    # Remove all contents in the folder containing shapefiles\n","    for f in glob(os.path.join(shp_file_path, '*')):\n","        os.remove(f)\n","\n","    # Save the dissolved boundary GeoDataFrame to a new shapefile\n","    cont_studyArea.to_file(os.path.join(main_dir, 'shapefile', 'studyArea_outline.shp'))\n","\n","def visualize_shapefile(shp_file_path):\n","    \"\"\"\n","    Visualizes a shapefile on a Folium map and checks/reprojects the coordinate system if needed.\n","\n","    Parameters:\n","    - shp_file_path (str): The path to the shapefile.\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('Visualize Shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Read the shapefile into a GeoDataFrame\n","    shp_boundary = gpd.read_file(os.path.join(shp_file_path))\n","\n","    # Check the projection of the shapefile\n","    if shp_boundary.crs != 'EPSG:4326':\n","        # Reproject the shapefile to EPSG:4326 if the coordinate systems don't match\n","        shp_lyr_crs = shp_boundary.to_crs(epsg=4326)\n","        print('Shapefile layer has been reprojected to match EPSG:4326.')\n","    else:\n","        shp_lyr_crs = shp_boundary\n","        print('Coordinate systems match!')\n","\n","    # Determine the bounds of the provided shapefile\n","    bounds = shp_lyr_crs.bounds\n","    west, south, east, north = bounds.loc[0]\n","    shp_bounds = [south, west]\n","\n","    # Create a Folium map centered on the shapefile bounds\n","    map = folium.Map(location=shp_bounds, zoom_start=10)\n","\n","    # Add the shapefile geometry to the Folium map\n","    folium.GeoJson(data=shp_boundary[\"geometry\"]).add_to(map)\n","\n","    # Display the Folium map\n","    display(map)\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('Shapefile visualization complete!')\n","    print('-----------------------------------------------------------------------------------------------')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"N_ehMAegaKne"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Upload Study Area Shapefile** </font> <br>\n","\n","#@markdown Here users can upload a shapefile (.shp) of their study area\n","\n","# generate drive folder\n","shp_dir = os.path.join(main_dir, 'shapefile')\n","shp_path = os.path.isdir(shp_dir)\n","if not shp_path:\n","  os.makedirs(shp_dir)\n","  print(\"created  folder: \", shp_dir)\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('Upload Shapefile')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop shapefile into the following folder: {shp_dir}')\n","input_response = input(\"Have you uploaded the study area (.shp) file (yes or no): \")\n","if input_response == 'yes':\n","  shp_file_path = os.path.join(main_dir, 'shapefile')\n","  # format shapefile\n","  format_shapefile(shp_file_path, main_dir)\n","  # visualize final shapefile\n","  visualize_shapefile(shp_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gYfts9AzFefc"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------1a) STUDY AREA---------------\",\n","    \"generate_shapefile\": \"no\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"1a_studyArea.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=1)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"LsWa49EQdWEI"},"source":["### <font color=grey> **Generate Study Area** </font>\n","\n","Check out the short video [Generating Study Area Shapefile in Magpie Workflow](https://youtu.be/AfKEiR6Ms6Y) for more information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"F6eP8Lyf7GPO"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"geopy\", \"folium\",\"simpledbf\",\"branca\",\"time\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","#!python -m pip install https://github.com/dustming/basinmaker/archive/fix_head_water_riv_segment.zip &> /dev/null\n","!python -m pip install https://github.com/dustming/basinmaker/archive/master.zip\n","\n","import time\n","from geopy.geocoders import Nominatim\n","import folium\n","import branca\n","import simpledbf\n","from IPython.display import display\n","from basinmaker import basinmaker\n","from shapely.geometry import box, Point\n","from basinmaker.postprocessing.plotleaflet import plot_routing_product_with_ipyleaflet\n","from basinmaker.postprocessing.downloadpd import Download_Routing_Product_For_One_Gauge\n","from basinmaker.postprocessing.downloadpdptspurepy import Download_Routing_Product_From_Points_Or_LatLon\n","from basinmaker.postprocessing.downloadpdptspurepy import Extract_Routing_Product\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","\n","def studyArea_location(city_name, define_lat, define_lon):\n","    \"\"\"\n","    Collects study area location information using either city name or defined coordinates.\n","\n","    Parameters:\n","    - city_name (str): The name of the city to determine coordinates.\n","    - define_lat (str): Defined latitude (if available).\n","    - define_lon (str): Defined longitude (if available).\n","\n","    Returns:\n","    Tuple of latitude and longitude.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Collect study area location information')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Determine coordinates based on city name using geopy.geocoders library\n","    if city_name != 'NA' and define_lat == 'NA':\n","        geolocator = Nominatim(user_agent=\"Magpie\")\n","        location = geolocator.geocode(city_name)\n","        print(location)\n","        found_lat, found_lon = location.latitude, location.longitude\n","        lat, lon = found_lat, found_lon\n","        print(\"Study area coordinates:\", lat, lon)\n","        return lat, lon\n","\n","    # Use defined coordinates\n","    elif city_name == 'NA' and define_lat != 'NA':\n","        lat, lon = float(define_lat), float(define_lon)\n","        print(\"Study area coordinates:\", lat, lon)\n","        return lat, lon\n","\n","    # If neither coordinates nor city name are defined, encourage the user to define a gauge name\n","    elif city_name and define_lat == \"NA\":\n","        lat = None\n","        print(\"Define gauge name\")\n","\n","def interactive_gauge_map(lat, lon, main_dir):\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Interactive gauge plot')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Read in CSV with gauge information\n","    gauge_info = pd.read_csv(os.path.join(main_dir, 'extras', 'subbasin_plots', 'obs_gauges_NA_v2-1.csv'))\n","\n","    # Format fancy folium pop-up\n","    def fancy_html(row):\n","        subId_val, obs_gauge, lat_info, lon_info, sub_Reg = (\n","            gauge_info['SubId'].iloc[row],\n","            gauge_info['Obs_NM'].iloc[row],\n","            gauge_info['POINT_Y'].iloc[row],\n","            gauge_info['POINT_X'].iloc[row],\n","            gauge_info['Sub_Region'].iloc[row]\n","        )\n","        html = f\"\"\"<!DOCTYPE html>\n","            <html>\n","            <p>SubID: {subId_val}</p>\n","            <p>Obs Gauge: {obs_gauge}</p>\n","            <p>Lat: {lat_info}</p>\n","            <p>Lon: {lon_info}</p>\n","            <p>Sub Region: {sub_Reg}</p>\n","            </html>\n","            \"\"\"\n","        return html\n","\n","    # Generate map with rectangular guide to assist in identifying the ideal subbasin/gauges to use\n","    if lat is None:\n","        print(\"Define gauge name in cell above\")\n","    else:\n","        grid_pt = (lat, lon)\n","        W, E, N, S = grid_pt[1] - 0.5, grid_pt[1] + 0.5, grid_pt[0] + 0.5, grid_pt[0] - 0.5\n","        upper_left, upper_right, lower_right, lower_left = (N, W), (N, E), (S, E), (S, W)\n","        line_color, fill_color, weight, text = 'red', 'red', 2, 'text'\n","        edges = [upper_left, upper_right, lower_right, lower_left]\n","\n","        map_osm = folium.Map(location=[lat, lon], zoom_start=9)\n","        folium.LatLngPopup().add_to(map_osm)\n","\n","        for i in range(len(gauge_info)):\n","            html = fancy_html(i)\n","            iframe = branca.element.IFrame(html=html, width=200, height=200)\n","            popup = folium.Popup(iframe, parse_html=True)\n","            # Adds markers for each gauge\n","            folium.Marker([gauge_info['POINT_Y'].iloc[i], gauge_info['POINT_X'].iloc[i]], popup=popup).add_to(map_osm)\n","\n","        # Displays interactive map\n","        display(map_osm.add_child(folium.vector_layers.Polygon(locations=edges, color=line_color, fill_color=fill_color,\n","                                                               weight=weight, popup=folium.Popup(text))))\n","\n","def download_routing_product_lat_lon(lat, lon, product_name):\n","    \"\"\"\n","    Download BasinMaker routing product using latitude and longitude.\n","\n","    Parameters:\n","    - lat (float): Latitude coordinate.\n","    - lon (float): Longitude coordinate.\n","    - product_name (str): Name of the routing product to be downloaded.\n","\n","    Returns:\n","    - str: Path to the downloaded routing product.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Download routing product')\n","    print('-----------------------------------------------------------------------------------------------')\n","    print('product_name: ', product_name)\n","\n","    # Download routing product using provided coordinates\n","    product_path = Download_Routing_Product_From_Points_Or_LatLon(\n","        product_name=product_name, Lat=[lat], Lon=[lon]\n","    )\n","\n","    print('Successfully downloaded routing product using lat and lon!')\n","    return product_path\n","\n","def download_routing_product_gauge(product_name, gauge_name):\n","    \"\"\"\n","    Download BasinMaker routing product using a gauge name.\n","\n","    Parameters:\n","    - product_name (str): Name of the routing product to be downloaded.\n","    - gauge_name (str): Name of the gauge for which the routing product is downloaded.\n","\n","    Returns:\n","    - str: Path to the downloaded routing product.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Download routing product')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Download routing product using the provided gauge name\n","    subid, product_path = Download_Routing_Product_For_One_Gauge(gauge_name=gauge_name, product_name=product_name)\n","\n","    print('Successfully downloaded routing product using the gauge name!')\n","    return product_path\n","\n","\n","# extracts drainage area\n","def extract_drainage_area(product_path,most_down_stream_subbasin_ids,\n","                          most_up_stream_subbasin_ids,temporary_dir,version_num,main_dir):\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Extract drainage area and simplify drainage product')\n","    print('-----------------------------------------------------------------------------------------------')\n","    most_down_stream_subbasin_ids_lst = [most_down_stream_subbasin_ids]\n","    most_up_stream_subbasin_ids_lst = [most_up_stream_subbasin_ids]\n","\n","    # define the folder path for downloaded and unziped lake river routing prodcut folder, where several GIS files exist\n","    unzip_routing_product_folder = product_path\n","\n","    # define another folder that will save the outputs\n","    folder_product_for_interested_gauges=os.path.join(temporary_dir,f'catchment_extraction_{most_down_stream_subbasin_ids}')\n","    # if folder doesn's exist\n","    if not os.path.exists(folder_product_for_interested_gauges):\n","      os.makedirs(folder_product_for_interested_gauges)\n","\n","    # Initialize the basinmaker\n","    start = time.time()\n","    bm = basinmaker.postprocess()\n","\n","    # extract subregion of the routing product\n","    bm.Select_Subregion_Of_Routing_Structure(\n","        path_output_folder = folder_product_for_interested_gauges,\n","        routing_product_folder = unzip_routing_product_folder,\n","        most_down_stream_subbasin_ids=most_down_stream_subbasin_ids_lst,\n","        most_up_stream_subbasin_ids=most_up_stream_subbasin_ids_lst,               # -1: extract to the most-upstream (headwater) subbasin; other subbasin ID: extract the areas from the outlet to the provided subbasin.\n","        gis_platform=\"purepy\",\n","    )\n","    end = time.time()\n","    print(\"This section took  \", end - start, \" seconds\")\n","\n","    # read in study area\n","    studyArea_bound = gpd.read_file(os.path.join(folder_product_for_interested_gauges,f'catchment_without_merging_lakes_{version_num}.shp'))\n","    studyArea_bound[\"dissolve\"] = 1\n","\n","    boundary = studyArea_bound[['dissolve', 'geometry']]\n","    cont_studyArea = boundary.dissolve(by='dissolve')\n","\n","    # generate drive folder\n","    shp_dir = os.path.join(main_dir, 'shapefile')\n","    shp_path = os.path.isdir(shp_dir)\n","    if not shp_path:\n","      os.makedirs(shp_dir)\n","      print(\"created  folder: \", shp_dir)\n","\n","    # save to drive\n","    cont_studyArea.to_file(os.path.join(shp_dir,'studyArea_outline.shp'))\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Shapefile complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def format_shapefile(shp_file_path):\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Format Shapefile')\n","  print('-----------------------------------------------------------------------------------------------')\n","  # find name of shapefile\n","  for shp_file in os.listdir(os.path.join(shp_file_path)):\n","      if shp_file.endswith(\".shp\"):\n","        shp_file_name = shp_file\n","\n","  studyArea_bound = gpd.read_file(os.path.join(shp_file_path,shp_file_name))\n","  studyArea_bound[\"dissolve\"] = 1\n","\n","  boundary = studyArea_bound[['dissolve', 'geometry']]\n","  cont_studyArea = boundary.dissolve(by='dissolve')\n","\n","  # remove contents in folder\n","  for f in glob (os.path.join(shp_file_path,'*')):\n","    os.remove(f)\n","\n","  cont_studyArea.to_file(os.path.join(main_dir, 'shapefile','studyArea_outline.shp'))\n","\n","def visualize_shapefile(shp_file_path):\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Visualize Shapefile')\n","  print('-----------------------------------------------------------------------------------------------')\n","\n","  shp_boundary = gpd.read_file(os.path.join(shp_file_path))\n","\n","  # check projection\n","  if shp_boundary.crs !=  'EPSG:4326':\n","    # reproject\n","    shp_lyr_crs = shp_boundary.to_crs(epsg=4326)\n","    print('Shapefile layer has been reprojected to match shapefile')\n","  else:\n","    shp_lyr_crs = shp_boundary\n","    print('Coordinate systems match!')\n","\n","  # determine the boundary of the provided shapefile\n","  bounds = shp_lyr_crs.bounds\n","  west, south, east, north = bounds = bounds.loc[0]\n","  shp_bounds = [south,west]\n","\n","  map = folium.Map(location=shp_bounds, zoom_start=10)\n","  folium.GeoJson(data=shp_boundary[\"geometry\"]).add_to(map)\n","  display(map)\n","\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Shapefile complete!')\n","  print('-----------------------------------------------------------------------------------------------')\n","\n","def remove_temp_data(main_dir, temporary_dir, product_path):\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Remove unnecessary files')\n","  print('-----------------------------------------------------------------------------------------------')\n","\n","  # Remove the temporary directory if it exists\n","  if os.path.exists(temporary_dir):\n","      shutil.rmtree(temporary_dir)\n","      print(f\"Deleted temporary directory: {temporary_dir}\")\n","\n","  # Remove the product directory if it exists\n","  if os.path.exists(product_path):\n","      shutil.rmtree(product_path)\n","      print(f\"Deleted product directory: {product_path}\")\n","\n","  # Remove all .zip files in the current directory\n","  zip_files_rm = glob(\"*.zip\")\n","  for files_rm in zip_files_rm:\n","      os.remove(files_rm)\n","      print(f\"Deleted zip file: {files_rm}\")\n","\n","  # Remove folders that start with \"drainage_region\" in the main directory\n","  for item in os.listdir('/content'):\n","      item_path = os.path.join('/content', item)\n","      if os.path.isdir(item_path) and item.startswith(\"drainage_region\"):\n","          shutil.rmtree(item_path)\n","          print(f\"Deleted folder: {item_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"n-9Mf04ehBR7"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Option A** </font> <br>\n","#@markdown <font color=grey> example, \"Waterloo, ON, Canada\" <br>\n","#@markdown *If you rather enter your own coordinates, enter \"NA\" into city_name* </font> <br>\n","\n","city_name = 'Waterloo, ON, Canada' #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Option B** </font> <br>\n","#@markdown <font color=grey> example, 43.4652699 -80.5222961 <br>\n","#@markdown *If you rather use the city name derived coordinates, leave this as \"NA\" (be sure to include \" \" around NA)* </font> <br>\n","\n","define_lat = 'NA' #@param {type:\"string\"}\n","define_lon = 'NA' #@param {type:\"string\"}\n","\n","# assign lat and lon to either found or identified coordinates\n","if city_name != 'NA' and define_lat == 'NA':\n","    lat, lon = studyArea_location(city_name,define_lat,define_lon)\n","elif city_name == 'NA' and define_lat != 'NA':\n","    lat,lon = float(define_lat), float(define_lon),\n","    print(\"Study area coordinates:\", lat,lon)\n","elif city_name and define_lat == \"NA\":\n","    lat = None\n","    print(\"Define gauge name in cell below\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mo-0HPDthnU7"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Identifying obervation gauges** </font> <br><br>\n","#@markdown  Check \"visualize\" to produce an interactive map that includes an overlaying rectangle to assist in identifying which observation gauges are located near your study area. It also provides information about which subbasins and subbasin region your study area is in. <br><br>\n","#@markdown Give the map a little time to load even after the cell is done running, it is loading a lot of information and can take a bit. <br><br>\n","\n","map_visualize = True #@param {type:\"boolean\"}\n","\n","# generate map with rectangular guide to assist in identifying the ideal subbasin/gauges to use\n","if map_visualize == True:\n","  interactive_map_response = 'yes'\n","  if lat is None:\n","    print(\"Define gauge name in cell above\")\n","  else:\n","    interactive_gauge_map(lat,lon, main_dir)\n","else:\n","  interactive_map_response = 'no'"]},{"cell_type":"markdown","metadata":{"id":"wRdNz5Jtc31p"},"source":["<font color=#5559AB> **Download routing product for catchment of interest** </font> <br>\n","The North American Lake-River Routing Product (version 2.1) covers the main drainage regions across the North America (Canada and the USA). The Ontario Lake-River Routing Product(version 1.0) covers the main drainage regions across the Ontario Province, Canada. <br>\n","Both the NA routing product and the OLRRP provide sub-region-wise product for download. <br>\n","BasinMaker has two built-in functions for routing product download, which are named Download_Routing_Product_For_One_Gauge and Download_Routing_Product_From_Points_Or_LatLon.<br><br>\n","This leaves us two options for data download: <br>\n","\n","<font color=#5559AB> **Option #1**: Provide function Download_Routing_Product_For_One_Gauge with gauge ID. </font> <br>\n","\n","<font color=#5559AB>**Option #2**: Provide function Download_Routing_Product_From_Points_Or_LatLon with the outlet coordinates (lat-lon in degree decimals). </font><br><br>\n","\n","*All options are for BasinMaker to find out the subbasin ID of the outlet subbasin. BasinMaker will use subbasin ID to extract the drainage areas.*<br>\n","\n","The map above or the following link can be used to help identify gauges of interest: </font>\n","https://wateroffice.ec.gc.ca/search/historical_e.html"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","collapsed":true,"id":"--IWmA96cpwW"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define the product name** </font><br>\n","#@markdown [OLRRP](https://uwaterloo-olrrp.shinyapps.io/OLRRP-V2/) to use the Ontario Lake-River Routing Product(version 1.0)<br>\n","#@markdown [NALRP](https://hydrology.uwaterloo.ca/basinmaker/download_regional.html) to use the North American Lake-River Routing Product (version 2.1)</font><br>\n","\n","product_name = \"NALRP\"  #@param [\"NALRP\", \"OLRRP\"]\n","\n","\n","#@markdown <font color= #5559AB> **Option 1:**</font> Download routing product using gauge name<br>\n","#@markdown <font color=grey>example, \"02GA024\" </font><br>\n","\n","gauge_name = \"02GA024\" #@param {type:\"string\"}\n","\n","#@markdown <font color= #5559AB> **Option 2:**</font> Download routing product using lat and lon <br>\n","#@markdown <font color=grey>*The previously defined latitude and longitudes will be used if the \"Define Gauge Name\" is set to \"NA\"* <br>\n","\n","# downloads BasinMaker routing product with lat and lon\n","\n","if product_name == \"NALRP\":\n","    if gauge_name == \"NA\":\n","      lat_val = [float(define_lat)]\n","      lon_val = [float(define_lon)]\n","      subid,product_path = Download_Routing_Product_From_Points_Or_LatLon(product_name = product_name,Lat = lat_val,Lon = lon_val)\n","    else:\n","      product_path = download_routing_product_gauge(product_name,gauge_name)\n","\n","if product_name == \"OLRRP\":\n","    if gauge_name != \"NA\":\n","\n","      print('\\n-----------------------------------------------------------------------------------------------')\n","      print('( ) Download routing product')\n","      print('-----------------------------------------------------------------------------------------------')\n","      print('product_name: ', product_name)\n","\n","      # define path\n","      product_path = os.path.join('/content','drainage_region_olrrp')\n","\n","      # Download routing product using provided coordinates\n","      Extract_Routing_Product(version='v2-0', by='Obs_NM', obs_nm=gauge_name,output_path=product_path)\n","    else:\n","      print('Downloading the routing product with lat and lon for OLRRP is currently unavailable')\n"]},{"cell_type":"code","source":["def map_with_gauges_and_clipped_shapefile(subbasin_path, river_path, gauge_csv_path, subbasin_of_interest=None):\n","    \"\"\"\n","    Display a map with gauges, clipped subbasins, rivers, and the grey outline of the original shapefile.\n","    \"\"\"\n","\n","    import geopandas as gpd\n","    import pandas as pd\n","    import folium\n","    from shapely.geometry import box\n","    from IPython.display import display\n","\n","    # Load subbasin and river shapefiles\n","    try:\n","        gdf = gpd.read_file(subbasin_path)\n","        river_gdf = gpd.read_file(river_path)\n","    except Exception as e:\n","        print(f\"Error loading shapefiles: {e}\")\n","        return\n","\n","    # Load gauge data\n","    try:\n","        gauge_df = pd.read_csv(gauge_csv_path)\n","        gauge_gdf = gpd.GeoDataFrame(\n","            gauge_df,\n","            geometry=gpd.points_from_xy(gauge_df['POINT_X'], gauge_df['POINT_Y']),\n","            crs=\"EPSG:4326\"\n","        )\n","    except Exception as e:\n","        print(f\"Error loading gauge data: {e}\")\n","        return\n","\n","    # Ensure CRS consistency\n","    gdf = gdf.to_crs(epsg=4326)\n","    river_gdf = river_gdf.to_crs(epsg=4326)\n","    gauge_gdf = gauge_gdf.to_crs(epsg=4326)\n","\n","    # Define rectangle bounds based on the extent of the subbasin shapefile or a specific subbasin\n","    if subbasin_of_interest:\n","        subbasin = gdf[gdf['SubId'] == subbasin_of_interest]\n","        if not subbasin.empty:\n","            bounds = subbasin.total_bounds  # [minx, miny, maxx, maxy]\n","        else:\n","            print(f\"Subbasin of interest {subbasin_of_interest} not found.\")\n","            return\n","    else:\n","        bounds = gdf.total_bounds  # [minx, miny, maxx, maxy]\n","\n","    # Expand bounds for visualization\n","    minx, miny, maxx, maxy = bounds\n","    W, E, S, N = minx - 0.2, maxx + 0.2, miny - 0.2, maxy + 0.2\n","    rectangle_polygon = box(W, S, E, N)\n","\n","    # Clip shapefiles\n","    gdf_clipped = gpd.clip(gdf, rectangle_polygon)\n","    river_clipped = gpd.clip(river_gdf, rectangle_polygon)\n","    gauge_clipped = gpd.clip(gauge_gdf, rectangle_polygon)\n","\n","    # Center map at the centroid of the rectangle\n","    center_lat = (S + N) / 2\n","    center_lon = (W + E) / 2\n","\n","    # Create the map\n","    map_osm = folium.Map(\n","        location=[center_lat, center_lon],\n","        zoom_start=12,\n","        tiles=\"https://{s}.basemaps.cartocdn.com/light_nolabels/{z}/{x}/{y}{r}.png\",\n","        attr=\"© OpenStreetMap contributors © CARTO\"\n","    )\n","\n","    # Add the grey outline of the original shapefile\n","    folium.GeoJson(\n","        gdf.to_json(),\n","        name=\"Original Subbasin Outline\",\n","        style_function=lambda feature: {'color': 'grey', 'weight': 1, 'fillOpacity': 0},\n","    ).add_to(map_osm)\n","\n","    # Add clipped rivers\n","    folium.GeoJson(\n","        river_clipped.to_json(),\n","        name=\"Clipped Rivers\",\n","        style_function=lambda feature: {'color': 'lightblue', 'weight': 3},\n","    ).add_to(map_osm)\n","\n","    # Add subbasins and highlight subbasin of interest\n","    for _, row in gdf_clipped.iterrows():\n","        centroid = row.geometry.centroid\n","        subid = row.get(\"SubId\", \"Unknown\")\n","\n","        if subbasin_of_interest and subid == subbasin_of_interest:\n","            folium.GeoJson(\n","                row.geometry,\n","                style_function=lambda feature: {'fillColor': 'yellow', 'color': 'orange', 'weight': 3, 'fillOpacity': 0.5},\n","                name=f\"Highlighted Subbasin {subbasin_of_interest}\"\n","            ).add_to(map_osm)\n","\n","        # Add subbasin ID\n","        folium.Marker(\n","            location=[centroid.y, centroid.x],\n","            icon=folium.DivIcon(html=f'<div style=\"font-size: 10px;\">{subid}</div>'),\n","        ).add_to(map_osm)\n","\n","    # Add clipped gauges\n","    for _, row in gauge_clipped.iterrows():\n","        folium.Marker(\n","            location=[row.geometry.y, row.geometry.x],\n","            popup=row.get(\"Obs_NM\", \"Unnamed Gauge\"),\n","            icon=folium.Icon(color=\"blue\", icon=\"info-sign\"),\n","        ).add_to(map_osm)\n","\n","    # Add the clipped subbasin shapefile\n","    folium.GeoJson(\n","        gdf_clipped.to_json(),\n","        name=\"Clipped Subbasins\",\n","        style_function=lambda feature: {\n","            'fillColor': 'grey',\n","            'color': 'grey',\n","            'weight': 0.5,\n","            'fillOpacity': 0,\n","        }\n","    ).add_to(map_osm)\n","\n","    # Add red rectangle\n","    folium.vector_layers.Polygon(\n","        locations=[(N, W), (N, E), (S, E), (S, W)],\n","        color='darkred',\n","        weight=3,\n","        fill=False\n","    ).add_to(map_osm)\n","\n","    # Layer control\n","    folium.LayerControl().add_to(map_osm)\n","\n","    # Display the map\n","    display(map_osm)\n","\n","#@markdown <font color= #5559AB> **View Subbasin of Interest** </font><br>\n","#@markdown Check box and enter name of subbasin of interest to view the location of the subbasin <br>\n","#@markdown <font color=grey>example, 3086525 </font><br>\n","\n","view_subbasin = True # @param {type:\"boolean\"}\n","\n","if view_subbasin:\n","    # define  subbasin of interest\n","    subbasin_of_interest = 3086525 #@param\n","\n","    if product_name == 'OLRRP':\n","        subbasin_path = '/content/drainage_region_olrrp/finalcat_info_v2-0.shp'\n","        river_path = '/content/drainage_region_olrrp/finalcat_info_riv_v2-0.shp'\n","    elif product_name == 'NALRP':\n","\n","        search_prefix = \"drainage_region_\"\n","        base_dir = \"/content\"\n","\n","        # Find the first matching folder\n","        found_folder = next(\n","            (os.path.join(root, dir_name)\n","            for root, dirs, _ in os.walk(base_dir)\n","            for dir_name in dirs if dir_name.startswith(search_prefix)),\n","            None\n","        )\n","\n","        subbasin_path = os.path.join(found_folder, 'finalcat_info_v2-1.shp')\n","        river_path = os.path.join(found_folder, 'finalcat_info_riv_v2-1.shp')\n","\n","    # path to gauge ID\n","    gauge_csv_path = os.path.join(main_dir, \"extras\", \"subbasin_plots\",\"obs_gauges_NA_v2-1.csv\")  # Path to gauge CSV file\n","    map_with_gauges_and_clipped_shapefile(\n","        subbasin_path=subbasin_path,\n","        river_path=river_path,\n","        gauge_csv_path=gauge_csv_path,\n","        subbasin_of_interest=subbasin_of_interest  # Optional\n","    )"],"metadata":{"cellView":"form","id":"KsiKGAmx61JM"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"NF7ObsaWdQ1e"},"outputs":[],"source":["#@markdown <font color= #5559AB> **Extract drainage area and simplify drainage product** </font><br>\n","#@markdown BasinMaker needs the ID of subbasin (subId) which the gauge is situated in <br>\n","#@markdown <font color=grey>example, 3086525 </font><br>\n","\n","most_down_stream_subbasin_ids = 3086525 #@param\n","\n","most_up_stream_subbasin_ids = -1 #@param\n","\n","if product_name == \"NALRP\":\n","  version_num = \"v2-1\"\n","if product_name == \"OLRRP\":\n","  version_num = \"v2-0\"\n","\n","extract_drainage_area(product_path,most_down_stream_subbasin_ids,\n","                          most_up_stream_subbasin_ids,temporary_dir,version_num,main_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wvSGpAGoSWuv"},"outputs":[],"source":["#@markdown <font color= grey> **Visualize shapefile of study area**</font><br>\n","#@markdown produces an interactive map for users to visualize the generated shapefile boundary area\n","\n","shp_file_path = os.path.join(main_dir, 'shapefile')\n","visualize_shapefile(shp_file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bxEs3ITEDQ85"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","# delete temorary folder\n","remove_temp_data(main_dir, temporary_dir,product_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"M6ogvn5DKAfu"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file:\n","  data = {\n","      \"_comment\": \"------------1a) STUDY AREA---------------\",\n","      \"generate_DEM\": \"yes\",\n","      \"define_lat_shp\": f\"{define_lat}\",\n","      \"define_lon_shp\": f\"{define_lon}\",\n","      \"city_name_shp\": f\"{city_name}\",\n","\n","      \"interactive_map_response\": f\"{interactive_map_response}\",\n","\n","      \"product_name_shp\": f\"{product_name}\",\n","      \"gauge_name_shp\": f\"{gauge_name}\",\n","\n","      \"version_num_shp\": f\"{version_num}\",\n","\n","      \"most_down_stream_subbasin_ids_shp\": most_down_stream_subbasin_ids,\n","      \"most_up_stream_subbasin_ids_shp\": most_up_stream_subbasin_ids,\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"1a_studyArea.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"0jFuJD0zVI5Y"},"source":["**References**\n","\n","Han, M., H. Shen, B. A. Tolson, J. R. Craig, J. Mai, S. Lin, N. B. Basu, F. Awol. (2021). BasinMaker 3.0: a GIS toolbox for distributed watershed delineation of complex lake-river routing networks. Environmental Modelling and Software."]},{"cell_type":"markdown","metadata":{"id":"ASk0UnaqqUwE"},"source":["## <font color=#5559AB> 1b) DEM  </font>\n","\n","Magpie utilizes the [MERIT DEM](https://developers.google.com/earth-engine/datasets/catalog/MERIT_DEM_v1_0_3) available through Google Earth Engine. MERIT DEM is a high-accuracy global DEM at 3 arcs second resolution (~90 m at the equator) produced by eliminating major error components from existing DEMs (NASA SRTM3 DEM, JAXA AW3D DEM, Viewfinder Panoramas DEM).\n","\n","In this section, the DEM layer is downloaded, clipped, and saved to the mounted Google Drive.\n","\n","Only a <font color=red>shapefile of the study area</font> is required to run this subsection.\n"]},{"cell_type":"markdown","metadata":{"id":"YyVqr8g2WxFy"},"source":["### <font color=grey> **Upload DEM** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"OB0r6fDF1u2O"},"outputs":[],"source":["#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def check_projection(shp_file_path, main_dir,temporary_dir):\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Check projection of shapefile')\n","  print('-----------------------------------------------------------------------------------------------')\n","  # find name of shapefile\n","  for shp_file in os.listdir(os.path.join(shp_file_path)):\n","      if shp_file.endswith(\".shp\"):\n","        shp_file_name = shp_file\n","  print('Shapefile name: ', shp_file_name)\n","  print('Shapefile path: ', os.path.join(main_dir, \"shapefile\",shp_file_name))\n","  shp_lyr_check = gpd.read_file(os.path.join(main_dir, \"shapefile\",shp_file_name))\n","  print('Shapefile CRS: ', shp_lyr_check.crs)\n","\n","  if shp_lyr_check.crs !=  'EPSG:4326':\n","    # reproject\n","    shp_lyr_crs = shp_lyr_check.to_crs(epsg=4326)\n","    shp_lyr_crs.to_file(os.path.join(main_dir, \"shapefile\",shp_file))\n","    print('Shapefile layer has been reprojected to match shapefile')\n","  else:\n","    shp_lyr_crs = shp_lyr_check\n","    print('Coordinate systems match!')\n","\n","def format_and_visualize_dem(shp_file_path, temporary_dir, main_dir):\n","    \"\"\"\n","    Clip DEM to the study area, reproject if necessary, and save the clipped DEM to drive.\n","\n","    Args:\n","        shp_file_path (str): Path to the directory containing the shapefile.\n","        temporary_dir (str): Temporary directory path.\n","        main_dir (str): Main directory path.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Clip DEM to study area')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find name of shapefile\n","    for shp_file in os.listdir(os.path.join(shp_file_path)):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Find name of DEM file\n","    for dem_file in os.listdir(os.path.join(os.path.join(temporary_dir, 'DEM'))):\n","        if dem_file.endswith(\".tif\"):\n","            dem_file_name = dem_file\n","\n","    # Open raster DEM layer\n","    dem_lyr = rxr.open_rasterio(os.path.join(temporary_dir, 'DEM', dem_file_name), masked=True).squeeze()\n","\n","    # Load shapefile (crop extent)\n","    crop_extent = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name))\n","\n","    print('Shapefile CRS:', crop_extent.crs)\n","    print('DEM CRS:', dem_lyr.rio.crs)\n","\n","    # Check if CRS match and reproject if necessary\n","    if crop_extent.crs != dem_lyr.rio.crs:\n","        dem_lyr = dem_lyr.rio.reproject(crop_extent.crs)\n","        print('\\nDEM layer has been reprojected to match shapefile')\n","    else:\n","        print('\\nCoordinate systems match')\n","\n","    # Open crop extent (study area extent boundary)\n","    crop_extent_buffered = crop_extent.buffer(0.001)\n","\n","    # Clip the DEM layer\n","    lidar_clipped = dem_lyr.rio.clip(crop_extent_buffered, crop_extent_buffered.crs)\n","    print('\\nDEM layer has been clipped')\n","\n","    # Define output directory path\n","    dem_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')\n","\n","    if not os.path.exists(dem_output_dir):\n","        os.makedirs(dem_output_dir)\n","\n","    # Save clipped DEM layer to drive\n","    path_to_tif_file = os.path.join(dem_output_dir, 'dem.tif')\n","    lidar_clipped.rio.to_raster(path_to_tif_file)\n","    print('\\nDEM layer has been saved to drive')\n","\n","\n","def visualize_dem(DEM_visualize, Aspect_visualize, save_aspect, main_dir, temporary_dir):\n","    \"\"\"\n","    Visualize DEM layer, produce slope and aspect layers, and save them to drive.\n","\n","    Args:\n","        DEM_visualize (str): 'yes' to visualize DEM layer, 'no' otherwise.\n","        Slope_visualize (str): 'yes' to visualize slope layer, 'no' otherwise.\n","        Aspect_visualize (str): 'yes' to visualize aspect layer, 'no' otherwise.\n","        save_shp_of_aspect (str): 'yes' to save shapefile of aspect layer, 'no' otherwise.\n","        main_dir (str): Main directory path.\n","        temporary_dir (str): Temporary directory path.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize DEM layer and produce slope and aspect layers')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Define output directory path for slope and aspect\n","    slope_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Slope')\n","    aspect_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Aspect')\n","\n","    # Create output directories if they don't exist\n","    for output_dir in [slope_output_dir, aspect_output_dir]:\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","    if DEM_visualize == 'yes':\n","        for dem_file in os.listdir(os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')):\n","            if dem_file.endswith(\".tif\"):\n","                print('---------------------------------------------- DEM Layer ----------------------------------------------')\n","                # Open and visualize DEM layer\n","                dem_lyr = rxr.open_rasterio(os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM', dem_file),\n","                                            masked=True).squeeze()\n","                f, ax = plt.subplots(figsize=(8, 10))\n","                dem_lyr.plot(ax=ax)\n","                ax.set_axis_off()\n","                plt.show()\n","\n","                # Calculate and print statistics of DEM layer\n","                dem_min = int(dem_lyr.min())\n","                dem_max = int(dem_lyr.max())\n","                dem_mean = int(dem_lyr.mean())\n","                print('\\nMinimum Elevation:', dem_min)\n","                print('Maximum Elevation:', dem_max)\n","                print('Mean Elevation:', dem_mean)\n","\n","    def calculate_aspect(dem_file, output_path):\n","        \"\"\"\n","        Calculate the aspect of a DEM and save the aspect raster.\n","\n","        :param dem_file: Path to the input DEM file.\n","        :param output_path: Path to save the aspect output.\n","        \"\"\"\n","        with rasterio.open(dem_file) as src:\n","            # Read DEM as a 2D numpy array\n","            dem = src.read(1, resampling=Resampling.bilinear)\n","            transform = src.transform\n","            no_data = src.nodata or -9999  # Default to -9999 if no_data is None\n","\n","            # Handle no_data values\n","            dem[dem == no_data] = np.nan\n","\n","            # Calculate gradients\n","            dx, dy = np.gradient(dem, np.abs(transform[0]), np.abs(transform[4]))\n","\n","            # Calculate aspect\n","            aspect = np.arctan2(dy, -dx)  # Aspect in radians\n","            aspect = np.degrees(aspect)  # Convert to degrees\n","            aspect = (aspect + 360) % 360  # Normalize to [0, 360]\n","\n","            # Save aspect as a GeoTIFF\n","            profile = src.profile\n","            profile.update(dtype=rasterio.float32, count=1, nodata=np.nan)\n","            with rasterio.open(output_path, 'w', **profile) as dst:\n","                dst.write(aspect.astype(rasterio.float32), 1)\n","\n","        print(f\"Aspect saved to {output_path}\")\n","        return aspect\n","\n","    if Aspect_visualize == 'yes':\n","        dem_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')\n","        aspect_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Aspect')\n","        os.makedirs(aspect_output_dir, exist_ok=True)\n","\n","        for dem_file in os.listdir(dem_dir):\n","            if dem_file.endswith(\".tif\"):\n","                print('---------------------------------------------- Aspect Layer ----------------------------------------------')\n","                dem_path = os.path.join(dem_dir, dem_file)\n","                aspect_path = os.path.join(aspect_output_dir, \"aspect.tif\")\n","\n","                # Calculate aspect\n","                aspect_array = calculate_aspect(dem_path, aspect_path)\n","\n","                # Visualize aspect\n","                plt.figure(figsize=(8, 9))\n","                plt.imshow(aspect_array, cmap='jet', extent=rasterio.open(dem_path).bounds)\n","                plt.colorbar(label=\"Aspect (degrees)\")\n","                plt.title(\"Aspect Layer\")\n","                plt.xlabel(\"Longitude\")\n","                plt.ylabel(\"Latitude\")\n","                plt.show()\n","\n","                # Calculate statistics\n","                aspect_min = int(np.nanmin(aspect_array))\n","                aspect_max = int(np.nanmax(aspect_array))\n","                aspect_mean = int(np.nanmean(aspect_array))\n","                print('\\nMinimum Aspect:', aspect_min)\n","                print('Maximum Aspect:', aspect_max)\n","                print('Mean Aspect:', aspect_mean)\n","\n","    if save_aspect == \"yes\":\n","        # Save shapefile of aspect layer\n","        aspect_rast = os.path.join(aspect_output_dir, 'aspect.tif')\n","        aspect_shp = os.path.join(aspect_output_dir, 'aspect.shp')\n","\n","        # Use gdal_polygonize to convert raster to vector (shapefile)\n","        with open(os.path.join(temporary_dir, 'polygon.sh'), 'w') as f3:\n","            print(f'gdal_polygonize.py \"{aspect_rast}\" \"{aspect_shp}\" -b 1 -f \"ESRI Shapefile\"', file=f3)\n","\n","        sh_file = os.path.join(temporary_dir, 'polygon.sh')\n","        subprocess.run(['bash', sh_file])\n","\n","        # Open and format the shapefile\n","        aspect_shp_gdf = gpd.read_file(aspect_shp)\n","        aspect_shp_gdf[\"O_ID_2\"] = list(range(1, (len(aspect_shp_gdf.index) + 1)))\n","        aspect_shp_gdf[\"Aspect\"] = aspect_shp_gdf.DN\n","\n","        # Save the final elevation band shapefile to drive\n","        aspect_shp_gdf.to_file(aspect_shp)\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) DEM Complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yImFysItYsKI"},"outputs":[],"source":["#@markdown <font color=grey> **Upload DEM File** </font>\n","\n","#@markdown drag-and-drop the DEM file into the specified folder\n","\n","dem_temp_dir = os.path.join(temporary_dir,'DEM')\n","if not os.path.exists(dem_temp_dir):\n","  os.makedirs(dem_temp_dir)\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('( ) Upload DEM')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop DEM file into following folder: {dem_temp_dir}')\n","response = input(\"Have you uploaded the DEM (.tif) file (yes or no): \")\n","if response == \"yes\":\n","  shp_file_path = os.path.join(main_dir, 'shapefile')\n","  # check projection\n","  check_projection(shp_file_path, main_dir,temporary_dir)\n","  # format and visualize\n","  format_and_visualize_dem(shp_file_path,temporary_dir,main_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"82TpNqVOZRfe"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Check the box to visualize clipped DEM layer** </font>\n","\n","#@markdown Slope and aspect are computed from the DEM layer to provide users additional information about their study area </font> <br>\n","DEM_visualize = True #@param {type:\"boolean\"}\n","Aspect_visualize = True #@param {type:\"boolean\"}\n","\n","#@markdown Check box to save a shapefile layer of aspect to drive  </font> <br>\n","save_aspect = False #@param {type:\"boolean\"}\n","\n","if DEM_visualize == True:\n","  DEM_visualize = \"yes\"\n","if Aspect_visualize == True:\n","  Aspect_visualize = \"yes\"\n","if save_aspect == True:\n","  save_aspect = \"yes\"\n","\n","visualize_dem(DEM_visualize, Aspect_visualize, save_aspect, main_dir, temporary_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XXcQw4s6dJXl"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lth9C_PK-wpQ"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------ 1b) DEM ---------------\",\n","    \"generate_DEM\": \"no\",\n","    \"upload_DEM\": \"yes\",\n","    \"DEM_visualize\": f\"{DEM_visualize}\",\n","    \"Aspect_visualize\": f\"{Aspect_visualize}\",\n","    \"save_aspect\": f\"{save_aspect}\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"1b_dem.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"_vokMNBydCjm"},"source":["### <font color=grey> **Generate DEM - Google Earth Engine** </font>\n","\n","Check out the short video [Creating DEM Layer in Magpie Workflow](https://youtu.be/S8rh7aovu_0) for more information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kLDoXl9r5j0k"},"outputs":[],"source":["import ee\n","import requests\n","from rasterio.enums import Resampling\n","#import richdem as rd\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","# Trigger the authentication flow.\n","service_account = 'magpie-developer@magpie-id-409519.iam.gserviceaccount.com'\n","credentials = ee.ServiceAccountCredentials(service_account, os.path.join(main_dir,'extras','magpie-key.json'))\n","\n","# Initialize the library.\n","ee.Initialize(credentials)\n","\n","def check_projection(shp_file_path, main_dir,temporary_dir):\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Check projection of shapefile')\n","  print('-----------------------------------------------------------------------------------------------')\n","  # find name of shapefile\n","  for shp_file in os.listdir(os.path.join(shp_file_path)):\n","      if shp_file.endswith(\".shp\"):\n","        shp_file_name = shp_file\n","  print('Shapefile name: ', shp_file_name)\n","  print('Shapefile path: ', os.path.join(main_dir, \"shapefile\",shp_file_name))\n","  shp_lyr_check = gpd.read_file(os.path.join(main_dir, \"shapefile\",shp_file_name))\n","  print('Shapefile CRS: ', shp_lyr_check.crs)\n","\n","  if shp_lyr_check.crs !=  'EPSG:4326':\n","    # reproject\n","    shp_lyr_crs = shp_lyr_check.to_crs(epsg=4326)\n","    shp_lyr_crs.to_file(os.path.join(main_dir, \"shapefile\",shp_file_name))\n","    print('Shapefile layer has been reprojected to match shapefile')\n","  else:\n","    shp_lyr_crs = shp_lyr_check\n","    print('Coordinate systems match!')\n","\n","def download_DEM(shp_file_path, data_source, band_name, scale, main_dir, temporary_dir):\n","    \"\"\"\n","    Download a Digital Elevation Model (DEM) based on the provided shapefile and parameters.\n","\n","    Args:\n","        shp_file_path (str): Path to the directory containing the shapefile.\n","        data_source: Earth Engine data source.\n","        band_name (str): Name of the band in the DEM.\n","        scale (float): Scale for the DEM download.\n","        main_dir (str): Main directory path.\n","        temporary_dir (str): Temporary directory path.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Download DEM')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Define buffer size\n","    buffer_size = 0.01\n","\n","    # Find name of shapefile\n","    shp_file_path = os.path.join(main_dir, 'shapefile')\n","    for shp_file in os.listdir(os.path.join(shp_file_path)):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Determine the boundary of the provided shapefile\n","    bounds = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name)).bounds\n","    west, south, east, north = bounds = bounds.loc[0]\n","    west -= buffer_size * (east - west)\n","    east += buffer_size * (east - west)\n","    south -= buffer_size * (north - south)\n","    north += buffer_size * (north - south)\n","\n","    img = ee.Image(data_source)\n","    region = ee.Geometry.BBox(west, south, east, north)\n","\n","    # Multi-band GeoTIFF file.\n","    url = img.getDownloadUrl({\n","        'bands': [band_name],\n","        'region': region,\n","        'scale': scale,\n","        'format': 'GEO_TIFF'\n","    })\n","\n","    # Define output directory for DEM\n","    dem_dir = os.path.join(temporary_dir, 'DEM')\n","    if not os.path.exists(dem_dir):\n","        os.makedirs(dem_dir)\n","\n","    # Download DEM using requests\n","    response = requests.get(url)\n","    with open(os.path.join(dem_dir, 'dem.tif'), 'wb') as fd:\n","        fd.write(response.content)\n","\n","def format_and_visualize_dem(shp_file_path, temporary_dir, main_dir):\n","    \"\"\"\n","    Clip DEM to the study area, reproject if necessary, and save the clipped DEM to drive.\n","\n","    Args:\n","        shp_file_path (str): Path to the directory containing the shapefile.\n","        temporary_dir (str): Temporary directory path.\n","        main_dir (str): Main directory path.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Clip DEM to study area')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find name of shapefile\n","    for shp_file in os.listdir(os.path.join(shp_file_path)):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Find name of DEM file\n","    for dem_file in os.listdir(os.path.join(os.path.join(temporary_dir, 'DEM'))):\n","        if dem_file.endswith(\".tif\"):\n","            dem_file_name = dem_file\n","\n","    # Open raster DEM layer\n","    dem_lyr = rxr.open_rasterio(os.path.join(temporary_dir, 'DEM', dem_file_name), masked=True).squeeze()\n","\n","    # Load shapefile (crop extent)\n","    crop_extent = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name))\n","\n","    print('Shapefile CRS:', crop_extent.crs)\n","    print('DEM CRS:', dem_lyr.rio.crs)\n","\n","    # Check if CRS match and reproject if necessary\n","    if crop_extent.crs != dem_lyr.rio.crs:\n","        dem_lyr = dem_lyr.rio.reproject(crop_extent.crs)\n","        print('\\nDEM layer has been reprojected to match shapefile')\n","    else:\n","        print('\\nCoordinate systems match')\n","\n","    # Open crop extent (study area extent boundary)\n","    crop_extent_buffered = crop_extent.buffer(0.001)\n","\n","    # Clip the DEM layer\n","    lidar_clipped = dem_lyr.rio.clip(crop_extent_buffered, crop_extent_buffered.crs)\n","    print('\\nDEM layer has been clipped')\n","\n","    # Define output directory path\n","    dem_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')\n","\n","    if not os.path.exists(dem_output_dir):\n","        os.makedirs(dem_output_dir)\n","\n","    # Save clipped DEM layer to drive\n","    path_to_tif_file = os.path.join(dem_output_dir, 'dem.tif')\n","    lidar_clipped.rio.to_raster(path_to_tif_file)\n","    print('\\nDEM layer has been saved to drive')\n","\n","\n","def visualize_dem(DEM_visualize, Aspect_visualize, save_aspect, main_dir, temporary_dir):\n","    \"\"\"\n","    Visualize DEM layer, produce slope and aspect layers, and save them to drive.\n","\n","    Args:\n","        DEM_visualize (str): 'yes' to visualize DEM layer, 'no' otherwise.\n","        Aspect_visualize (str): 'yes' to visualize aspect layer, 'no' otherwise.\n","        save_shp_of_aspect (str): 'yes' to save shapefile of aspect layer, 'no' otherwise.\n","        main_dir (str): Main directory path.\n","        temporary_dir (str): Temporary directory path.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize DEM layer and produce slope and aspect layers')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Define output directory path for slope and aspect\n","    slope_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Slope')\n","    aspect_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Aspect')\n","\n","    # Create output directories if they don't exist\n","    for output_dir in [slope_output_dir, aspect_output_dir]:\n","        if not os.path.exists(output_dir):\n","            os.makedirs(output_dir)\n","\n","    if DEM_visualize == 'yes':\n","        for dem_file in os.listdir(os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')):\n","            if dem_file.endswith(\".tif\"):\n","                print('---------------------------------------------- DEM Layer ----------------------------------------------')\n","                # Open and visualize DEM layer\n","                dem_lyr = rxr.open_rasterio(os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM', dem_file),\n","                                            masked=True).squeeze()\n","                f, ax = plt.subplots()\n","                dem_lyr.plot(ax=ax)\n","                ax.set_axis_off()\n","                plt.show()\n","\n","                # Calculate and print statistics of DEM layer\n","                dem_min = int(dem_lyr.min())\n","                dem_max = int(dem_lyr.max())\n","                dem_mean = int(dem_lyr.mean())\n","                print('\\nMinimum Elevation:', dem_min)\n","                print('Maximum Elevation:', dem_max)\n","                print('Mean Elevation:', dem_mean)\n","\n","    def calculate_aspect(dem_file, output_path):\n","        \"\"\"\n","        Calculate the aspect of a DEM and save the aspect raster.\n","\n","        :param dem_file: Path to the input DEM file.\n","        :param output_path: Path to save the aspect output.\n","        \"\"\"\n","        with rasterio.open(dem_file) as src:\n","            # Read DEM as a 2D numpy array\n","            dem = src.read(1, resampling=Resampling.bilinear)\n","            transform = src.transform\n","            no_data = src.nodata or -9999  # Default to -9999 if no_data is None\n","\n","            # Handle no_data values\n","            dem[dem == no_data] = np.nan\n","\n","            # Calculate gradients\n","            dx, dy = np.gradient(dem, np.abs(transform[0]), np.abs(transform[4]))\n","\n","            # Calculate aspect\n","            aspect = np.arctan2(dy, -dx)  # Aspect in radians\n","            aspect = np.degrees(aspect)  # Convert to degrees\n","            aspect = (aspect + 360) % 360  # Normalize to [0, 360]\n","\n","            # Save aspect as a GeoTIFF\n","            profile = src.profile\n","            profile.update(dtype=rasterio.float32, count=1, nodata=np.nan)\n","            with rasterio.open(output_path, 'w', **profile) as dst:\n","                dst.write(aspect.astype(rasterio.float32), 1)\n","\n","        print(f\"Aspect saved to {output_path}\")\n","        return aspect\n","\n","    if Aspect_visualize == 'yes':\n","        dem_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')\n","        aspect_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Aspect')\n","        os.makedirs(aspect_output_dir, exist_ok=True)\n","\n","        for dem_file in os.listdir(dem_dir):\n","            if dem_file.endswith(\".tif\"):\n","                print('---------------------------------------------- Aspect Layer ----------------------------------------------')\n","                dem_path = os.path.join(dem_dir, dem_file)\n","                aspect_path = os.path.join(aspect_output_dir, \"aspect.tif\")\n","\n","                # Calculate aspect\n","                aspect_array = calculate_aspect(dem_path, aspect_path)\n","\n","                # Visualize aspect\n","                plt.figure(figsize=(8, 9))\n","                plt.imshow(aspect_array, cmap='jet', extent=rasterio.open(dem_path).bounds)\n","                plt.colorbar(label=\"Aspect (degrees)\")\n","                plt.title(\"Aspect Layer\")\n","                plt.xlabel(\"Longitude\")\n","                plt.ylabel(\"Latitude\")\n","                plt.show()\n","\n","                # Calculate statistics\n","                aspect_min = int(np.nanmin(aspect_array))\n","                aspect_max = int(np.nanmax(aspect_array))\n","                aspect_mean = int(np.nanmean(aspect_array))\n","                print('\\nMinimum Aspect:', aspect_min)\n","                print('Maximum Aspect:', aspect_max)\n","                print('Mean Aspect:', aspect_mean)\n","\n","    if save_aspect == \"yes\":\n","        # Save shapefile of aspect layer\n","        aspect_rast = os.path.join(aspect_output_dir, 'aspect.tif')\n","        aspect_shp = os.path.join(aspect_output_dir, 'aspect.shp')\n","\n","        # Use gdal_polygonize to convert raster to vector (shapefile)\n","        with open(os.path.join(temporary_dir, 'polygon.sh'), 'w') as f3:\n","            print(f'gdal_polygonize.py \"{aspect_rast}\" \"{aspect_shp}\" -b 1 -f \"ESRI Shapefile\"', file=f3)\n","\n","        sh_file = os.path.join(temporary_dir, 'polygon.sh')\n","        subprocess.run(['bash', sh_file])\n","\n","        # Open and format the shapefile\n","        aspect_shp_gdf = gpd.read_file(aspect_shp)\n","        aspect_shp_gdf[\"O_ID_2\"] = list(range(1, (len(aspect_shp_gdf.index) + 1)))\n","        aspect_shp_gdf[\"Aspect\"] = aspect_shp_gdf.DN\n","\n","        # Save the final elevation band shapefile to drive\n","        aspect_shp_gdf.to_file(aspect_shp)\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) DEM Complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Bv1OmbzjWkuu"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Download DEM data** </font>\n","\n","#@markdown By default, Magpie is set up to download the MERIT DEM. However, by changing the data source, band name, and scale (information available in Earth Engine Data Catalog) users can download other DEM datasets available on Google Earth Engine\n","\n","#@markdown Please refer to the [Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets/catalog) to view other data source options\n","\n","#@markdown If Earth Engine cannot download the extent of your study area, increase size of scale\n","\n","#@markdown <font color=grey> for example, adjust scale to \"90\"\n","\n","# shapefile path\n","shp_file_path = os.path.join(main_dir, 'shapefile')\n","\n","data_source = \"MERIT/DEM/v1_0_3\" #@param {type:\"string\"}\n","\n","band_name = \"dem\" #@param {type:\"string\"}\n","\n","scale = 90 #@param\n","\n","# check projection\n","check_projection(shp_file_path, main_dir,temporary_dir)\n","# download DEM\n","download_DEM(shp_file_path, data_source, band_name, scale, main_dir, temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WVlR9FbBOGFv"},"outputs":[],"source":["#@markdown <font color=grey> **Clip DEM layer to study area** </font>\n","\n","format_and_visualize_dem(shp_file_path,temporary_dir,main_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"TnVpWA3nSBE8"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Check the box to visualize clipped DEM layer** </font>\n","\n","#@markdown Slope and aspect are computed from the DEM layer to provide users additional information about their study area </font> <br>\n","DEM_visualize = True #@param {type:\"boolean\"}\n","Aspect_visualize = True #@param {type:\"boolean\"}\n","\n","#@markdown Check box to save a shapefile layer of aspect to drive  </font> <br>\n","save_aspect = False #@param {type:\"boolean\"}\n","\n","if DEM_visualize == True:\n","  DEM_visualize = \"yes\"\n","else:\n","  DEM_visualize = \"no\"\n","if Aspect_visualize == True:\n","  Aspect_visualize = \"yes\"\n","else:\n","  Aspect_visualize = \"no\"\n","if save_aspect == True:\n","  save_aspect = \"yes\"\n","else:\n","  save_aspect = \"no\"\n","\n","visualize_dem(DEM_visualize, Aspect_visualize, save_aspect, main_dir, temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"LDXg1iu_fpts"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2Ow2szWN7E7x"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file == True:\n","  data = {\n","      \"_comment\": \"------------ 1b) DEM ---------------\",\n","      \"generate_DEM\": \"yes\",\n","      \"data_source_dem\": f\"{data_source}\",\n","      \"band_name_dem\": f\"{band_name}\",\n","      \"scale_dem\": f\"{scale}\",\n","      \"DEM_visualize\": f\"{DEM_visualize}\",\n","      \"Aspect_visualize\": f\"{Aspect_visualize}\",\n","      \"save_aspect\": f\"{save_aspect}\",\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"1b_dem.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"JJomczMSTnZv"},"source":["**References**\n","\n","Barnes, Richard. 2016. RichDEM: Terrain Analysis Software. http://github.com/r-barnes/richdem\n","\n","C Barnes, R., Lehman, C., Mulla, D., 2014. Priority-flood: An optimal depression-filling and watershed-labeling algorithm for digital elevation models. Computers & Geosciences 62, 117–127. doi:10.1016/j.cageo.2013.04.024\n","\n","Yamazaki, D., Ikeshima, D., Tawatari, R., Yamaguchi, T., O'Loughlin, F., Neal, J. C., ... & Bates, P. D. (2017). A high‐accuracy map of global terrain elevations. Geophysical Research Letters, 44(11), 5844-5853."]},{"cell_type":"markdown","metadata":{"id":"s6xO_kvDhSjn"},"source":["### <font color=grey> **Generate DEM - R elevatr Package** </font>\n","\n","Check out the short video [Creating DEM Layer in Magpie Workflow](https://youtu.be/S8rh7aovu_0) for more information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"AsSffEzgg-Ey"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Check the box to generate/visualize the following layers** </font>\n","\n","#@markdown Slope and aspect are computed from the DEM layer to provide users additional information about their study area </font> <br>\n","DEM_visualize = True #@param {type:\"boolean\"}\n","Slope_visualize = True #@param {type:\"boolean\"}\n","Aspect_visualize = True #@param {type:\"boolean\"}\n","\n","#@markdown Check box to save a shapefile layer of aspect to drive  </font> <br>\n","save_aspect = False #@param {type:\"boolean\"}\n","\n","# check libraries\n","libraries_to_check = [\"rpy2==3.5.1\",\"requests\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","%load_ext rpy2.ipython\n","\n","import requests\n","\n","# File path for output\n","output_file = \"/content/variable_info.txt\"\n","\n","# Writing to a file\n","with open(output_file, \"w\") as file:\n","    file.write(\"main_dir = '{}'\\n\".format(main_dir))\n","    file.write(\"DEM_visualize = '{}'\\n\".format(DEM_visualize))\n","    file.write(\"Slope_visualize = '{}'\\n\".format(Slope_visualize))\n","    file.write(\"Aspect_visualize = '{}'\\n\".format(Aspect_visualize))\n","    file.write(\"save_aspect_shp = '{}'\\n\".format(save_aspect))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"NjeBg29USswV"},"outputs":[],"source":["#@markdown <font color=grey> **Install and Load packages**\n","\n","%%R\n","\n","# Define the library path and packages to check\n","library_path <- \"/content/google_drive/MyDrive/R_Packages\"\n","packages <- c(\"terra\", \"sf\", \"slippymath\", \"raster\", \"s2\", \"elevatr\")\n","\n","# Create the library path if it doesn't exist\n","if (!dir.exists(library_path)) {\n","  dir.create(library_path, recursive = TRUE)\n","}\n","\n","# Function to check and install missing packages\n","check_and_install <- function(package_name) {\n","  if (!require(package_name, character.only = TRUE, lib.loc = library_path)) {\n","    install.packages(package_name, lib = library_path, repos = \"http://cran.r-project.org\")\n","    library(package_name, lib.loc = library_path, character.only = TRUE)\n","  }\n","}\n","\n","# Check and install each package\n","for (pkg in packages) {\n","  check_and_install(pkg)\n","}\n","\n","# Example of loading a specific library after the check\n","library(elevatr, lib.loc = library_path)\n","library(terra, lib.loc = library_path)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"L4fD_NjirdCt"},"outputs":[],"source":["#@markdown <font color=grey> **Set paths for variables**\n","\n","%%R\n","\n","# Define the file path\n","file_path <- \"/content/variable_info.txt\"\n","\n","# Read the file\n","lines <- readLines(file_path)\n","\n","# Initialize variables\n","main_dir <- NULL\n","DEM_visualize <- NULL\n","Slope_visualize <- NULL\n","Aspect_visualize <- NULL\n","save_aspect_shp <- NULL\n","\n","# Extract values from the lines\n","for (line in lines) {\n","  if (grepl(\"main_dir\", line)) {\n","    main_dir <- sub(\"main_dir = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"DEM_visualize\", line)) {\n","    DEM_visualize <- sub(\"DEM_visualize = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"Slope_visualize\", line)) {\n","    Slope_visualize <- sub(\"Slope_visualize = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"Aspect_visualize\", line)) {\n","    Aspect_visualize <- sub(\"Aspect_visualize = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"save_aspect_shp\", line)) {\n","    save_aspect_shp <- sub(\"save_aspect_shp = '(.*)'\", \"\\\\1\", line)\n","  }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5s0N9Kn0Ow8y"},"outputs":[],"source":["#@markdown <font color=grey> **Download DEM layer**\n","\n","%%R\n","\n","# Construct the full path to shapefile\n","shp_path <- file.path(main_dir, 'shapefile', 'studyArea_outline.shp')\n","\n","# Read the polygon shapefile\n","shp_file <- st_read(shp_path)\n","\n","# Fetch elevation data using elevatr\n","# Ensure the CRS of the shapefile is appropriate for the elevatr package (e.g., WGS84 or EPSG:4326)\n","study_area_elev <- st_transform(shp_file, crs = 4326)  # Transform to WGS84 if needed\n","shp_buffer <- st_buffer(study_area_elev, 500)\n","\n","# download DEM\n","elevation_data <- get_elev_raster(locations = shp_buffer,\n","                                  z = 10,  # Adjust zoom level for desired resolution\n","                                  prj = st_crs(shp_buffer)$wkt,\n","                                  clip = \"locations\")\n","\n","# overlay shapefile on DEM\n","cat('Overlay shapefile on DEM')\n","plot(elevation_data)\n","plot(st_geometry(study_area_elev), add = TRUE, col = \"black\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ivlm7cpoOxHq"},"outputs":[],"source":["#@markdown <font color=grey> **Save and visualization outputs**\n","\n","%%R\n","\n","# DEM\n","if (DEM_visualize == 'yes') {\n","  # Plot elevation data\n","  plot(elevation_data, main = \"Elevation\")\n","}\n","\n","# Construct the full path to DEM\n","elev_path <- file.path(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM', 'dem.tif')\n","\n","# Check if the directory exists, and create it if it doesn't\n","if (!dir.exists(elev_path)) {\n","  dir.create(elev_path, recursive = TRUE)\n","}\n","\n","    # Save the results as a file\n","writeRaster(elevation_data, elev_path, overwrite = TRUE)\n","\n","# compute slope\n","slope_raster <- terrain(elevation_data, opt = \"slope\", unit = \"degrees\")\n","\n","if (Slope_visualize == 'yes') {\n","  # plot\n","  plot(slope_raster, main = \"Slope (Degrees)\")\n","  # save\n","  slope_path <- file.path(main_dir, 'workflow_outputs', '1_HRU_data', 'Slope', 'slope.tif')\n","  writeRaster(slope_raster, slope_path, overwrite = TRUE)\n","}\n","\n","# compute aspect\n","aspect_raster <- terrain(elevation_data, opt = \"aspect\", unit = \"degrees\")\n","\n","if (Aspect_visualize == 'yes') {\n","  # plot\n","  plot(aspect_raster, main = \"Aspect (Degrees)\")\n","  # save\n","  aspect_path <- file.path(main_dir, 'workflow_outputs', '1_HRU_data', 'Aspect', 'aspect.tif')\n","  writeRaster(aspect_raster, aspect_path, overwrite = TRUE)\n","} else if (save_aspect_shp == 'yes') {\n","  # Convert aspect raster to polygons\n","  aspect_path_shp <- file.path(main_dir, 'workflow_outputs', '1_HRU_data', 'Aspect', 'aspect.shp')\n","  aspect_polygons <- as.polygons(terra::rast(aspect_raster))\n","  # Save slope polygons as a shapefile\n","  st_write(st_as_sf(aspect_polygons), aspect_path_shp, delete_layer = TRUE)\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fBuAi2CaOxKX"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","# delete temorary folder\n","os.remove('/content/variable_info.txt')"]},{"cell_type":"markdown","metadata":{"id":"jPCK2juUG1tP"},"source":["## <font color=#5559AB> 1c) Elevation Bands </font>\n","\n","The elevation band is a polygon shapefile derived from the DEM layer and is used for more accurate precipitation and temperature distributions in modelling. Users have the option to define the increment of the elevation bands.\n","\n","All that is required is a <font color=red>DEM layer</font> of the study area.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"yc11UpIQ0oL2"},"source":["### <font color=grey> **Upload Elevation Bands** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Gy5XmFaL0o-E"},"outputs":[],"source":["#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def upload_elev_bands(reclassify_dir, main_dir):\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Upload Elevation Band Shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the shapefile in the 'shapefile' directory\n","    shp_file_name = next((shp_file for shp_file in os.listdir(os.path.join(main_dir, 'shapefile')) if shp_file.endswith(\".shp\")), None)\n","\n","    # Find the name of the elevation band shapefile in the reclassify directory\n","    elevB_file_name = next((elevB_file for elevB_file in os.listdir(reclassify_dir) if elevB_file.endswith(\".shp\")), None)\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check Projection')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Load the study area shapefile\n","    shp_extent = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name))\n","\n","    # Load the elevation band shapefile\n","    elevB_extent = gpd.read_file(os.path.join(reclassify_dir, elevB_file_name))\n","\n","    if shp_extent.crs != elevB_extent.crs:\n","        # Reproject the elevation band layer to match the study area shapefile\n","        elevB_extent = elevB_extent.to_crs(shp_extent.crs)\n","        print('\\nElevation Band layer has been reprojected to match the shapefile')\n","    else:\n","        print('\\nCoordinate systems match')\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize and save elevation bands')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Display the table\n","    display(elevB_extent)\n","\n","    # Visualize elevation bands\n","    f, ax = plt.subplots(figsize=(9, 10))\n","    elevB_extent.plot(categorical=False, legend=True, ax=ax)\n","    ax.set(title=\"Elevation Bands\")\n","    ax.set_axis_off()\n","    plt.show()\n","\n","    # Save the final elevation band shapefile to drive\n","    elev_band_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Elevation_band')\n","    os.makedirs(elev_band_output_dir, exist_ok=True)\n","    elevB_extent.to_file(os.path.join(elev_band_output_dir, 'studyArea_elev_band.shp'))\n","\n","def remove_temp_data(temporary_dir):\n","    if os.path.exists(os.path.join(temporary_dir)):\n","      shutil.rmtree(os.path.join(temporary_dir))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"xUbZP_Fc1-WV"},"outputs":[],"source":["#@markdown <font color=grey> **Upload Elevation Band Layer** </font>\n","\n","#@markdown drag-and-drop the elevation band shapefile (.shp) layer into the specified folder\n","\n","# set directory/path\n","reclassify_dir = os.path.join(temporary_dir, 'elev_band')\n","if not os.path.exists(reclassify_dir):\n","  os.makedirs(reclassify_dir)\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('( ) Upload elevation band layer')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop elevation band = file into following folder: {dem_temp_dir}')\n","response = input(\"Have you uploaded the elevation band (.shp) file (yes or no): \")\n","if response == \"yes\":\n","  upload_elev_bands(reclassify_dir, main_dir)\n","  remove_temp_data(temporary_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XoUDdsNy3VeX"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------ 1c) ELEVATION BANDS ---------------\",\n","    \"generate_elev_bands\": \"no\",\n","    \"upload_elev_bands\": \"yes\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"1c_elevationBand.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"dVIW4K3v0ohT"},"source":["### <font color=grey> **Generate Elevation Bands** </font>\n","\n","Check out the short video [Creating Elevation Bands in Magpie Workflow](https://youtu.be/F5bH0wtPPhM) for more information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PTAPTMbfBzE_"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"zipfile\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import zipfile\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def min_max_elev(reclassify_dir, main_dir):\n","    \"\"\"\n","    Determine the minimum and maximum elevation of a Digital Elevation Model (DEM).\n","\n","    Args:\n","        reclassify_dir (str): Directory for temporary files, including the downloaded package.\n","        main_dir (str): Main directory path.\n","\n","    Returns:\n","        int: Maximum elevation of the DEM.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Determine minimum and maximum elevation')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Create reclassify_dir if it doesn't exist\n","    if not os.path.exists(reclassify_dir):\n","        os.makedirs(reclassify_dir)\n","\n","    # Download and extract the gdal_reclassify package\n","    reclassify_url = 'https://github.com/hburdett1/Magpie_Workflow_Developer/archive/refs/heads/main.zip'\n","    wget.download(reclassify_url, out=reclassify_dir)\n","\n","    # Unzip the downloaded package\n","    extension = \".zip\"\n","    os.chdir(reclassify_dir)\n","    for item in os.listdir(reclassify_dir):\n","        if item.endswith(extension):\n","            file_name = os.path.abspath(item)\n","            zip_ref = zipfile.ZipFile(file_name)\n","            zip_ref.extractall(reclassify_dir)\n","            zip_ref.close()\n","            os.remove(file_name)\n","\n","    # Find name of DEM file\n","    dem_path = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')\n","    for dem_file in os.listdir(os.path.join(dem_path)):\n","        if dem_file.endswith(\".tif\"):\n","            dem_file_name = dem_file\n","\n","    # Open DEM layer using rasterio and rioxarray\n","    src = rxr.open_rasterio(os.path.join(dem_path, dem_file_name), masked=True).squeeze()\n","\n","    # Determine and print minimum elevation of DEM layer\n","    dem_min = int(src.min())\n","    print('\\nMinimum Elevation: ', dem_min)\n","\n","    # Determine and print maximum elevation of DEM layer\n","    dem_max = int(src.max())\n","    print('Maximum Elevation: ', dem_max)\n","\n","    return dem_max\n","\n","def reclassify_dem(increment_val, min_elev, dem_max, reclassify_dir, main_dir, temporary_dir):\n","    \"\"\"\n","    Reclassify a Digital Elevation Model (DEM) based on elevation bands and convert it to a shapefile.\n","\n","    Args:\n","        increment_val (float): Increment value for elevation bands.\n","        min_elev (float): Minimum elevation value.\n","        dem_max (float): Maximum elevation value.\n","        reclassify_dir (str): Directory for temporary files, including the reclassify script.\n","        main_dir (str): Main directory path.\n","        temporary_dir (str): Temporary directory path.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Reclassify DEM')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Initialize variables for elevation bands\n","    min_val = min_elev\n","    inc_lst = ['<=0', f'<{min_elev}']\n","\n","    # Generate elevation bands\n","    while min_elev < (dem_max - increment_val):\n","        min_elev = min_elev + increment_val\n","        inc_lst.append(f'<{min_elev}')\n","\n","    # Join elevation bands for gdal_reclassify command\n","    band_range = ','.join(str(x) for x in inc_lst)\n","    print('Elevation Band Increments: ', band_range)\n","\n","    # Create band IDs\n","    id_vals = list(range(len(inc_lst)))\n","    band_id = ','.join(str(i) for i in id_vals)\n","    print('Number of Elevation Bands: ', band_id)\n","    print('\\n')\n","\n","    # Define paths for gdal commands\n","    gdal_reclassify = os.path.join(reclassify_dir, 'Magpie_Workflow_Developer-main', '1_Data_Collection','gdal_reclassify.py')\n","    elev_band_rast_path = os.path.join(reclassify_dir, 'elev_bands_rast.tif')\n","    elev_band_shp_path = os.path.join(reclassify_dir, 'elev_bands.shp')\n","\n","    # Find name of DEM file\n","    dem_path = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'DEM')\n","    for dem_file in os.listdir(dem_path):\n","        if dem_file.endswith(\".tif\"):\n","            dem_file_name = dem_file\n","    dem_path_full = os.path.join(dem_path, dem_file_name)\n","\n","    # Write bash command to reclassify\n","    with open(os.path.join(temporary_dir, 'reclass.sh'), 'w') as f1:\n","        print(f'python \"{gdal_reclassify}\" \"{dem_path_full}\" \"{elev_band_rast_path}\" -c \"{band_range}\" -r \"{band_id}\" -d 0 -n true -p \"COMPRESS=LZW\"', file=f1)\n","\n","    reclass_sh_file = os.path.join(temporary_dir, 'reclass.sh')\n","    subprocess.run(['bash', reclass_sh_file])\n","\n","    # Write bash command to convert raster to shapefile\n","    with open(os.path.join(temporary_dir, 'polygonize.sh'), 'w') as f2:\n","        print(f'gdal_polygonize.py \"{elev_band_rast_path}\" \"{elev_band_shp_path}\" -b 1 -f \"ESRI Shapefile\"', file=f2)\n","\n","    poly_sh_file = os.path.join(temporary_dir, 'polygonize.sh')\n","    subprocess.run(['bash', poly_sh_file])\n","\n","def visualize_elev_bands_func(increment_val, min_elev, temporary_dir, main_dir, dem_max):\n","    \"\"\"\n","    Visualize and save elevation bands based on a shapefile with elevation band information.\n","\n","    Args:\n","        increment_val (float): Increment value for elevation bands.\n","        min_elev (float): Minimum elevation value.\n","        temporary_dir (str): Temporary directory path.\n","        main_dir (str): Main directory path.\n","        dem_max (float): Maximum elevation value.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize and save elevation bands')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Load polygon shapefile\n","    elev_band_shp_path = os.path.join(temporary_dir, 'elev_band', 'elev_bands.shp')\n","    elev_band_shp = gpd.read_file(elev_band_shp_path)\n","\n","    # Aggregate elevation bands based on ID values\n","    elev_band_dissolve = elev_band_shp.dissolve(by='DN')\n","    elev_band_dissolve.to_file(os.path.join(temporary_dir, 'elev_bands_dissolved.shp'))\n","\n","    # Create lists of minimum and maximum contour values\n","    inc_min = [min_elev]\n","    while min_elev < (dem_max - increment_val):\n","        min_elev = min_elev + increment_val\n","        inc_min.append(min_elev)\n","    inc_min_lst = inc_min[:-1]\n","    max_inc = [max_val + increment_val for max_val in inc_min_lst]\n","\n","    print('\\nMinimum Contour Lines: ', inc_min_lst)\n","    print('Maximum Contour Lines: ', max_inc)\n","\n","    # Format shapefile with contour values\n","    elev_band_final = gpd.read_file(os.path.join(temporary_dir, 'elev_bands_dissolved.shp'))\n","    elev_band_final[\"O_ID_1\"] = list(range(1, (len(elev_band_final[\"DN\"]) + 1)))  # Define new ID column\n","    elev_band_final[\"ContourMin\"] = inc_min_lst  # Assign minimum contour values\n","    elev_band_final[\"ContourMax\"] = [max_val + increment_val for max_val in inc_min_lst]  # Create list of maximum contour values\n","    del elev_band_final[\"DN\"]  # Remove old ID column\n","\n","    # Display the table\n","    display(elev_band_final)\n","\n","    # Visualize elevation bands\n","    f, ax = plt.subplots(figsize=(9, 10))\n","    elev_band_shp.plot(column='DN', categorical=False, legend=True, ax=ax)\n","    ax.set(title=\"Elevation Bands\")\n","    ax.set_axis_off()\n","    plt.show()\n","\n","    # Save the final elevation band shapefile to drive\n","    elev_band_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Elevation_band')\n","    if not os.path.exists(elev_band_output_dir):\n","        os.makedirs(elev_band_output_dir)\n","    elev_band_final.to_file(os.path.join(elev_band_output_dir, 'studyArea_elev_band.shp'))\n","\n","    # Remove temporary directory\n","    shutil.rmtree(os.path.join(temporary_dir))\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Elevation Bands Complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yppHdOF9wu0W"},"outputs":[],"source":["#@markdown <font color=grey> **Determine minimum and maximum elevation** </font>\n","\n","# set directory/path\n","reclassify_dir = os.path.join(temporary_dir, 'elev_band')\n","if not os.path.exists(reclassify_dir):\n","  os.makedirs(reclassify_dir)\n","\n","dem_max = min_max_elev(reclassify_dir, main_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2F09hDttLhTi"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define minimum elevation value** </font><br>\n","#@markdown <font color=grey> ex, 100 </font>\n","\n","min_elev = 300 #@param\n","\n","#@markdown <font color=#5559AB> **Define increment of elevation band values** </font><br>\n","#@markdown <font color=grey> ex, 100 </font>\n","\n","increment_val = 100 #@param\n","\n","reclassify_dem(increment_val, min_elev, dem_max, reclassify_dir, main_dir, temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"x3dObR1zvdpq"},"outputs":[],"source":["#@markdown <font color=grey> **Visualize elevation bands** </font>\n","\n","visualize_elev_bands_func(increment_val, min_elev, temporary_dir, main_dir, dem_max)\n","print('Elevation band shapefile saved to drive')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"vcTJJBqqxKJ-"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hh2e4SIG5JG1"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file == True:\n","  data = {\n","      \"_comment\": \"------------ 1c) ELEVATION BANDS ---------------\",\n","      \"generate_elev_bands\": \"yes\",\n","      \"upload_elev_bands\": \"no\",\n","      \"min_elev\": f\"{min_elev}\",\n","      \"increment_val\": f\"{increment_val}\",\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"1c_elevationBand.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"ddqQn6b1SOU8"},"source":["## <font color=#5559AB> 1d) Landcover </font>\n","\n","The landcover layer is a polygon shapefile derived from the Moderate Resolution Imaging Spectroradiometer (MODIS) Land Cover Type ([MCD12Q1](https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MCD12Q1#description)) Version 6.1 data set available through Google Earth Engine. This dataset provides a harmonized view of the physical cover of Earth’s surface based on supervised classifications of MODIS Terra and Aqua reflectance data at yearly intervals.\n","\n","In this section, users can download the MODIS MCD12Q1 land cover product, clip, and save the landcover shapefile to their drive.\n","\n","Only a <font color=red>shapefile of the study area</font> is required to run this subsection."]},{"cell_type":"markdown","metadata":{"id":"JvmQG8S2D8Hg"},"source":["### <font color=grey> **Upload Landcover** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hBDK4zMcAahG"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"shapely\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","from shapely.geometry import box\n","from IPython.display import display\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def check_projection(shp_file_path, main_dir, temporary_dir):\n","    \"\"\"\n","    Check the projection of a shapefile and reproject if necessary.\n","\n","    Parameters:\n","    - shp_file_path: Path to the shapefile.\n","    - main_dir: Main directory containing the shapefile.\n","    - temporary_dir: Temporary directory for storing reprojected shapefiles.\n","\n","    Returns:\n","    - shp_file_name: Name of the shapefile.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check projection of shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find name of shapefile\n","    for shp_file in os.listdir(os.path.join(shp_file_path)):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    print('Shapefile name: ', shp_file_name)\n","    print('Shapefile path: ', os.path.join(main_dir, \"shapefile\", shp_file_name))\n","\n","    shp_lyr_check = gpd.read_file(os.path.join(main_dir, \"shapefile\", shp_file_name))\n","    print('Shapefile CRS: ', shp_lyr_check.crs)\n","\n","    temp_dir = os.path.join(temporary_dir)\n","    if not os.path.exists(temp_dir):\n","        os.makedirs(temp_dir)\n","\n","    if shp_lyr_check.crs != 'EPSG:4326':\n","        # Reproject\n","        shp_lyr_crs = shp_lyr_check.to_crs(epsg=4326)\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","        print('Shapefile layer has been reprojected to match shapefile')\n","    else:\n","        shp_lyr_crs = shp_lyr_check\n","        print('Coordinate systems match!')\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    return shp_file_name\n","\n","def overlay_shp_on_landcov(shp_file_path, shp_file_name, temporary_dir):\n","    \"\"\"\n","    Overlay shapefile on landcover data and visualize the result.\n","\n","    Parameters:\n","    - shp_file_path: Path to the shapefile.\n","    - shp_file_name: Name of the shapefile.\n","    - temporary_dir: Temporary directory for storing landcover data.\n","\n","    Returns:\n","    - landcov_lyr: Landcover layer.\n","    - crop_extent: Crop extent layer.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Overlay shapefile on landcover')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find name of raster\n","    for tif_file in os.listdir(os.path.join(temporary_dir, 'Landcover')):\n","        if tif_file.endswith(\".tif\"):\n","            tif_file_name = tif_file\n","\n","    # Open raster\n","    landcov_lyr = rxr.open_rasterio(os.path.join(temporary_dir, 'Landcover', tif_file_name), masked=True).squeeze()\n","    # Load shapefile\n","    crop_extent = gpd.read_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    print('Shapefile CRS: ', crop_extent.crs)\n","    print('Landcover CRS: ', landcov_lyr.rio.crs)\n","\n","    if crop_extent.crs != landcov_lyr.rio.crs:\n","        # Reproject\n","        landcov_lyr = landcov_lyr.rio.reproject(crop_extent.crs)\n","        print('Landcover layer has been reprojected to match the shapefile')\n","    else:\n","        print('Coordinate systems match!')\n","\n","    f, ax = plt.subplots(figsize=(10, 5))\n","    landcov_lyr.plot.imshow(ax=ax)\n","\n","    crop_extent.plot(ax=ax, alpha=.8, color=\"black\")\n","    ax.set(title=\"Raster Layer with Shapefile Overlayed\")\n","    ax.set_axis_off()\n","\n","    landcov_shapfile_visualization = True\n","\n","    if landcov_shapfile_visualization:\n","        plt.show()\n","\n","    return landcov_lyr, crop_extent\n","\n","def clip_and_format(landcov_dir, landcov_lyr, crop_extent, temporary_dir):\n","    \"\"\"\n","    Clip and format landcover into a shapefile.\n","\n","    Parameters:\n","    - landcov_dir: Directory to save the processed data.\n","    - landcov_lyr: Landcover layer to be clipped.\n","    - crop_extent: Study area extent boundary.\n","    - temporary_dir: Temporary directory to store intermediate files.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Clip and format landcover into shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Open crop extent (the study area extent boundary)\n","    crop_extent1 = crop_extent.buffer(.001)\n","\n","    # Clip the landcover layer\n","    lidar_clipped = landcov_lyr.rio.clip(crop_extent1, crop_extent1.crs)\n","    print('Landcover layer has been clipped')\n","\n","    # Save clipped landcover layer to drive\n","    path_to_tif_file = os.path.join(landcov_dir, 'clipped_lyr.tif')\n","    lidar_clipped.rio.to_raster(path_to_tif_file)\n","    print('Layer has been saved to temporary folder')\n","\n","    # Define pathways necessary for GDAL Commands\n","    clip_name = 'studyArea_outline'\n","    clipped = os.path.join(landcov_dir, 'clipped.tif')\n","    clipped_lyr = os.path.join(landcov_dir, 'clipped_lyr.tif')\n","    landcov_shp = os.path.join(landcov_dir, 'studyArea_landcov.shp')\n","    bash_dir = os.path.join(temporary_dir, 'bash_scripts')\n","\n","    # Create the bash directory if it doesn't exist\n","    if not os.path.exists(bash_dir):\n","        os.makedirs(bash_dir)\n","        print(\"Created folder: \", bash_dir)\n","\n","    # GDAL polygonize\n","    with open(os.path.join(bash_dir, 'polygon.sh'), 'w') as f3:\n","        print(f'gdal_polygonize.py \"{clipped_lyr}\" \"{landcov_shp}\" -b 1 -f \"ESRI Shapefile\"', file=f3)\n","\n","    # Define bash command path\n","    polygon_sh = os.path.join(bash_dir, 'polygon.sh')\n","\n","    # Run bash command\n","    subprocess.run(['bash', polygon_sh])\n","\n","def format_google_earth_data(main_dir, temporary_dir):\n","    \"\"\"\n","    Format landcover attribute names.\n","\n","    Parameters:\n","    - main_dir: Main directory where the workflow is located.\n","    - temporary_dir: Temporary directory to store intermediate files.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Format landcover attribute names')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    landcov_dir = os.path.join(temporary_dir, 'Landcover')\n","\n","    # Find the name of the shapefile\n","    for land_shp_file in os.listdir(landcov_dir):\n","        if land_shp_file.endswith(\".shp\"):\n","            land_shp_file_name = land_shp_file\n","\n","    landcov_shp = gpd.read_file(os.path.join(landcov_dir, land_shp_file_name))\n","    landcov_dissolve = landcov_shp.dissolve(by='DN')\n","    landcov_dissolve[\"Landuse_ID\"] = landcov_dissolve.index\n","    landcov_final = landcov_dissolve.reset_index()\n","    landcov_final = landcov_final.drop('DN', axis=1)\n","\n","    land_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","    if not os.path.exists(land_output_dir):\n","        os.makedirs(land_output_dir)\n","\n","    # Save the final landcover shapefile to drive\n","    landcov_final.to_file(os.path.join(land_output_dir, 'studyArea_landcover.shp'))\n","\n","def visualize_landcover(main_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize landcover outputs')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Create output directory if it doesn't exist\n","    landcov_dir_out = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","    if not os.path.exists(landcov_dir_out):\n","        os.makedirs(landcov_dir_out)\n","\n","    # Find name of shapefile in the output directory\n","    for land_shp_file in os.listdir(landcov_dir_out):\n","        if land_shp_file.endswith(\".shp\"):\n","            land_shp_file_name1 = land_shp_file\n","\n","    # Read the shapefile into a GeoDataFrame\n","    landcov_final = gpd.read_file(os.path.join(landcov_dir_out, land_shp_file_name1))\n","\n","    # Display GeoDataFrame table\n","    display(landcov_final)\n","\n","    # Plot landcover using GeoDataFrame\n","    f, ax = plt.subplots(figsize=(8, 9))\n","    landcov_final.plot(column='Landuse_ID', categorical=True, legend=True, ax=ax)\n","    ax.set_axis_off()\n","    plt.show()\n","\n","    # Calculate and display area for each landcover type\n","    landcov_final['area'] = landcov_final.area\n","    df_L1 = pd.DataFrame(landcov_final.drop(columns='geometry'))\n","    df_landcov = df_L1.groupby('Landuse_ID').sum()\n","    df_landcov.reset_index(inplace=True)\n","\n","    # Collect unique landcover IDs\n","    unique_landcov = df_landcov['Landuse_ID'].unique()\n","\n","    # Compute general area percentage for each landcover type\n","    for val_L in unique_landcov:\n","        area_poly_L = df_landcov.loc[df_landcov['Landuse_ID'] == val_L, 'area']\n","        landcov_area = float((area_poly_L / df_landcov['area'].sum()) * 100)\n","        print(f'ID ({val_L}): {round(landcov_area, 2)}%')\n","\n","    # Print section footer\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Landcover is complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def upload_landcover(land_temp_dir, main_dir, temporary_dir):\n","    # Find the name of the study area shapefile\n","    for shp_file in os.listdir(os.path.join(main_dir, 'shapefile')):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Find the name of the landcover shapefile\n","    for landcov_file in os.listdir(land_temp_dir):\n","        if landcov_file.endswith(\".shp\"):\n","            landcov_file_name = landcov_file\n","\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check Projection')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Load study area shapefile\n","    shp_extent = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name))\n","    # Load landcover shapefile\n","    landcov_extent = gpd.read_file(os.path.join(land_temp_dir, landcov_file_name))\n","\n","    # Check if the landcover shapefile has the required column 'Landuse_ID'\n","    if 'Landuse_ID' in landcov_extent:\n","        # Check if coordinate systems match, reproject if necessary\n","        if shp_extent.crs != landcov_extent.crs:\n","            # Reproject landcover layer to match the study area shapefile\n","            landcov_lyr = landcov_extent.to_crs(shp_extent.crs)\n","            print('\\nLandcover layer has been reprojected to match the shapefile')\n","        else:\n","            landcov_lyr = landcov_extent\n","            print('\\nCoordinate systems match')\n","\n","        # Print section header\n","        print('\\n-----------------------------------------------------------------------------------------------')\n","        print('( ) Visualize and save landcover layer')\n","        print('-----------------------------------------------------------------------------------------------')\n","\n","        # Display landcover GeoDataFrame table\n","        display(landcov_lyr)\n","\n","        # Visualize landcover\n","        f, ax = plt.subplots(figsize=(8, 9))\n","        landcov_lyr.plot(column='Landuse_ID', categorical=True, legend=True, ax=ax)\n","        ax.set_axis_off()\n","        plt.show()\n","\n","        # Calculate and display area for each landcover type\n","        landcov_lyr['area'] = landcov_lyr.area\n","        df_L1 = pd.DataFrame(landcov_lyr.drop(columns='geometry'))\n","        df_landcov = df_L1.groupby('Landuse_ID').sum()\n","        df_landcov.reset_index(inplace=True)\n","\n","        # Collect unique landcover IDs\n","        unique_landcov = df_landcov['Landuse_ID'].unique()\n","\n","        # Compute general area percentage for each landcover type\n","        for val_L in unique_landcov:\n","            area_poly_L = df_landcov.loc[df_landcov['Landuse_ID'] == val_L, 'area']\n","            landcov_area = float((area_poly_L / df_landcov['area'].sum()) * 100)\n","            print(f'ID ({val_L}): {round(landcov_area, 2)}%')\n","\n","        # Save the final landcover shapefile to drive\n","        landcov_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","        if not os.path.exists(landcov_output_dir):\n","            os.makedirs(landcov_output_dir)\n","        landcov_lyr.to_file(os.path.join(landcov_output_dir, 'studyArea_landcover.shp'))\n","    else:\n","        # Print an error message if the required column name is not found\n","        print('--- Invalid landuse column name ---\\n')\n","        print('Please change the landuse column name to Landuse_ID and run again')\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9rNEXvPzb3P2"},"outputs":[],"source":["#@markdown <font color=grey> **Upload Landcover File** </font>\n","\n","#@markdown Drag-and-drop the landcover file into the specified folder, both raster (.tif) and shapefile (.shp) files are accepted\n","\n","#@markdown If the layer is a raster, it will be clipped, formatted, and converted into a shapefile layer of the study area.\n","\n","#@markdown Is the layer is a shapefile, Magpie checks the required columns for BasinMaker are present and checks the projection\n","\n","\n","land_temp_dir = os.path.join(temporary_dir,'Landcover')\n","if not os.path.exists(land_temp_dir):\n","  os.makedirs(land_temp_dir)\n","\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('( ) Upload Landcover')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop landcover file into following folder: {land_temp_dir}')\n","response = input(\"Have you uploaded the landcover file (yes or no): \")\n","if response == \"yes\":\n","  for land_file in os.listdir(land_temp_dir):\n","    if land_file.endswith(\".tif\"):\n","      shp_file_path = os.path.join(main_dir, 'shapefile')\n","      # step 1\n","      shp_file_name = check_projection(shp_file_path,main_dir,temporary_dir)\n","      # step 2\n","      landcov_lyr, crop_extent = overlay_shp_on_landcov(shp_file_path, shp_file_name,temporary_dir)\n","      # step 3\n","      # define output directory\n","      landcov_dir = os.path.join(temporary_dir,'Landcover')\n","      if not os.path.exists(landcov_dir):\n","        os.makedirs(landcov_dir)\n","      clip_and_format(landcov_dir, landcov_lyr, crop_extent,temporary_dir)\n","      format_google_earth_data(main_dir,temporary_dir)\n","      # step 4\n","      visualize_landcover(main_dir)\n","      remove_temp_data(temporary_dir)\n","    if land_file.endswith(\".shp\"):\n","      upload_landcover(land_temp_dir,main_dir,temporary_dir)\n","      remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"n34Vl78OAZ7J"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------ 1d) LANDCOVER ---------------\",\n","    \"generate_landcover\": \"yes\",\n","    \"upload_landcover\": \"no\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"1d_landcover.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"8rb198U7hOC4"},"source":["### <font color=grey> **Generate Landcover** </font>\n","\n","Check out the short video [Generating Landcover Data with Magpie Workflow](https://youtu.be/JlAD33ox_wk) for more information."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mjIKFYBIAkva"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"requests\", \"shapely\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import ee\n","import requests\n","from shapely.geometry import box\n","from IPython.display import display\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","# Trigger the authentication flow.\n","service_account = 'magpie-developer@magpie-id-409519.iam.gserviceaccount.com'\n","credentials = ee.ServiceAccountCredentials(service_account, os.path.join(main_dir,'extras','magpie-key.json'))\n","\n","# Initialize the library.\n","ee.Initialize(credentials)\n","\n","def check_projection(shp_file_path, main_dir, temporary_dir):\n","    \"\"\"\n","    Check the projection of a shapefile and reproject if necessary.\n","\n","    Parameters:\n","    - shp_file_path: Path to the shapefile.\n","    - main_dir: Main directory containing the shapefile.\n","    - temporary_dir: Temporary directory for storing reprojected shapefiles.\n","\n","    Returns:\n","    - shp_file_name: Name of the shapefile.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check projection of shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find name of shapefile\n","    for shp_file in os.listdir(os.path.join(shp_file_path)):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    print('Shapefile name: ', shp_file_name)\n","    print('Shapefile path: ', os.path.join(main_dir, \"shapefile\", shp_file_name))\n","\n","    shp_lyr_check = gpd.read_file(os.path.join(main_dir, \"shapefile\", shp_file_name))\n","    print('Shapefile CRS: ', shp_lyr_check.crs)\n","\n","    temp_dir = os.path.join(temporary_dir)\n","    if not os.path.exists(temp_dir):\n","        os.makedirs(temp_dir)\n","\n","    if shp_lyr_check.crs != 'EPSG:4326':\n","        # Reproject\n","        shp_lyr_crs = shp_lyr_check.to_crs(epsg=4326)\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","        print('Shapefile layer has been reprojected to match shapefile')\n","    else:\n","        shp_lyr_crs = shp_lyr_check\n","        print('Coordinate systems match!')\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    return shp_file_name\n","\n","def download_landcover(shp_file_path, shp_file_name, data_source, year_of_interest, band_name, scale, temporary_dir):\n","    \"\"\"\n","    Download landcover data based on the given shapefile bounding box.\n","\n","    Parameters:\n","    - shp_file_path: Path to the shapefile.\n","    - shp_file_name: Name of the shapefile.\n","    - data_source: Source of landcover data.\n","    - year_of_interest: Year of the landcover data.\n","    - band_name: Name of the band.\n","    - scale: Scale of the download.\n","    - temporary_dir: Temporary directory for storing downloaded landcover data.\n","\n","    Returns:\n","    - landcov_dir: Directory path where the landcover data is stored.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Download Landcover')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","\n","    # Define buffer\n","    buffer_size = 0.3\n","\n","    # Determine the boundary of the provided shapefile\n","    bounds = gpd.read_file(os.path.join(temporary_dir, shp_file_name)).bounds\n","    west, south, east, north = bounds = bounds.loc[0]\n","    west -= buffer_size\n","    south -= buffer_size\n","\n","    print('Bounding box: ', west, south, east, north)\n","\n","    if data_source == 'USGS':\n","        full_data_source = \"USGS/NLCD_RELEASES/2020_REL/NALCMS\"\n","        band_name = 'landcover'\n","        year_of_interest = '2020'\n","    elif data_source == 'MODIS':\n","        full_data_source = f\"MODIS/061/MCD12Q1/{year_of_interest}_01_01\"\n","        band_name = 'LC_Type1'\n","\n","    # Concatenate input data to generate full path\n","    img = ee.Image(full_data_source)\n","    region = ee.Geometry.BBox(west, south, east, north)\n","\n","    # Multi-band GeoTIFF file.\n","    url = img.getDownloadUrl({\n","        'bands': band_name,\n","        'region': region,\n","        'scale': scale,\n","        'format': 'GEO_TIFF'\n","    })\n","\n","    # Define output directory\n","    landcov_dir = os.path.join(temporary_dir, 'Landcover')\n","    if not os.path.exists(landcov_dir):\n","        os.makedirs(landcov_dir)\n","\n","    response = requests.get(url)\n","    with open(os.path.join(landcov_dir, 'study_area_landcov.tif'), 'wb') as fd:\n","        fd.write(response.content)\n","\n","    # Path to clipped output file\n","    clipped_bounds = os.path.join(landcov_dir, 'study_area_landcov.tif')\n","    return landcov_dir\n","\n","def overlay_shp_on_landcov(shp_file_path, shp_file_name, temporary_dir):\n","    \"\"\"\n","    Overlay shapefile on landcover data and visualize the result.\n","\n","    Parameters:\n","    - shp_file_path: Path to the shapefile.\n","    - shp_file_name: Name of the shapefile.\n","    - temporary_dir: Temporary directory for storing landcover data.\n","\n","    Returns:\n","    - landcov_lyr: Landcover layer.\n","    - crop_extent: Crop extent layer.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Overlay shapefile on landcover')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find name of raster\n","    for tif_file in os.listdir(os.path.join(temporary_dir, 'Landcover')):\n","        if tif_file.endswith(\".tif\"):\n","            tif_file_name = tif_file\n","\n","    # Open raster\n","    landcov_lyr = rxr.open_rasterio(os.path.join(temporary_dir, 'Landcover', tif_file_name), masked=True).squeeze()\n","    # Load shapefile\n","    crop_extent = gpd.read_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    print('Shapefile CRS: ', crop_extent.crs)\n","    print('Landcover CRS: ', landcov_lyr.rio.crs)\n","\n","    if crop_extent.crs != landcov_lyr.rio.crs:\n","        # Reproject\n","        landcov_lyr = landcov_lyr.rio.reproject(crop_extent.crs)\n","        print('Landcover layer has been reprojected to match the shapefile')\n","    else:\n","        print('Coordinate systems match!')\n","\n","    f, ax = plt.subplots(figsize=(10, 5))\n","    landcov_lyr.plot.imshow(ax=ax)\n","\n","    crop_extent.plot(ax=ax, alpha=.8, color=\"black\")\n","    ax.set(title=\"Raster Layer with Shapefile Overlayed\")\n","    ax.set_axis_off()\n","\n","    landcov_shapfile_visualization = True\n","\n","    if landcov_shapfile_visualization:\n","        plt.show()\n","\n","    return landcov_lyr, crop_extent\n","\n","def clip_and_format(landcov_dir, landcov_lyr, crop_extent, temporary_dir):\n","    \"\"\"\n","    Clip and format landcover into a shapefile.\n","\n","    Parameters:\n","    - landcov_dir: Directory to save the processed data.\n","    - landcov_lyr: Landcover layer to be clipped.\n","    - crop_extent: Study area extent boundary.\n","    - temporary_dir: Temporary directory to store intermediate files.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Clip and format landcover into shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Open crop extent (the study area extent boundary)\n","    crop_extent1 = crop_extent.buffer(.001)\n","\n","    # Clip the landcover layer\n","    lidar_clipped = landcov_lyr.rio.clip(crop_extent1, crop_extent1.crs)\n","    print('Landcover layer has been clipped')\n","\n","    # Save clipped landcover layer to drive\n","    path_to_tif_file = os.path.join(landcov_dir, 'clipped_lyr.tif')\n","    lidar_clipped.rio.to_raster(path_to_tif_file)\n","    print('Layer has been saved to temporary folder')\n","\n","    # Define pathways necessary for GDAL Commands\n","    clip_name = 'studyArea_outline'\n","    clipped = os.path.join(landcov_dir, 'clipped.tif')\n","    clipped_lyr = os.path.join(landcov_dir, 'clipped_lyr.tif')\n","    landcov_shp = os.path.join(landcov_dir, 'studyArea_landcov.shp')\n","    bash_dir = os.path.join(temporary_dir, 'bash_scripts')\n","\n","    # Create the bash directory if it doesn't exist\n","    if not os.path.exists(bash_dir):\n","        os.makedirs(bash_dir)\n","        print(\"Created folder: \", bash_dir)\n","\n","    # GDAL polygonize\n","    with open(os.path.join(bash_dir, 'polygon.sh'), 'w') as f3:\n","        print(f'gdal_polygonize.py \"{clipped_lyr}\" \"{landcov_shp}\" -b 1 -f \"ESRI Shapefile\"', file=f3)\n","\n","    # Define bash command path\n","    polygon_sh = os.path.join(bash_dir, 'polygon.sh')\n","\n","    # Run bash command\n","    subprocess.run(['bash', polygon_sh])\n","\n","def format_google_earth_data(main_dir, temporary_dir):\n","    \"\"\"\n","    Format landcover attribute names.\n","\n","    Parameters:\n","    - main_dir: Main directory where the workflow is located.\n","    - temporary_dir: Temporary directory to store intermediate files.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Format landcover attribute names')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    landcov_dir = os.path.join(temporary_dir, 'Landcover')\n","\n","    # Find the name of the shapefile\n","    for land_shp_file in os.listdir(landcov_dir):\n","        if land_shp_file.endswith(\".shp\"):\n","            land_shp_file_name = land_shp_file\n","\n","    landcov_shp = gpd.read_file(os.path.join(landcov_dir, land_shp_file_name))\n","    landcov_dissolve = landcov_shp.dissolve(by='DN')\n","    landcov_dissolve[\"Landuse_ID\"] = landcov_dissolve.index\n","    landcov_final = landcov_dissolve.reset_index()\n","    landcov_final = landcov_final.drop('DN', axis=1)\n","\n","    land_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","    if not os.path.exists(land_output_dir):\n","        os.makedirs(land_output_dir)\n","\n","    # Save the final landcover shapefile to drive\n","    landcov_final.to_file(os.path.join(land_output_dir, 'studyArea_landcover.shp'))\n","\n","def visualize_landcover(main_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize landcover outputs')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Create output directory if it doesn't exist\n","    landcov_dir_out = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","    if not os.path.exists(landcov_dir_out):\n","        os.makedirs(landcov_dir_out)\n","\n","    # Find name of shapefile in the output directory\n","    for land_shp_file in os.listdir(landcov_dir_out):\n","        if land_shp_file.endswith(\".shp\"):\n","            land_shp_file_name1 = land_shp_file\n","\n","    # Read the shapefile into a GeoDataFrame\n","    landcov_final = gpd.read_file(os.path.join(landcov_dir_out, land_shp_file_name1))\n","\n","    # Display GeoDataFrame table\n","    display(landcov_final)\n","\n","    # Plot landcover using GeoDataFrame\n","    f, ax = plt.subplots(figsize=(8, 9))\n","    landcov_final.plot(column='Landuse_ID', categorical=True, legend=True, ax=ax)\n","    ax.set_axis_off()\n","    plt.show()\n","\n","    # Calculate and display area for each landcover type\n","    landcov_final['area'] = landcov_final.area\n","    df_L1 = pd.DataFrame(landcov_final.drop(columns='geometry'))\n","    df_landcov = df_L1.groupby('Landuse_ID').sum()\n","    df_landcov.reset_index(inplace=True)\n","\n","    # Collect unique landcover IDs\n","    unique_landcov = df_landcov['Landuse_ID'].unique()\n","\n","    # Compute general area percentage for each landcover type\n","    for val_L in unique_landcov:\n","        area_poly_L = df_landcov.loc[df_landcov['Landuse_ID'] == val_L, 'area']\n","        landcov_area = float((area_poly_L / df_landcov['area'].sum()) * 100)\n","        print(f'ID ({val_L}): {round(landcov_area, 2)}%')\n","\n","    # Print section footer\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Landcover is complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BomjXVsd2Shc"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Download  landcover data** </font>\n","\n","#@markdown Magpie offers two landcover options from Google Earth Engine:\n","\n","#@markdown >[USGS Landcover](https://developers.google.com/earth-engine/datasets/catalog/USGS_NLCD_RELEASES_2020_REL_NALCMS#bands) which is only available for 2020, 30m resolution\n","\n","#@markdown >[MODIS Landcover](https://developers.google.com/earth-engine/datasets/catalog/MODIS_061_MCD12Q1) data which is available at a yearly interval, 500m resolution\n","\n","#@markdown Users can change the data source, band name, and scale (information available in Earth Engine Data Catalog) users can download other landcover datasets available on Google Earth Engine\n","\n","#@markdown Please refer to the [Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets/catalog) to view other data source options\n","\n","#@markdown If Earth Engine cannot download the extent of your study area, increase size of scale\n","\n","#@markdown <font color=grey> for example, adjust scale to \"90\"\n","\n","# define paths\n","shp_file_path = os.path.join(main_dir, 'shapefile')\n","shp_file_name = check_projection(shp_file_path,main_dir,temporary_dir)\n","\n","# define input variables\n","data_source = \"USGS\" #@param [\"USGS\", \"MODIS\"]\n","\n","year_of_interest = \"2020\" #@param {type:\"string\"}\n","\n","band_name = \"landcover\" #@param {type:\"string\"}\n","\n","scale = 60 #@param\n","\n","# download landcover\n","landcov_dir = download_landcover(shp_file_path, shp_file_name, data_source,\n","                                 year_of_interest, band_name, scale, temporary_dir)\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"9ki2RjIshX0I"},"outputs":[],"source":["#@markdown <font color=grey> **Clip and Format** </font>\n","\n","landcov_dir = os.path.join(temporary_dir, 'Landcover')\n","\n","landcov_lyr, crop_extent = overlay_shp_on_landcov(shp_file_path, shp_file_name,temporary_dir)\n","# step 3\n","clip_and_format(landcov_dir, landcov_lyr, crop_extent,temporary_dir)\n","format_google_earth_data(main_dir,temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5i0WkhADb3T1"},"outputs":[],"source":["#@markdown <font color=grey> **Visualize Landcover Layer** </font>\n","\n","\n","# step 4\n","visualize_landcover(main_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fpJ7-4V8b3Z9"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"z6nV5Qy7CX1Y"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file == True:\n","  data = {\n","      \"_comment\": \"------------ 1d) LANDCOVER ---------------\",\n","      \"generate_landcover\": \"yes\",\n","      \"upload_landcover\": \"no\",\n","\n","      \"data_source_landcover\": f\"{data_source}\",\n","      \"year_of_interest\": f\"{year_of_interest}\",\n","      \"band_name_landcover\": f\"{band_name}\",\n","      \"scale_landcover\": scale,\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"1d_landcover.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"fAbbxdyAWuK4"},"source":["**References**\n","\n","Friedl, M. A., Sulla-Menashe, D., Tan, B., Schneider, A., Ramankutty, N., Sibley, A., et al.\n","(2010). MODIS collection 5 global land cover: Algorithm refinements and characterization of new datasets. Remote Sensing of Environment, 114, 168–182."]},{"cell_type":"markdown","metadata":{"id":"Q1ShqGk8eh2k"},"source":["## <font color=#5559AB> 1e) Soil </font>\n","\n","The soil layer is a polygon shapefile derived from the [soil texture classes](https://developers.google.com/earth-engine/datasets/catalog/OpenLandMap_SOL_SOL_CLAY-WFRACTION_USDA-3A1A1A_M_v02#description) (USDA system) data set derived from predicted soil texture fractions hosted on Google Earth Engine.\n","\n","Only a <font color=red>shapefile of the study area</font> is required to run this subsection."]},{"cell_type":"markdown","metadata":{"id":"vANU4a9PH8B1"},"source":["### <font color=grey> **Upload Soil** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FMW1BxZkDwGi"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"requests\", \"shapely\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import requests\n","from shapely.geometry import box\n","from IPython.display import display\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def check_projection(shp_file_path, main_dir, temporary_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check projection of shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the shapefile in the given path\n","    for shp_file in os.listdir(shp_file_path):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Print shapefile information\n","    print('Shapefile name: ', shp_file_name)\n","    print('Shapefile path: ', os.path.join(main_dir, \"shapefile\", shp_file_name))\n","\n","    # Read the shapefile into a GeoDataFrame to check its CRS\n","    shp_lyr_check = gpd.read_file(os.path.join(main_dir, \"shapefile\", shp_file_name))\n","    print('Shapefile CRS: ', shp_lyr_check.crs)\n","\n","    # Create a temporary directory if it doesn't exist\n","    temp_dir = os.path.join(temporary_dir)\n","    if not os.path.exists(temp_dir):\n","        os.makedirs(temp_dir)\n","\n","    # Check if the CRS is EPSG:4326, if not, reproject the shapefile\n","    if shp_lyr_check.crs != 'EPSG:4326':\n","        # Reproject the shapefile layer to match EPSG:4326\n","        shp_lyr_crs = shp_lyr_check.to_crs(epsg=4326)\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","        print('Shapefile layer has been reprojected to match EPSG:4326')\n","    else:\n","        shp_lyr_crs = shp_lyr_check\n","        print('Coordinate systems match!')\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    # Return the name of the reprojected shapefile\n","    return shp_file_name\n","\n","def overlay_shp_on_soil(temporary_dir, shp_file_name):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Overlay shapefile on soil')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the raster file in the temporary directory\n","    for tif_file in os.listdir(os.path.join(temporary_dir, 'Soil')):\n","        if tif_file.endswith(\".tif\"):\n","            tif_file_name = tif_file\n","\n","    # Open the soil raster\n","    soil_lyr = rxr.open_rasterio(os.path.join(temporary_dir, 'Soil', tif_file_name), masked=True).squeeze()\n","\n","    # Load the shapefile\n","    crop_extent = gpd.read_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    # Print CRS information\n","    print('Shapefile CRS: ', crop_extent.crs)\n","    print('Soil CRS: ', soil_lyr.rio.crs)\n","\n","    # Check if coordinate systems match, reproject if necessary\n","    if crop_extent.crs != soil_lyr.rio.crs:\n","        soil_lyr = soil_lyr.rio.reproject(crop_extent.crs)\n","        print('Soil layer has been reprojected to match the shapefile')\n","    else:\n","        print('Coordinate systems match!')\n","\n","    # Plot the overlay\n","    f, ax = plt.subplots(figsize=(10, 5))\n","    soil_lyr.plot.imshow(ax=ax)\n","    crop_extent.plot(ax=ax, alpha=0.8, color=\"black\")\n","    ax.set(title=\"Raster Layer with Shapefile Overlayed\")\n","    ax.set_axis_off()\n","\n","    soil_shapfile_visualization = True\n","\n","    if soil_shapfile_visualization:\n","        plt.show()\n","\n","    # Return the soil layer and crop extent\n","    return soil_lyr, crop_extent\n","\n","def clip_and_format(soil_dir, soil_lyr, crop_extent, temporary_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Clip and format soil into shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Clip soil product to the study area\n","    # Open the crop extent (the study area extent boundary)\n","    crop_extent_buffered = crop_extent.buffer(.001)\n","\n","    # Clip the soil layer\n","    soil_clipped = soil_lyr.rio.clip(crop_extent_buffered, crop_extent_buffered.crs)\n","    print('Soil layer has been clipped')\n","\n","    # Save the clipped soil layer to the temporary folder\n","    path_to_tif_file = os.path.join(soil_dir, 'clipped_lyr.tif')\n","    soil_clipped.rio.to_raster(path_to_tif_file)\n","    print('Layer has been saved to the temporary folder')\n","\n","    # Define necessary paths\n","    clip_name = 'studyArea_outline'\n","    clipped_lyr = os.path.join(soil_dir, 'clipped_lyr.tif')\n","    soil_shp = os.path.join(soil_dir, 'studyArea_soil.shp')\n","\n","    # Define the bash script directory\n","    bash_dir = os.path.join(temporary_dir, 'bash_scripts')\n","    if not os.path.exists(bash_dir):\n","        os.makedirs(bash_dir)\n","        print(\"Created folder: \", bash_dir)\n","\n","    # GDAL polygonize\n","    with open(os.path.join(bash_dir, 'polygon.sh'), 'w') as f3:\n","        print(f'gdal_polygonize.py \"{clipped_lyr}\" \"{soil_shp}\" -b 1 -f \"ESRI Shapefile\"', file=f3)\n","\n","    # Define the path to the bash command\n","    polygon_sh = os.path.join(bash_dir, 'polygon.sh')\n","\n","    # Run the bash command\n","    subprocess.run(['bash', polygon_sh])\n","\n","def visualize_soil(soil_dir, main_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize soil outputs')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the shapefile in the soil directory\n","    for soil_shp_file in os.listdir(soil_dir):\n","        if soil_shp_file.endswith(\".shp\"):\n","            soil_shp_file_name = soil_shp_file\n","\n","    # Read the soil shapefile\n","    soil_shp = gpd.read_file(os.path.join(soil_dir, soil_shp_file_name))\n","\n","    # Dissolve the soil shapefile by 'DN' field\n","    soil_dissolve = soil_shp.dissolve(by='DN')\n","    soil_dissolve[\"Soil_ID\"] = soil_dissolve.index\n","    soil_final = soil_dissolve.reset_index()\n","    soil_final = soil_final.drop('DN', axis=1)\n","\n","    # Display the dissolved soil shapefile\n","    display(soil_final)\n","\n","    # Create the output directory for soil shapefile\n","    soil_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Soil')\n","    if not os.path.exists(soil_output_dir):\n","        os.makedirs(soil_output_dir)\n","\n","    # Save the final soil shapefile to the output directory\n","    soil_final.to_file(os.path.join(soil_output_dir, 'studyArea_soil.shp'))\n","\n","    print('------------------------------------- Soil -------------------------------------')\n","\n","    # Read the final soil shapefile\n","    soil_final = gpd.read_file(os.path.join(soil_output_dir, 'studyArea_soil.shp'))\n","\n","    # Plot the soil shapefile\n","    f, ax = plt.subplots(figsize=(8, 9))\n","    soil_final.plot(column='Soil_ID', categorical=True, legend=True, ax=ax)\n","    ax.set_axis_off()\n","    plt.show()\n","\n","    # Calculate and print the percentage area for each soil type\n","    soil_final['area'] = soil_final.area\n","    df_S1 = pd.DataFrame(soil_final.drop(columns='geometry'))\n","    df_soil = df_S1.groupby('Soil_ID').sum()\n","    df_soil.reset_index(inplace=True)\n","\n","    # Collect unique soil IDs\n","    unique_soil = df_soil['Soil_ID'].unique()\n","    un_soi_len_lst = list(range(unique_soil.size))\n","\n","    for val_s in unique_soil:\n","        area_poly_S = df_soil.loc[df_soil['Soil_ID'] == val_s, 'area']\n","        soil_area = float((area_poly_S / df_soil['area'].sum()) * 100)\n","        for num in un_soi_len_lst:\n","            if val_s == unique_soil[num]:\n","                print(f'ID ({val_s}): {round(soil_area, 2)}%')\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Soil is complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def upload_soil(soil_temp_dir, main_dir):\n","    # Find the name of the shapefile in the shapefile directory\n","    for shp_file in os.listdir(os.path.join(main_dir, 'shapefile')):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Find the name of the shapefile in the soil temporary directory\n","    for soil_file in os.listdir(soil_temp_dir):\n","        if soil_file.endswith(\".shp\"):\n","            soil_file_name = soil_file\n","\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check Projection')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Load the study area shapefile\n","    shp_extent = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name))\n","\n","    # Load the soil shapefile\n","    soil_extent = gpd.read_file(os.path.join(soil_temp_dir, soil_file_name))\n","\n","    # Check if 'Soil_ID' column exists in soil shapefile\n","    if 'Soil_ID' in soil_extent:\n","        # Check if coordinate systems match, reproject if necessary\n","        if shp_extent.crs != soil_extent.crs:\n","            # Reproject soil layer to match the study area shapefile\n","            soil_lyr = soil_extent.to_crs(shp_extent.crs)\n","            print('\\nSoil layer has been reprojected to match the shapefile')\n","        else:\n","            soil_lyr = soil_extent\n","            print('\\nCoordinate systems match')\n","\n","        # Print section header for visualization\n","        print('\\n-----------------------------------------------------------------------------------------------')\n","        print('( ) Visualize and save soil layer')\n","        print('-----------------------------------------------------------------------------------------------')\n","\n","        # Display the soil layer table\n","        display(soil_lyr)\n","\n","        # Visualize the soil layer\n","        f, ax = plt.subplots(figsize=(8, 9))\n","        soil_lyr.plot(column='Soil_ID', categorical=True, legend=True, ax=ax)\n","        ax.set_axis_off()\n","        plt.show()\n","\n","        # Calculate the area for each soil type\n","        soil_lyr['area'] = soil_lyr.area\n","        df_S1 = pd.DataFrame(soil_lyr.drop(columns='geometry'))\n","        df_soil = df_S1.groupby('Soil_ID').sum()\n","        df_soil.reset_index(inplace=True)\n","\n","        # Collect unique soil IDs\n","        unique_soil = df_soil['Soil_ID'].unique()\n","        un_soi_len_lst = list(range(unique_soil.size))\n","\n","        # Print the percentage area for each soil type\n","        for val_s in unique_soil:\n","            area_poly_S = df_soil.loc[df_soil['Soil_ID'] == val_s, 'area']\n","            soil_area = float((area_poly_S / df_soil['area'].sum()) * 100)\n","            for num in un_soi_len_lst:\n","                if val_s == unique_soil[num]:\n","                    print(f'ID ({val_s}): {round(soil_area, 2)}%')\n","\n","        # Save the final soil shapefile to the output directory\n","        soil_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Soil')\n","        if not os.path.exists(soil_output_dir):\n","            os.makedirs(soil_output_dir)\n","        soil_lyr.to_file(os.path.join(soil_output_dir, 'studyArea_soil.shp'))\n","    else:\n","        print('--- Invalid soil column name ---\\n')\n","        print('Please change the soil column name to Soil_ID and run again')\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Z1wTl-rYHq03"},"outputs":[],"source":["#@markdown <font color=grey> **Upload Soil File** </font>\n","\n","#@markdown Drag-and-drop the soil file into the specified folder, both raster (.tif) and shapefile (.shp) files are accepted\n","\n","#@markdown If the layer is a raster, it will be clipped, formatted, and converted into a shapefile layer of the study area.\n","\n","#@markdown Is the layer is a shapefile, Magpie checks the required columns for BasinMaker are present and checks the projection\n","\n","# define the output directory\n","soil_dir = os.path.join(temporary_dir, 'Soil')\n","if not os.path.exists(soil_dir):\n","    os.makedirs(soil_dir)\n","# define temporary directory\n","soil_temp_dir = os.path.join(temporary_dir,'Soil')\n","if not os.path.exists(soil_temp_dir):\n","  os.makedirs(soil_temp_dir)\n","\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('( ) Upload Soil')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop soil file into following folder: {soil_temp_dir}')\n","response = input(\"Have you uploaded the soil file (yes or no): \")\n","if response == \"yes\":\n","  for soil_file in os.listdir(soil_temp_dir):\n","      if soil_file.endswith(\".tif\"):\n","        shp_file_path = os.path.join(main_dir, 'shapefile')\n","        # step 1\n","        shp_file_name = check_projection(shp_file_path, main_dir, temporary_dir)\n","        # step 2\n","        soil_lyr, crop_extent = overlay_shp_on_soil(temporary_dir, shp_file_name)\n","        # step 3\n","        clip_and_format(soil_dir, soil_lyr, crop_extent, temporary_dir)\n","        # step 4\n","        visualize_soil(soil_dir, main_dir)\n","        remove_temp_data(temporary_dir)\n","      elif soil_file.endswith(\".shp\"):\n","        upload_soil(soil_temp_dir, main_dir)\n","        remove_temp_data(temporary_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"j5TIvwLTGf6s"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------ 1e) SOIL ---------------\",\n","    \"generate_soil\": \"no\",\n","    \"upload_soil\": \"yes\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"1e_soil.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"SQNKEE60WwYq"},"source":["### <font color=grey> **Generate Soil** </font>\n","\n","Check out the short video [Handling Soil Data with Magpie Workflow](https://youtu.be/vobTEKVu9iw) for more information."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_m_J01tlGI9M"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"requests\", \"shapely\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import ee\n","import requests\n","from shapely.geometry import box\n","from IPython.display import display\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","# Trigger the authentication flow.\n","service_account = 'magpie-developer@magpie-id-409519.iam.gserviceaccount.com'\n","credentials = ee.ServiceAccountCredentials(service_account, os.path.join(main_dir,'extras','magpie-key.json'))\n","\n","# Initialize the library.\n","ee.Initialize(credentials)\n","\n","def check_projection(shp_file_path, main_dir, temporary_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check projection of shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the shapefile in the given path\n","    for shp_file in os.listdir(shp_file_path):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Print shapefile information\n","    print('Shapefile name: ', shp_file_name)\n","    print('Shapefile path: ', os.path.join(main_dir, \"shapefile\", shp_file_name))\n","\n","    # Read the shapefile into a GeoDataFrame to check its CRS\n","    shp_lyr_check = gpd.read_file(os.path.join(main_dir, \"shapefile\", shp_file_name))\n","    print('Shapefile CRS: ', shp_lyr_check.crs)\n","\n","    # Create a temporary directory if it doesn't exist\n","    temp_dir = os.path.join(temporary_dir)\n","    if not os.path.exists(temp_dir):\n","        os.makedirs(temp_dir)\n","\n","    # Check if the CRS is EPSG:4326, if not, reproject the shapefile\n","    if shp_lyr_check.crs != 'EPSG:4326':\n","        # Reproject the shapefile layer to match EPSG:4326\n","        shp_lyr_crs = shp_lyr_check.to_crs(epsg=4326)\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","        print('Shapefile layer has been reprojected to match EPSG:4326')\n","    else:\n","        shp_lyr_crs = shp_lyr_check\n","        print('Coordinate systems match!')\n","        shp_lyr_crs.to_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    # Return the name of the reprojected shapefile\n","    return shp_file_name\n","\n","def download_soil(shp_file_path, shp_file_name, data_source, band_name, scale, temporary_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Download Soil')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Define buffer size\n","    buffer_size = 0.3\n","\n","    # Determine the boundary of the provided shapefile\n","    bounds = gpd.read_file(os.path.join(temporary_dir, shp_file_name)).bounds\n","    west, south, east, north = bounds = bounds.loc[0]\n","    west -= buffer_size\n","    south -= buffer_size\n","    print('Bounding box: ', west, south, east, north)\n","\n","    # Concatenate input data to generate the full path\n","    #full_data_source = f'{data_source}/{year_of_interest}_01_01'\n","    full_data_source = f'{data_source}'\n","\n","    # Create an Earth Engine image\n","    img = ee.Image(full_data_source)\n","    region = ee.Geometry.BBox(west, south, east, north)\n","\n","    # Get the download URL for the image\n","    url = img.getDownloadUrl({\n","        'bands': [band_name],\n","        'region': region,\n","        'scale': scale,\n","        'format': 'GEO_TIFF'\n","    })\n","\n","    # Define the output directory\n","    soil_dir = os.path.join(temporary_dir, 'Soil')\n","    if not os.path.exists(soil_dir):\n","        os.makedirs(soil_dir)\n","\n","    # Download the image using the generated URL\n","    response = requests.get(url)\n","    with open(os.path.join(soil_dir, 'study_area_soil.tif'), 'wb') as fd:\n","        fd.write(response.content)\n","\n","    # Define paths\n","    clipped_bounds = os.path.join(soil_dir, 'study_area_soil.tif')\n","    soil_filled = os.path.join(soil_dir, 'soil_filled.tif')\n","\n","    # Define the directory for bash scripts\n","    bash_dir = os.path.join(temporary_dir, 'bash_scripts')\n","    if not os.path.exists(bash_dir):\n","        os.makedirs(bash_dir)\n","        print(\"Created folder:\", bash_dir)\n","\n","    # GDAL fill no data\n","    with open(os.path.join(bash_dir, 'fillnodata.sh'), 'w') as f1:\n","        print(f'gdal_fillnodata.py -md 10 -b 1 -of GTiff \"{clipped_bounds}\" \"{soil_filled}\"', file=f1)\n","\n","    # Format and run the bash command\n","    fill_data = os.path.join(bash_dir, 'fillnodata.sh')\n","    subprocess.run(['bash', fill_data])\n","\n","    # Remove the old unfilled soil layer\n","    os.remove(clipped_bounds)\n","\n","    return soil_dir\n","\n","def overlay_shp_on_soil(temporary_dir, shp_file_name):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Overlay shapefile on soil')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the raster file in the temporary directory\n","    for tif_file in os.listdir(os.path.join(temporary_dir, 'Soil')):\n","        if tif_file.endswith(\".tif\"):\n","            tif_file_name = tif_file\n","\n","    # Open the soil raster\n","    soil_lyr = rxr.open_rasterio(os.path.join(temporary_dir, 'Soil', tif_file_name), masked=True).squeeze()\n","\n","    # Load the shapefile\n","    crop_extent = gpd.read_file(os.path.join(temporary_dir, shp_file_name))\n","\n","    # Print CRS information\n","    print('Shapefile CRS: ', crop_extent.crs)\n","    print('Soil CRS: ', soil_lyr.rio.crs)\n","\n","    # Check if coordinate systems match, reproject if necessary\n","    if crop_extent.crs != soil_lyr.rio.crs:\n","        soil_lyr = soil_lyr.rio.reproject(crop_extent.crs)\n","        print('Soil layer has been reprojected to match the shapefile')\n","    else:\n","        print('Coordinate systems match!')\n","\n","    # Plot the overlay\n","    f, ax = plt.subplots(figsize=(10, 5))\n","    soil_lyr.plot.imshow(ax=ax)\n","    crop_extent.plot(ax=ax, alpha=0.8, color=\"black\")\n","    ax.set(title=\"Raster Layer with Shapefile Overlayed\")\n","    ax.set_axis_off()\n","\n","    soil_shapfile_visualization = True\n","\n","    if soil_shapfile_visualization:\n","        plt.show()\n","\n","    # Return the soil layer and crop extent\n","    return soil_lyr, crop_extent\n","\n","def clip_and_format(soil_dir, soil_lyr, crop_extent, temporary_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Clip and format soil into shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Clip soil product to the study area\n","    # Open the crop extent (the study area extent boundary)\n","    crop_extent_buffered = crop_extent.buffer(.001)\n","\n","    # Clip the soil layer\n","    soil_clipped = soil_lyr.rio.clip(crop_extent_buffered, crop_extent_buffered.crs)\n","    print('Soil layer has been clipped')\n","\n","    # Save the clipped soil layer to the temporary folder\n","    path_to_tif_file = os.path.join(soil_dir, 'clipped_lyr.tif')\n","    soil_clipped.rio.to_raster(path_to_tif_file)\n","    print('Layer has been saved to the temporary folder')\n","\n","    # Define necessary paths\n","    clip_name = 'studyArea_outline'\n","    clipped_lyr = os.path.join(soil_dir, 'clipped_lyr.tif')\n","    soil_shp = os.path.join(soil_dir, 'studyArea_soil.shp')\n","\n","    # Define the bash script directory\n","    bash_dir = os.path.join(temporary_dir, 'bash_scripts')\n","    if not os.path.exists(bash_dir):\n","        os.makedirs(bash_dir)\n","        print(\"Created folder: \", bash_dir)\n","\n","    # GDAL polygonize\n","    with open(os.path.join(bash_dir, 'polygon.sh'), 'w') as f3:\n","        print(f'gdal_polygonize.py \"{clipped_lyr}\" \"{soil_shp}\" -b 1 -f \"ESRI Shapefile\"', file=f3)\n","\n","    # Define the path to the bash command\n","    polygon_sh = os.path.join(bash_dir, 'polygon.sh')\n","\n","    # Run the bash command\n","    subprocess.run(['bash', polygon_sh])\n","\n","def visualize_soil(soil_dir, main_dir):\n","    # Print section header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize soil outputs')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the name of the shapefile in the soil directory\n","    for soil_shp_file in os.listdir(soil_dir):\n","        if soil_shp_file.endswith(\".shp\"):\n","            soil_shp_file_name = soil_shp_file\n","\n","    # Read the soil shapefile\n","    soil_shp = gpd.read_file(os.path.join(soil_dir, soil_shp_file_name))\n","\n","    # Dissolve the soil shapefile by 'DN' field\n","    soil_dissolve = soil_shp.dissolve(by='DN')\n","    soil_dissolve[\"Soil_ID\"] = soil_dissolve.index\n","    soil_final = soil_dissolve.reset_index()\n","    soil_final = soil_final.drop('DN', axis=1)\n","\n","    # Display the dissolved soil shapefile\n","    display(soil_final)\n","\n","    # Create the output directory for soil shapefile\n","    soil_output_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Soil')\n","    if not os.path.exists(soil_output_dir):\n","        os.makedirs(soil_output_dir)\n","\n","    # Save the final soil shapefile to the output directory\n","    soil_final.to_file(os.path.join(soil_output_dir, 'studyArea_soil.shp'))\n","\n","    print('------------------------------------- Soil -------------------------------------')\n","\n","    # Read the final soil shapefile\n","    soil_final = gpd.read_file(os.path.join(soil_output_dir, 'studyArea_soil.shp'))\n","\n","    # Plot the soil shapefile\n","    f, ax = plt.subplots(figsize=(8, 9))\n","    soil_final.plot(column='Soil_ID', categorical=True, legend=True, ax=ax)\n","    ax.set_axis_off()\n","    plt.show()\n","\n","    # Calculate and print the percentage area for each soil type\n","    soil_final['area'] = soil_final.area\n","    df_S1 = pd.DataFrame(soil_final.drop(columns='geometry'))\n","    df_soil = df_S1.groupby('Soil_ID').sum()\n","    df_soil.reset_index(inplace=True)\n","\n","    # Collect unique soil IDs\n","    unique_soil = df_soil['Soil_ID'].unique()\n","    un_soi_len_lst = list(range(unique_soil.size))\n","\n","    for val_s in unique_soil:\n","        area_poly_S = df_soil.loc[df_soil['Soil_ID'] == val_s, 'area']\n","        soil_area = float((area_poly_S / df_soil['area'].sum()) * 100)\n","        for num in un_soi_len_lst:\n","            if val_s == unique_soil[num]:\n","                print(f'ID ({val_s}): {round(soil_area, 2)}%')\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Soil is complete!')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4Az2GJCzH7h3"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Download OpenLandMap Soil Texture Class data** </font>\n","\n","#@markdown By default, Magpie is set up to download OpenLandMap Soil Texture Class data which is available at a yearly interval. However, by changing the data source, band name, and scale (information available in Earth Engine Data Catalog) users can download other soil datasets available on Google Earth Engine\n","\n","#@markdown Please refer to the [Earth Engine Data Catalog](https://developers.google.com/earth-engine/datasets/catalog) to view other data source options\n","\n","#@markdown Note** - users may need to adjust the scale size to 90, if downloading for a larger study area\n","\n","data_source = \"OpenLandMap/SOL/SOL_TEXTURE-CLASS_USDA-TT_M/v02\" #@param {type:\"string\"}\n","\n","#year_of_interest = 2000 #@param {type:\"string\"}\n","\n","band_name = \"b0\" #@param {type:\"string\"}\n","\n","scale = 90 #@param\n","\n","# define path\n","shp_file_path = os.path.join(main_dir, 'shapefile')\n","\n","# determine shapefile name\n","shp_file_name = check_projection(shp_file_path, main_dir, temporary_dir)\n","\n","soil_dir = download_soil(shp_file_path, shp_file_name, data_source, band_name, scale, temporary_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Jxd6EzDWH7kj"},"outputs":[],"source":["#@markdown <font color=grey> **Clip and Format** </font>\n","\n","soil_lyr, crop_extent = overlay_shp_on_soil(temporary_dir, shp_file_name)\n","# step 3\n","clip_and_format(soil_dir, soil_lyr, crop_extent, temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"aE0qYwy9H7pH"},"outputs":[],"source":["#@markdown <font color=grey> **Visualize Landcover Layer** </font>\n","\n","visualize_soil(soil_dir, main_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wSP7tKMqZUFK"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2S2TtmFvGhEH"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file == True:\n","  data = {\n","      \"_comment\": \"------------ 1e) SOIL ---------------\",\n","      \"generate_soil\": \"yes\",\n","      \"upload_soil\": \"no\",\n","      \"data_source_landcover\": f\"{data_source}\",\n","      \"band_name_landcover\": f\"{band_name}\",\n","      \"scale_landcover\": scale,\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"1e_soil.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"eEN-kKdZYDRJ"},"source":["**References**\n","\n","Hengl, T. (2018). Clay Content in%(Kg/Kg) at 6 Standard Depths (0, 10, 30, 60, 100 and 200 Cm) at 250 M Resolution (Version v02)[Data Set]."]},{"cell_type":"markdown","metadata":{"id":"lEzzcRumhD4E"},"source":["## <font color=#5559AB> 1f) Classification </font>\n","\n","This section allows users to define land cover, vegetation, and soil classifications. The data is saved in a CSV file and is used to classify the RVH and RVP Raven model files.\n","\n","Users can choose a classification option from the drop bar (arrow) or double-click to create their own classifications.\n"]},{"cell_type":"markdown","metadata":{"id":"bVEZtNvBWepv"},"source":["### <font color=grey> **Upload Classifications** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"qVVuVKFHXiui"},"outputs":[],"source":["#@markdown <font color=grey> **Upload Landcover Classification** </font> <br>\n","\n","#@markdown Here users can upload a landcover classification (.csv) of their study area\n","\n","# define landcover directory\n","land_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","# create if it does note exists\n","if not os.path.exists(land_dir):\n","    os.makedirs(land_dir)\n","    print(\"created  folder: \", land_dir)\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('Upload landcover classification')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'Please drag-and-drop the file into the following folder: {land_dir}')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"x8wMhNRwWgXv"},"outputs":[],"source":["#@markdown <font color=grey> **Upload Vegetation Classification** </font> <br>\n","\n","#@markdown Here users can upload a vegetation classification (.csv) of their study area\n","\n","# define vegetation directory\n","veg_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Vegetation')\n","# create if it does note exists\n","if not os.path.exists(veg_dir):\n","    os.makedirs(veg_dir)\n","    print(\"created  folder: \", veg_dir)\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('Upload vegetation classification')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'Please drag-and-drop the file into the following folder: {veg_dir}')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"T9OXgmPOYoHx"},"outputs":[],"source":["#@markdown <font color=grey> **Upload Soil Classification** </font> <br>\n","\n","#@markdown Here users can upload a soil classification (.csv) of their study area\n","\n","# define soil directory\n","soil_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Soil')\n","# create if it does note exists\n","if not os.path.exists(soil_dir):\n","    os.makedirs(soil_dir)\n","    print(\"created  folder: \", soil_dir)\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('Upload vegetation classification')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'Please drag-and-drop the file into the following folder: {soil_dir}')"]},{"cell_type":"markdown","metadata":{"id":"UR7hlhCHWetb"},"source":["### <font color=grey> **Generate Classifications** </font>\n","\n","Check out the short video [Preparing Data Classifications with Magpie Workflow](https://youtu.be/W9udFXIZNyw) for more information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"7KPNRXm3Wqim"},"outputs":[],"source":["#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def landcover_classification(main_dir, landuse_ID, landuse_Classifications):\n","    \"\"\"\n","    Classify landcover based on provided information.\n","\n","    Parameters:\n","    - main_dir (str): Main directory path.\n","    - landuse_ID (str): Comma-separated string of landuse IDs.\n","    - landuse_Classifications (str): Comma-separated string of landuse classifications.\n","    \"\"\"\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Landcover classification')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    land_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","\n","    if not os.path.exists(land_dir):\n","        os.makedirs(land_dir)\n","\n","    if landuse_ID == \"NA\":\n","        print('Please upload landcover csv file to: ', land_dir)\n","    else:\n","        landID_val = list(landuse_ID.split(\",\"))\n","        landclass_val = list(landuse_Classifications.split(\",\"))\n","\n","        land_class = {\n","            'Landuse_ID': landID_val,\n","            'LAND_USE_C': landclass_val\n","        }\n","\n","        land_class_df = pd.DataFrame(land_class)\n","        land_class_df['Landuse_ID'] = land_class_df['Landuse_ID'].astype(str)\n","\n","        print(land_class_df)\n","\n","        land_class_df.to_csv(os.path.join(land_dir, 'landcover_info.csv'), index=False)\n","\n","def vegetation_classification(main_dir, veg_ID, veg_Classifications):\n","    \"\"\"\n","    Classify vegetation based on provided information.\n","\n","    Parameters:\n","    - main_dir (str): Main directory path.\n","    - veg_ID (str): Comma-separated string of vegetation IDs.\n","    - veg_Classifications (str): Comma-separated string of vegetation classifications.\n","    \"\"\"\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Vegetation classification')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    veg_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Landcover')\n","\n","    if not os.path.exists(veg_dir):\n","        os.makedirs(veg_dir)\n","\n","    if veg_ID == \"NA\":\n","        print('Please upload vegetation csv file to: ', veg_dir)\n","    else:\n","        vegID_val = list(veg_ID.split(\",\"))\n","        vegClass_val = list(veg_Classifications.split(\",\"))\n","\n","        veg_class = {\n","            'Veg_ID':  vegID_val,\n","            'VEG_C': vegClass_val\n","        }\n","\n","        veg_class_df = pd.DataFrame(veg_class)\n","        veg_class_df['Veg_ID'] = veg_class_df['Veg_ID'].astype(str)\n","\n","        print(veg_class_df)\n","\n","        veg_class_df.to_csv(os.path.join(veg_dir, 'veg_info.csv'), index=False)\n","\n","def soil_classification(main_dir, soil_ID, soil_Classifications):\n","    \"\"\"\n","    Classify soil based on provided information.\n","\n","    Parameters:\n","    - main_dir (str): Main directory path.\n","    - soil_ID (str): Comma-separated string of soil IDs.\n","    - soil_Classifications (str): Comma-separated string of soil classifications.\n","    \"\"\"\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Soil classification')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    soil_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'Soil')\n","\n","    if not os.path.exists(soil_dir):\n","        os.makedirs(soil_dir)\n","\n","    if soil_ID == \"NA\":\n","        print('Please upload soil csv file to: ', soil_dir)\n","    else:\n","        soilID_val = list(soil_ID.split(\",\"))\n","        soilClass_val = list(soil_Classifications.split(\",\"))\n","\n","        soil_class = {\n","            'Soil_ID':  soilID_val,\n","            'SOIL_PROF': soilClass_val\n","        }\n","\n","        soil_class_df = pd.DataFrame(soil_class)\n","        soil_class_df.to_csv(os.path.join(soil_dir, 'soil_info.csv'), index=False)\n","\n","        print(soil_class_df)"]},{"cell_type":"markdown","metadata":{"id":"Iz-31jqhgyRV"},"source":["#### <font color=#5559AB> **Generate Landcover Classification Table** </font>"]},{"cell_type":"markdown","metadata":{"id":"41ytbQP-lEDa"},"source":["##### <font color=grey> **USGS Landcover Classification Table** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fR_jYd-9g10i"},"outputs":[],"source":["#@markdown **Define landcover classification ID's:**\n","\n","#@markdown include -1 for BasinMaker for lake classification\n","landuse_ids = '1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,-1' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","#@markdown **Define landcover classification's:**\n","landuse_classifications = 'FOREST,FOREST,FOREST,FOREST,FOREST,FOREST,SHRUBLAND,SHRUBLAND,GRASSLAND,GRASSLAND,SHRUBLAND,GRASSLAND,BARREN,WETLAND,CROPLAND,BARREN,URBAN,WATER,WATER,LAKE' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","# run function\n","landcover_classification(main_dir, landuse_ids, landuse_classifications)"]},{"cell_type":"markdown","metadata":{"id":"iCAfcvarlPA5"},"source":["##### <font color=grey> **MODIS Landcover Classification Table** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"dvWMvgZ_Jh3t"},"outputs":[],"source":["#@markdown **Define landcover classification ID's:**\n","landuse_ids = '1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,-1' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","#@markdown **Define landcover classification's:**\n","landuse_classifications = 'FOREST,FOREST,FOREST,FOREST,FOREST,SHRUBLAND,SHRUBLAND,GRASSLAND,GRASSLAND,GRASSLAND,WETLAND,CROPLAND,BARREN,URBAN,CROPLAND,WATER,WATER,LAKE' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","# run function\n","landcover_classification(main_dir, landuse_ids, landuse_classifications)\n"]},{"cell_type":"markdown","metadata":{"id":"nKvQuv0-lRhK"},"source":["##### <font color=grey> **Other Landcover Type Classification Table** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FHJ1S08ilY4p"},"outputs":[],"source":["#@markdown **Define landcover classification ID's:**\n","landuse_ids = '1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,-1' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","#@markdown **Define landcover classification's:**\n","landuse_classifications = 'FOREST,FOREST,FOREST,FOREST,FOREST,SHRUBLAND,SHRUBLAND,GRASSLAND,GRASSLAND,GRASSLAND,WETLAND,CROPLAND,BARREN,URBAN,CROPLAND,WATER,WATER,LAKE' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","# run function\n","landcover_classification(main_dir, landuse_ids, landuse_classifications)"]},{"cell_type":"markdown","metadata":{"id":"cvkBInUglhCQ"},"source":["#### <font color=#5559AB> **Generate Vegetation Classification Table** </font>"]},{"cell_type":"markdown","metadata":{"id":"5wXu6zxQFjgj"},"source":["##### <font color=grey> **USGS Vegetation Classification Table** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lPMpImtNFjgj"},"outputs":[],"source":["#@markdown **Define vegetation classification ID's:**\n","\n","#@markdown include -1 for BasinMaker for lake classification\n","veg_ids = '1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,-1' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","#@markdown **Define vegetation classification's:**\n","veg_classifications = 'FOREST,FOREST,FOREST,FOREST,FOREST,FOREST,SHRUBLAND,SHRUBLAND,GRASSLAND,GRASSLAND,SHRUBLAND,GRASSLAND,BARREN,WETLAND,CROPLAND,BARREN,URBAN,WATER,WATER,LAKE' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","# run function\n","vegetation_classification(main_dir, veg_ids, veg_classifications)"]},{"cell_type":"markdown","metadata":{"id":"t80YAQ52Fjgk"},"source":["##### <font color=grey> **MODIS Vegetation Classification Table** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zaTkJbbaFjgk"},"outputs":[],"source":["#@markdown **Define vegetation classification ID's:**\n","veg_ids = '1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,-1' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","#@markdown **Define vegetation classification's:**\n","veg_classifications = 'FOREST,FOREST,FOREST,FOREST,FOREST,SHRUBLAND,SHRUBLAND,GRASSLAND,GRASSLAND,GRASSLAND,WETLAND,CROPLAND,BARREN,URBAN,CROPLAND,WATER,WATER,LAKE' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","# run function\n","vegetation_classification(main_dir, veg_ids, veg_classifications)\n"]},{"cell_type":"markdown","metadata":{"id":"JvDXPOTZFjgk"},"source":["##### <font color=grey> **Other Landcover Type Classification Table** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YryRWlOIFjgk"},"outputs":[],"source":["#@markdown **Define vegetation classification ID's:**\n","veg_ids = '1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,-1' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","#@markdown **Define vegetation classification's:**\n","veg_classifications = 'FOREST,FOREST,FOREST,FOREST,FOREST,SHRUBLAND,SHRUBLAND,GRASSLAND,GRASSLAND,GRASSLAND,WETLAND,CROPLAND,BARREN,URBAN,CROPLAND,WATER,WATER,LAKE' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","# run function\n","vegetation_classification(main_dir, veg_ids, veg_classifications)"]},{"cell_type":"markdown","metadata":{"id":"Zp6jN05MloQ0"},"source":["#### <font color=#5559AB> **Generate Soil Classification Table** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"XVE6auHYJh8L"},"outputs":[],"source":["#@markdown **Define soil classification ID's:**\n","soil_ids = '1,2,3,4,5,6,7,8,9,10,11,12,-1' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","#@markdown **Define soil classification's:**\n","soil_classifications = 'SAND,LOAMY_SAND,SANDY_LOAM,SILT_LOAM,SILT,LOAM,SANDY_CLAY_LOAM,SILT_CLAY_LOAM,CLAY_LOAM,SANDY_CLAY,SILT_CLAY,CLAY,LAKE' # @param {type:\"string\"}\n","#@markdown _include commas in between values, no spaces_\n","\n","# run function\n","soil_classification(main_dir, soil_ids, soil_classifications)\n"]},{"cell_type":"markdown","metadata":{"id":"ag21BG_llvv8"},"source":["#### <font color=grey> **Write Model Decisions to Configuration File**"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WLwGTG5VVMFO"},"outputs":[],"source":["#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file == True:\n","  data = {\n","      \"_comment\": \"------------ 1f) CLASSIFICATION ---------------\",\n","      \"_comment\": \"--- Landcover ---\",\n","      \"landuse_ID\": f\"{landuse_ids}\",\n","      \"landuse_Classifications\": f\"{landuse_classifications}\",\n","\n","      \"_comment\": \"--- Vegetation ---\",\n","      \"veg_ID\": f\"{veg_ids}\",\n","      \"veg_Classifications\": f\"{veg_classifications}\",\n","\n","      \"_comment\": \"--- Soil ---\",\n","      \"soil_ID\": f\"{soil_ids}\",\n","      \"soil_Classifications\": f\"{soil_classifications}\",\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"1f_classification.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"oZCkPMOqkQeT"},"source":["# **2.0 Discretize Basin**\n","\n","Semi-distributed/distributed hydrological models are built on the concept of the hydrological response unit (HRU). An HRU is the smallest unit containing unique geospatial information. Thus, defining HRUs is the process to overlay and union different geospatial data layers, such as land cover, soil, and elevation band.\n","\n","To do so, Magpie utilizes the light version of BasinMaker, developed by Han et al. (2021), to generate HRUs. Please visit the [BasinMaker website](http://hydrology.uwaterloo.ca/basinmaker/) for more information\n","\n","Inputs required for BasinMaker to generate HRUs are flexible and include:\n","*   DEM\n","*   elevation bands\n","*   aspect\n","*   landcover\n","*   soil layers\n","\n","Be sure to visit section \"2.6 Classification\" to define the landcover, vegetation, and soil classifications that are stored in a CSV and used in the RVH and RVP Raven model files.\n","\n","If users are planning to use BasinMaker it is **strongly encouraged** that the shapefile is generated in section 2.1 Study Area and that the corresponding input layers match/exceed the extent of that shapefile.\n","\n","Check out the short video [Basin Discretization with BasinMaker Light: Magpie Tutorial](https://youtu.be/h5qO7jB0rxA) for more information"]},{"cell_type":"markdown","metadata":{"id":"meQdvkkrrN58"},"source":["<font color=#5559AB> **Download Routing Product for Catchment of Interest** </font> <br>\n","\n","The North American Lake-River Routing Product (version 2.1) covers the main drainage regions across North America (Canada and the USA). The Ontario Lake River Routing Product(version 1.0) covers the main drainage regions across the Ontario Province, Canada. <br>\n","Both the NA routing product and the OLRRP provide sub-region-wise products for download. <br>\n","BasinMaker has two built-in functions for routing product downloads, which are named Download_Routing_Product_For_One_Gauge and Download_Routing_Product_From_Points_Or_LatLon.<br><br>\n","This leaves us two options for data download: <br>\n","\n","<font color=#5559AB> **Option #1**: Provide function Download_Routing_Product_For_One_Gauge with gauge ID. </font> <br>\n","\n","<font color=#5559AB>**Option #2**: Provide function Download_Routing_Product_From_Points_Or_LatLon with the outlet coordinates (lat-lon in degree decimals). </font><br><br>\n","\n","*All options are for BasinMaker to find out the subbasin ID of the outlet subbasin. BasinMaker will use subbasin ID to extract the drainage areas.*<br>\n","<font color=grey>The map above or the following link can be used to help identify gauges of interest: </font>\n","https://wateroffice.ec.gc.ca/search/historical_e.html"]},{"cell_type":"markdown","metadata":{"id":"mYN9N5_scD8a"},"source":["### <font color=#5559AB>2a) Subbasins"]},{"cell_type":"markdown","metadata":{"id":"b7VUtWEWW5Y1"},"source":["#### <font color=grey> **Upload Subbasin Derived from Full BasinMaker** </font>"]},{"cell_type":"markdown","metadata":{"id":"R4Pot9Td6O5F"},"source":["Magpie offers post-processing with BasinMaker light. Users can use the full installation version on their local machines, either through the ArcGIS Pro and GRASS/QGIS GIS python environments and then upload their derived products to Magpie to complete post-processing steps like HRU delineation or RVH/RVP generation\n","\n","Intructions on how to install the full version of BasinMaker is available [here](https://basinmaker.readthedocs.io/en/latest/installation.html#full-installation)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"r57qvSwOctQ3"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"ipyleaflet\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","from ipyleaflet import Map, GeoJSON\n","\n","# Define another folder that will save the outputs\n","folder_product_after_increase_catchment_drainage_area = os.path.join(temporary_dir, 'drainage_area')\n","\n","def remove_temp_data(product_path,temporary_dir):\n","    \"\"\"\n","    Remove temporary data and zip files.\n","\n","    Parameters:\n","    - product_path (str): Path to the product folder.\n","    \"\"\"\n","    if os.path.exists(temporary_dir):\n","        shutil.rmtree(temporary_dir)\n","    if os.path.exists(product_path):\n","        shutil.rmtree(product_path)\n","    zip_files_rm = glob(os.path.join(product_path, \"*.zip\"))\n","    for files_rm in zip_files_rm:\n","        os.remove(files_rm)\n","\n","# Define the product name\n","#@markdown <font color=#5559AB> **Define the product name** </font><br>\n","#@markdown 'OLRRP'  to use the Ontario Lake-River Routing Product(version 2.0)<br>\n","#@markdown 'NALRP' to use the North American Lake-River Routing Product (version 2.1)</font><br>\n","# define version number\n","version_num = 'v2-1' #@param [\"v2-1\", \"v2-0\"] {type:\"raw\"}\n","\n","# define product path\n","product_path = 'None'\n","\n","routing_temp_dir = os.path.join(main_dir, 'workflow_outputs', 'routing_product')\n","if not os.path.exists(routing_temp_dir):\n","    os.makedirs(routing_temp_dir)\n","\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('( ) Upload routing product')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop routing product files into following folder: {routing_temp_dir}')\n","response = input(\"Have you uploaded the related subbasin shapefiles (.shp) file (yes or no): \")\n","\n","if response.lower() == \"yes\":\n","    # -- Visualize --\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Visualize Simplified Routing Product')\n","    print('-----------------------------------------------------------------------------------------------')\n","    # define routing number\n","    routing_product_version_number = version_num\n","    # plot product\n","    display(plot_routing_product_with_ipyleaflet(path_to_product_folder=routing_temp_dir, version_number=routing_product_version_number))\n","\n","    # Remove temporary data after visualization\n","    remove_temp_data(product_path,temporary_dir)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"F2MlPTgihu8-"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------ 2) DISCRETIZATION ---------------\",\n","\n","    \"_comment\": \"------------ 2a) BASINMAKER ROUTING PRODUCT ---------------\",\n","\n","    \"upload_routing_product\": \"yes\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"2a_routing_product.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"deJ3_kTzrP_6"},"source":["#### <font color=grey> **Derive Subbasin with Light BasinMaker** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PIAcNf2mdE56"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"folium\",\"simpledbf\",\"branca\",\n","                      \"time\",\"ipyleaflet\",\"ipywidgets\",\"pathlib\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","!python -m pip install https://github.com/dustming/basinmaker/archive/master.zip\n","\n","import time\n","import simpledbf\n","import zipfile\n","from pathlib import Path\n","from IPython.display import display\n","from basinmaker import basinmaker\n","from ipywidgets import HTML,Layout,IntSlider, ColorPicker, jslink ## only needed to plot figures\n","from ipyleaflet import Map, GeoData, basemaps, LayersControl,Popup,Marker,Polygon,Choropleth,WidgetControl## only needed to plot figures\n","from basinmaker.postprocessing.plotleaflet import plot_routing_product_with_ipyleaflet\n","from basinmaker.postprocessing.downloadpd import Download_Routing_Product_For_One_Gauge\n","from basinmaker.postprocessing.downloadpdptspurepy import Download_Routing_Product_From_Points_Or_LatLon\n","from basinmaker.postprocessing.downloadpdptspurepy import Extract_Routing_Product\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","# Define functions\n","def download_routing_product(product_name, gauge_name, define_lat, define_lon):\n","    \"\"\"\n","    Download routing product based on provided information.\n","\n","    Parameters:\n","    - product_name (str): Name of the routing product.\n","    - gauge_name (str): Gauge name for downloading by gauge.\n","    - city_name (str): City name for downloading by city coordinates.\n","    - define_lat (str): Latitude for downloading by specified coordinates.\n","    - define_lon (str): Longitude for downloading by specified coordinates.\n","\n","    Returns:\n","    - product_path (str): Path to the downloaded routing product.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Download routing product')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    if gauge_name != \"NA\":\n","        subid, product_path = Download_Routing_Product_For_One_Gauge(gauge_name=gauge_name, product_name=product_name)\n","        print('Successfully downloaded routing product using the gauge name!')\n","    elif define_lat != 'NA':\n","        lat, lon = float(define_lat), float(define_lon)\n","        print(\"Study area coordinates:\", lat, lon)\n","        coords = pd.DataFrame({'lat': [lat], 'lon': [lon]})\n","        subid, product_path = Download_Routing_Product_From_Points_Or_LatLon(product_name=product_name,\n","                                                                             Lat=coords['lat'], Lon=coords['lon'])\n","        print('Successfully downloaded routing product using the specified coordinates!')\n","\n","    # Check if the product path exists\n","    if os.path.exists(product_path):\n","        print('Product path exists:', product_path)\n","    else:\n","        print('Product path does not exist:', product_path)\n","        zip_path = f'{product_path}.zip'\n","\n","        # Check if a zip file with the same name exists\n","        if os.path.exists(zip_path):\n","            print('Zip file found. Extracting:', zip_path)\n","\n","            # Extract the zip file to the same directory as the product path\n","            with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n","                zip_ref.extractall(os.path.dirname(product_path))\n","            print('Extraction complete.')\n","        else:\n","            print('No zip file found with the same name:', zip_path)\n","\n","    return product_path\n","\n","def extract_drainage_area(product_path, subid_of_interested_gauges, most_up_stream_subbasin_ids):\n","    \"\"\"\n","    Extract drainage area based on provided subbasin IDs.\n","\n","    Parameters:\n","    - product_path (str): Path to the downloaded and unzipped lake-river routing product folder.\n","    - subid_of_interested_gauges (str): Subbasin ID where the gauge is situated.\n","    - most_up_stream_subbasin_ids (str): Most upstream subbasin IDs for extraction.\n","\n","    Returns:\n","    - folder_product_for_interested_gauges (str): Path to the folder containing the extracted drainage area.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Extract drainage area')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Define subbasin ID lists\n","    subid_of_interested_gauges_lst = [subid_of_interested_gauges]\n","    most_up_stream_subbasin_ids_lst = [most_up_stream_subbasin_ids]\n","\n","    # Define the folder path for downloaded and unzipped lake-river routing product folder\n","    unzip_routing_product_folder = product_path\n","\n","    # Define another folder that will save the outputs\n","    folder_product_for_interested_gauges = os.path.join(temporary_dir, 'catchment_extraction')\n","\n","    # Initialize the basinmaker\n","    start = time.time()\n","    bm = basinmaker.postprocess()\n","\n","    # Extract subregion of the routing product\n","    bm.Select_Subregion_Of_Routing_Structure(\n","        path_output_folder=folder_product_for_interested_gauges,\n","        routing_product_folder=unzip_routing_product_folder,\n","        most_down_stream_subbasin_ids=subid_of_interested_gauges_lst,\n","        most_up_stream_subbasin_ids=most_up_stream_subbasin_ids_lst,\n","        gis_platform=\"purepy\",\n","    )\n","    end = time.time()\n","    print(\"This section took \", end - start, \" seconds\")\n","\n","    return folder_product_for_interested_gauges\n","\n","def remove_small_lakes(lake_size):\n","    \"\"\"\n","    Remove small lakes based on the provided lake size threshold.\n","\n","    Parameters:\n","    - lake_size (float): Lake size threshold (unit: km^2).\n","\n","    Returns:\n","    - folder_product_after_filter_lakes (str): Path to the folder containing the drainage network after removing small lakes.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Remove small lakes')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Print all lake ID file paths in the temporary directory\n","    if os.path.exists(os.path.join(temporary_dir, 'catchment_extraction*', 'sl_connected_lake_v*.shp')):\n","      for lake_ID_file_path in glob(os.path.join(temporary_dir, 'catchment_extraction*', 'sl_connected_lake_v*.shp')):\n","          print(lake_ID_file_path)\n","          # Read the lake ID file\n","          lake_ID_file = gpd.read_file(lake_ID_file_path)\n","\n","          # Filter lakes based on size\n","          small_lakes = lake_ID_file[lake_ID_file[\"Lake_area\"] > lake_size]\n","\n","          # Extract lake IDs\n","          lake_list = (small_lakes['Hylak_id'].unique()).tolist()\n","          lake_IDs = [i for i in lake_list if i != 0]\n","          print(\"Lake IDs: \", lake_IDs)\n","\n","          # Define the output folder for interested gauges\n","          folder_product_for_interested_gauges = os.path.join(temporary_dir, 'catchment_extraction')\n","\n","          # Define the input product folder path, which is the output folder of the previous section\n","          input_routing_product_folder = folder_product_for_interested_gauges\n","\n","          # Define a list containing HyLakeId IDs of lakes of interest\n","          interested_lake_ids = lake_IDs\n","\n","          # Update the variable inside the if block\n","          folder_product_after_filter_lakes = os.path.join(temporary_dir, 'filter_lakes')\n","\n","          start = time.time()\n","\n","          # Remove small lakes using BasinMaker\n","          bm = basinmaker.postprocess()\n","\n","          bm.Remove_Small_Lakes(\n","              path_output_folder=folder_product_after_filter_lakes,\n","              routing_product_folder=input_routing_product_folder,\n","              connected_lake_area_thresthold=lake_size,\n","              non_connected_lake_area_thresthold=lake_size,\n","              selected_lake_ids=interested_lake_ids,\n","              gis_platform=\"purepy\",\n","          )\n","          end = time.time()\n","          print(\"This section took  \", end - start, \" seconds\")\n","    else:\n","      # Define the output folder for interested gauges\n","      folder_product_for_interested_gauges = os.path.join(temporary_dir, 'catchment_extraction')\n","      folder_product_after_filter_lakes = folder_product_for_interested_gauges\n","\n","    return folder_product_after_filter_lakes\n","\n","def simplify_drainage_area(minimum_subbasin_drainage_area):\n","    \"\"\"\n","    Simplify the drainage product by increasing the size of subbasins.\n","\n","    Parameters:\n","    - minimum_subbasin_drainage_area (float): Minimum drainage area of subbasins (unit: km^2).\n","\n","    Returns:\n","    - folder_product_after_increase_catchment_drainage_area (str): Path to the folder containing the drainage network after simplification.\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Simplify drainage product')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Print information about the process\n","    print(f\"Simplifying drainage product with a minimum subbasin drainage area of {minimum_subbasin_drainage_area} km^2\")\n","\n","    # Define the input folder path, which is the output folder of the previous section\n","    if os.path.exists(os.path.join(temporary_dir, 'catchment_extraction*', 'sl_connected_lake_v*.shp')):\n","      folder_product_after_filter_lakes = os.path.join(temporary_dir, 'filter_lakes')\n","    else:\n","      folder_product_after_filter_lakes = os.path.join(temporary_dir, 'catchment_extraction')\n","    input_routing_product_folder = folder_product_after_filter_lakes\n","\n","    # Define the output folder after increasing catchment drainage area\n","    folder_product_after_increase_catchment_drainage_area = os.path.join(temporary_dir, 'drainage_area')\n","\n","    # Initialize the basinmaker\n","    start = time.time()\n","    bm = basinmaker.postprocess()\n","\n","    # Remove river reaches and increase the size of subbasins\n","    bm.Decrease_River_Network_Resolution(\n","        path_output_folder=folder_product_after_increase_catchment_drainage_area,\n","        routing_product_folder=input_routing_product_folder,\n","        minimum_subbasin_drainage_area=minimum_subbasin_drainage_area,\n","        gis_platform=\"purepy\",\n","    )\n","    end = time.time()\n","    print(\"This section took  \", end - start, \" seconds\")\n","\n","    return folder_product_after_increase_catchment_drainage_area\n","\n","def save_routing_product(version_number):\n","    \"\"\"\n","    Save the simplified routing product to the drive.\n","\n","    Parameters:\n","    - version_number (str): Version number of the routing product.\n","\n","    Returns:\n","    - None\n","    \"\"\"\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Save routing product')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Define the routing directory\n","    routing_dir = os.path.join(main_dir, 'workflow_outputs', 'routing_product')\n","\n","    # Create the routing directory if it doesn't exist\n","    if not os.path.exists(routing_dir):\n","        os.makedirs(routing_dir)\n","        print(\"created folder:\", routing_dir)\n","\n","    # Define paths for different routing product files\n","    finalcat_info_riv_path = Path(os.path.join(temporary_dir, 'drainage_area', f'finalcat_info_riv_{version_number}.shp'))\n","    finalcat_info_path = Path(os.path.join(temporary_dir, 'drainage_area', f'finalcat_info_{version_number}.shp'))\n","    sl_connected_lake_path = Path(os.path.join(temporary_dir, 'drainage_area', f'sl_connected_lake_{version_number}.shp'))\n","    sl_non_connected_lake_path = Path(os.path.join(temporary_dir, 'drainage_area', f'sl_non_connected_lake_{version_number}.shp'))\n","\n","    # Helper function to reproject and save GeoDataFrame\n","    def reproject_and_save(gdf, output_path):\n","        if gdf.exists():\n","            gdf_data = gpd.read_file(gdf)\n","            # Reproject if needed\n","            if gdf_data.crs != 'EPSG:4326':\n","                gdf_data = gdf_data.to_crs(epsg=4326)\n","            # Save the GeoDataFrame\n","            gdf_data.to_file(output_path)\n","\n","    # Reproject and save each routing product file\n","    reproject_and_save(finalcat_info_riv_path, os.path.join(routing_dir, f'finalcat_info_riv_{version_number}.shp'))\n","    reproject_and_save(finalcat_info_path, os.path.join(routing_dir, f'finalcat_info_{version_number}.shp'))\n","    reproject_and_save(sl_connected_lake_path, os.path.join(routing_dir, f'sl_connected_lake_{version_number}.shp'))\n","    reproject_and_save(sl_non_connected_lake_path, os.path.join(routing_dir, f'sl_non_connected_lake_{version_number}.shp'))\n","\n","def remove_temp_data(main_dir, temporary_dir, product_path):\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Remove unnecessary files')\n","  print('-----------------------------------------------------------------------------------------------')\n","\n","  # Remove the temporary directory if it exists\n","  if os.path.exists(temporary_dir):\n","      shutil.rmtree(temporary_dir)\n","      print(f\"Deleted temporary directory: {temporary_dir}\")\n","\n","  # Remove the product directory if it exists\n","  if os.path.exists(product_path):\n","      shutil.rmtree(product_path)\n","      print(f\"Deleted product directory: {product_path}\")\n","\n","  # Remove all .zip files in the current directory\n","  zip_files_rm = glob(\"*.zip\")\n","  for files_rm in zip_files_rm:\n","      os.remove(files_rm)\n","      print(f\"Deleted zip file: {files_rm}\")\n","\n","  # Remove folders that start with \"drainage_region\" in the main directory\n","  for item in os.listdir('/content'):\n","      item_path = os.path.join('/content', item)\n","      if os.path.isdir(item_path) and item.startswith(\"drainage_region\"):\n","          shutil.rmtree(item_path)\n","          print(f\"Deleted folder: {item_path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4jhi2qYXSw6W"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define the product name** </font><br>\n","#@markdown [OLRRP](https://uwaterloo-olrrp.shinyapps.io/OLRRP-V2/) to use the Ontario Lake-River Routing Product(version 2.0)<br>\n","#@markdown [NALRP](https://hydrology.uwaterloo.ca/basinmaker/download_regional.html) to use the North American Lake-River Routing Product (version 2.1)</font><br>\n","\n","product_name = \"NALRP\"  #@param [\"NALRP\", \"OLRRP\"]\n","\n","# Define version number\n","version_number = None\n","\n","if product_name == \"NALRP\":\n","  version_number = 'v2-1'\n","elif product_name == \"OLRRP\":\n","  version_number = 'v2-0'\n","\n","#@markdown <font color= #5559AB> **Option 1:**</font> Download routing product using gauge name<br>\n","#@markdown <font color=grey>example, \"02GA024\" <br>\n","#@markdown *If you rather utilize coordinates, enter \"NA\" into gauge_name* </font> <br>\n","\n","gauge_name = \"02GA024\" #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Option 2:** </font>\n","#@markdown Download routing product using coordiantes</font><br>\n","\n","#@markdown <font color=grey> example, 43.4652699 -80.5222961 <br>\n","#@markdown *If you rather use the city name derived coordinates, leave this as \"NA\" (be sure to include \" \" around NA)* </font> <br>\n","\n","define_lat = \"NA\" #@param {type:\"string\"}\n","define_lon = \"NA\" #@param {type:\"string\"}\n","\n","\n","if product_name == \"NALRP\":\n","    # download routing product\n","    product_path = download_routing_product(product_name, gauge_name, define_lat, define_lon)\n","\n","if product_name == \"OLRRP\":\n","  if gauge_name != \"NA\":\n","      # define path\n","      product_path = os.path.join('/content','drainage_region_olrrp')\n","\n","      # Download routing product using provided coordinates\n","      Extract_Routing_Product(version='v2-0', by='Obs_NM', obs_nm=gauge_name,output_path=product_path)\n","  else:\n","      print('Downloading the routing product with lat and lon for OLRRP is currently unavailable')"]},{"cell_type":"markdown","metadata":{"id":"7Jgu7ApfXu5T"},"source":["<font color= #5559AB> **Extract drainage area** </font><br>\n","\n","The drainage area is extracted by utilizing the BasinMaker function `Select_Subregion_Of_Routing_Structure` to extract the routing product based on the subbasin ID.\n","\n","A more detailed description of the function `Select_Subregion_Of_Routing_Structure` can be found [here](https://basinmaker.readthedocs.io/en/latest/basinmaker_tools.html#extract-the-region-of-interest).\n","\n","The output of this function is the following GIS files that only covers the study area domain:\n","\n","* finalcat_info : subbasin polygons respecting lakes (all subbasins in the figure below comes from this file)\n","* finalcat_info_riv : river network polylines in each subbasin polygon\n","* obs_gauges : streamflow observation gauges included in the routing product\n","* sl_connected_lake : the lake polygons of lakes that are connected by the finalcat_info_riv.shp\n","* sl_non_connected_lake : the lake polygons of lakes that are not connected by the finalcat_info_riv.shp\n","\n","The explanation of these GIS files can be found in [BasinMaker website](http://hydrology.uwaterloo.ca/basinmaker/index.html).\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"otWm7cxrX4yn"},"outputs":[],"source":["#@markdown <font color= #5559AB> **Define subbasin ID** </font><br>\n","#@markdown BasinMaker needs the ID of subbasin (subId) which the gauge is situated in, example, 3086525 <br>\n","\n","subid_of_interested_gauges = 3086525 #@param\n","most_up_stream_subbasin_ids = -1 #@param <br><br>\n","\n","#@markdown the value of -1 for most_up_stream_subbasin_ids extracts to the most-upstream (headwater) subbasin whereas other subbasin ID's extract the areas from the outlet to the provided subbasin\n","\n","# step 2\n","folder_product_for_interested_gauges = extract_drainage_area(product_path,subid_of_interested_gauges,most_up_stream_subbasin_ids)\n","\n","# Define the path to the routing product folder\n","path_to_input_routing_product_folder = folder_product_for_interested_gauges\n","# plot product\n","plot_routing_product_with_ipyleaflet(path_to_product_folder = path_to_input_routing_product_folder,version_number = version_number)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WYhIXoduX5B4"},"outputs":[],"source":["#@markdown <font color= #5559AB> **Remove Smaller Lakes** </font>\n","\n","#@markdown This section filters lakes to simplify the network.\n","\n","#@markdown The BasinMaker function Remove_Small_Lakes will be used for this purpose.\n","\n","#@markdown A more detailed description on function `Remove_Small_Lakes` can be found [here](https://basinmaker.readthedocs.io/en/latest/basinmaker_tools.html#filter-lakes).\n","\n","#@markdown For example, if a user enters \"5\" (unit is $km^2$), BasinMaker then removes lakes with area less than $5km2$ (lakes equal to $5km2$ will not be removed).\n","\n","lake_size = 5 #@param\n","\n","folder_product_after_filter_lakes_derived = remove_small_lakes(lake_size)\n","\n","# Define the path to the routing product folder\n","path_to_input_routing_product_folder = folder_product_after_filter_lakes_derived # example Path_to_input_routing_product_folder = folder_product_after_filter_lakes\n","\n","# plot product\n","plot_routing_product_with_ipyleaflet(path_to_product_folder = path_to_input_routing_product_folder,version_number = version_number)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mLetqDKpgdkr"},"outputs":[],"source":["#@markdown <font color= #5559AB> **Simplify drainage product** </font>\n","\n","#@markdown In this section, the network is further simplified by increasing the size of subbasins.\n","\n","#@markdown For example, the minimum drainage area of subbasins can be adjusted to 50 $km^2$.\n","\n","#@markdown The BasinMaker function `Decrease_River_Network_Resolution`  will be used for this purpose. This function will merge any upstream subbasin with a drainage area at the outlet that is smaller than a given threshold with their downstream subbasin. The function starts in headwater subbasin and moves downstream and it does this throughout the domain being processed.\n","\n","#@markdown More detailed description of function `Decrease_River_Network_Resolution` can be found in [here](https://basinmaker.readthedocs.io/en/latest/basinmaker_tools.html#increase-catchment-area). </font>\n","\n","minimum_subbasin_drainage_area = 50.0 #@param\n","\n","# run function to simplify drainage basin\n","folder_product_after_increase_catchment_drainage_area = simplify_drainage_area(minimum_subbasin_drainage_area)\n","\n","# plot product\n","plot_routing_product_with_ipyleaflet(path_to_product_folder = folder_product_after_increase_catchment_drainage_area,version_number = version_number)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"4JspTOhMgdn3"},"outputs":[],"source":["#@markdown <font color=grey>**Save routing product to drive** <br>\n","\n","save_routing_product(version_number)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"GZQb8x05epsA"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(main_dir, temporary_dir, product_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","colab":{"background_save":true},"id":"nw-BD2-rhOaI"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file == True:\n","  data = {\n","      \"_comment\": \"------------ 2) DISCRETIZATION ---------------\",\n","\n","      \"_comment\": \"------------ 2a) BASINMAKER ROUTING PRODUCT ---------------\",\n","\n","      \"upload_routing_product\": \"no\",\n","\n","      \"product_name\": f\"{product_name}\",\n","      \"gauge_name\": f\"{gauge_name}\",\n","\n","      \"define_lat\": f\"{define_lat}\",\n","      \"define_lon\": f\"{define_lon}\",\n","\n","      \"version_num\": f\"{version_number}\",\n","\n","      \"most_down_stream_subbasin_ids\": subid_of_interested_gauges,\n","      \"most_up_stream_subbasin_ids\": most_up_stream_subbasin_ids,\n","\n","      \"lake_size\": lake_size,\n","      \"minimum_subbasin_drainage_area\": minimum_subbasin_drainage_area\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"2a_routing_product.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"Ullydtkc3DP-"},"source":["### <font color=#5559AB>2b) Hydrologic Response Units (HRUs)"]},{"cell_type":"markdown","metadata":{"id":"6Gw9Ceoo7idq"},"source":["#### <font color=grey> **Upload HRUs Derived from BasinMaker Light** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0j8089EKr0HR"},"outputs":[],"source":["def remove_temp_data(product_path,temporary_dir):\n","    \"\"\"\n","    Remove temporary data and zip files.\n","\n","    Parameters:\n","    - product_path (str): Path to the product folder.\n","    \"\"\"\n","    if os.path.exists(temporary_dir):\n","        shutil.rmtree(temporary_dir)\n","    if os.path.exists(product_path):\n","        shutil.rmtree(product_path)\n","    zip_files_rm = glob(os.path.join(product_path, \"*.zip\"))\n","    for files_rm in zip_files_rm:\n","        os.remove(files_rm)\n","\n","#@markdown <font color=#5559AB> **Define the product name** </font><br>\n","#@markdown 'OLRRP'  to use the Ontario Lake-River Routing Product(version 1.0)<br>\n","#@markdown 'NALRP' to use the North American Lake-River Routing Product (version 2.1)</font><br>\n","\n","# define version number\n","version_num = 'v2-1' #@param [\"v2-1\", \"v2-0\"] {type:\"raw\"}\n","\n","# define working directory path\n","HRU_output_folder = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'maps')\n","if not os.path.exists(HRU_output_folder):\n","  os.makedirs(HRU_output_folder)\n","\n","routing_dir = os.path.join(main_dir,'workflow_outputs','routing_product')\n","\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('( ) Upload HRUs')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop HRU files into following folder: {HRU_output_folder}')\n","response = input(\"Have you uploaded the related HRU shapefiles (.shp) file (yes or no): \")\n","if response == \"yes\":\n","  # -- Visualize --\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Visualize Uploaded HRUs')\n","  print('-----------------------------------------------------------------------------------------------')\n","  hru_polygon = gpd.read_file(os.path.join(HRU_output_folder, \"finalcat_hru_info.shp\"))\n","  # define paths\n","  path_connect_lake_polygon_dir = Path(os.path.join(routing_dir, \"sl_connected_lake_\"+version_number+\".shp\"))\n","  path_non_connect_lake_polygon_dir = Path(os.path.join(routing_dir, \"sl_non_connected_lake_\"+version_number+\".shp\"))\n","\n","  ax = hru_polygon.plot(linewidth = 1,edgecolor='black',facecolor=\"none\",zorder=0,figsize=(11,12))\n","\n","  if path_connect_lake_polygon_dir.exists():\n","    sl_lake_ply = gpd.read_file(os.path.join(routing_dir, \"sl_connected_lake_\"+version_number+\".shp\")).to_crs(hru_polygon.crs)\n","    sl_lake_ply.plot(ax = ax, linewidth = 0.00001,edgecolor='black',alpha=0.6,zorder=1)\n","\n","  if path_non_connect_lake_polygon_dir.exists():\n","    nsl_lake_ply = gpd.read_file(os.path.join(routing_dir, \"sl_non_connected_lake_\"+version_number+\".shp\")).to_crs(hru_polygon.crs)\n","    nsl_lake_ply.plot(ax = ax, linewidth = 0.00001,edgecolor='black',alpha=0.6,zorder=1)\n","\n","  # remove temporary files\n","  remove_temp_data(product_path,temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hORyzCA-mJf2"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------ 2b) BASINMAKER DERIVE HRUS ---------------\",\n","\n","    \"upload_HRU_files\": \"yes\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"2b_derive_hrus.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"T8yvPTxPr2mB"},"source":["#### <font color=grey> **Derive HRUs with BasinMaker Light** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EVrwSunOfmXV"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"geopy\", \"folium\",\"simpledbf\",\"branca\",\n","                      \"time\",\"ipyleaflet\",\"ipywidgets\",\"pathlib\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","!python -m pip install https://github.com/dustming/basinmaker/archive/master.zip\n","\n","import time\n","import shutil\n","import simpledbf\n","from pathlib import Path\n","from IPython.display import display\n","from basinmaker import basinmaker\n","import matplotlib.pyplot as plt ## only needed to plot figures\n","from ipywidgets import HTML,Layout,IntSlider, ColorPicker, jslink ## only needed to plot figures\n","from ipyleaflet import Map, GeoData, basemaps, LayersControl,Popup,Marker,Polygon,Choropleth,WidgetControl## only needed to plot figures\n","from basinmaker.postprocessing.plotleaflet import plot_routing_product_with_ipyleaflet\n","from basinmaker.postprocessing.downloadpd import Download_Routing_Product_For_One_Gauge\n","from basinmaker.postprocessing.downloadpdptspurepy import Download_Routing_Product_From_Points_Or_LatLon\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def define_hrus(landcover, soil, elevation_bands, dem, aspect,\n","                area_ratio_thresholds, pixel_size, version_number):\n","    \"\"\"\n","    Define Hydrologic Response Units (HRUs) using BasinMaker.\n","\n","    Parameters:\n","    - landcover (str): Flag indicating whether to include landcover data.\n","    - soil (str): Flag indicating whether to include soil data.\n","    - elevation_bands (str): Flag indicating whether to include elevation band data.\n","    - dem (str): Flag indicating whether to include Digital Elevation Model (DEM) data.\n","    - aspect (str): Flag indicating whether to include aspect data.\n","    - area_ratio_thresholds (list): List of area ratio thresholds.\n","    - pixel_size (float): Pixel size for processing.\n","    - version_number (str): Version number for the output files.\n","    \"\"\"\n","\n","    # Define working directory paths\n","    hru_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data')\n","    routing_dir = os.path.join(main_dir, 'workflow_outputs', 'routing_product')\n","\n","    # Generate maps folder\n","    HRU_output_folder = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'maps')\n","\n","    # Create the HRU output folder if it doesn't exist\n","    if not os.path.isdir(HRU_output_folder):\n","        os.makedirs(HRU_output_folder)\n","        print(\"created folder:\", HRU_output_folder)\n","\n","    delin_list = []\n","\n","    # Paths for DEM data\n","    if dem == True:\n","      for dem_file in os.listdir(os.path.join(hru_dir, 'DEM')):\n","        if dem_file.endswith(\".tif\"):\n","          dem_file_name = dem_file\n","      # Adjust the following paths based on your data locations\n","      path_to_dem = os.path.join(hru_dir, 'DEM',dem_file_name)\n","      print(path_to_dem)\n","    else:\n","      path_to_dem = '#'\n","\n","    # Paths for Elevation Bands data\n","    if elevation_bands == True:\n","      for elev_file in os.listdir(os.path.join(hru_dir, 'Elevation_band')):\n","        if elev_file.endswith(\".shp\"):\n","          elev_file_name = elev_file\n","      # Adjust the following paths based on your data locations\n","      path_other_polygon_1 = os.path.join(hru_dir, 'Elevation_band',elev_file_name)\n","      other_polygon = gpd.read_file(path_other_polygon_1)\n","      print(path_other_polygon_1)\n","      delin_list.append('O_ID_1')\n","    else:\n","      path_other_polygon_1 = '#'\n","\n","    # Paths for Aspect data\n","    if aspect == True:\n","        for aspect_file in os.listdir(os.path.join(hru_dir, 'Aspect')):\n","          if aspect_file.endswith(\".shp\"):\n","            aspect_file_name = aspect_file\n","        # Adjust the following paths based on your data locations\n","        path_to_aspect = os.path.join(hru_dir, 'Aspect',aspect_file_name)\n","        print(path_to_aspect)\n","    else:\n","      path_to_aspect = '#'\n","\n","    # Paths for Landcover data\n","    if landcover == True:\n","      for landcov_file in os.listdir(os.path.join(hru_dir, 'Landcover')):\n","        if landcov_file.endswith(\".shp\"):\n","          landcov_file_name = landcov_file\n","      # Adjust the following paths based on your data locations\n","      path_landuse_polygon = os.path.join(hru_dir,'Landcover',landcov_file_name)\n","      path_landuse_info = os.path.join(hru_dir,'Landcover','landcover_info.csv')\n","      path_veg_info = os.path.join(hru_dir, 'Landcover', \"veg_info.csv\")\n","      #landuse = gpd.read_file(path_landuse_polygon)\n","      landuse_info = pd.read_csv(path_landuse_info)\n","      print(path_landuse_polygon)\n","      delin_list.append('Landuse_ID')\n","      delin_list.append('Veg_ID')\n","    else:\n","      path_landuse_polygon = '#'\n","      path_landuse_info = os.path.join(hru_dir,'Landcover','landcover_info.csv')\n","      path_veg_info = os.path.join(hru_dir, 'Landcover', \"veg_info.csv\")\n","\n","    # Paths for Soil data\n","    if soil == True:\n","      for soil_file in os.listdir(os.path.join(hru_dir, 'Soil')):\n","        if soil_file.endswith(\".shp\"):\n","          soil_file_name = soil_file\n","      # Adjust the following paths based on your data locations\n","      path_soil_polygon = os.path.join(hru_dir,'Soil',soil_file_name)\n","      path_soil_info = os.path.join(hru_dir,'Soil',\"soil_info.csv\")\n","      #soil = gpd.read_file(path_soil_polygon)\n","      soil_info = pd.read_csv(path_soil_info)\n","      print(path_soil_polygon)\n","      delin_list.append('Soil_ID')\n","    else:\n","      path_soil_polygon = '#'\n","      path_soil_info = os.path.join(hru_dir,'Soil',\"soil_info.csv\")\n","\n","    # Define the input folder path\n","    input_routing_product_folder = routing_dir\n","\n","    # Define paths for connected and non-connected lake polygons\n","    path_connect_lake_polygon_dir = Path(os.path.join(routing_dir, f\"sl_connected_lake_{version_number}.shp\"))\n","    path_non_connect_lake_polygon_dir = Path(os.path.join(routing_dir, f\"sl_non_connected_lake_{version_number}.shp\"))\n","\n","    # Check if connected lake polygon exists\n","    path_connect_lake_polygon = path_connect_lake_polygon_dir if path_connect_lake_polygon_dir.exists() else '#'\n","\n","    # Check if non-connected lake polygon exists\n","    path_non_connect_lake_polygon = path_non_connect_lake_polygon_dir if path_non_connect_lake_polygon_dir.exists() else '#'\n","\n","    # Run BasinMaker\n","    bm = basinmaker.postprocess()\n","    start = time.time()\n","\n","    bm.Generate_HRUs(\n","        path_output_folder=HRU_output_folder,\n","        path_subbasin_polygon=os.path.join(routing_dir, f\"finalcat_info_{version_number}.shp\"),\n","        path_connect_lake_polygon=path_connect_lake_polygon,\n","        path_non_connect_lake_polygon=path_non_connect_lake_polygon,\n","        path_landuse_polygon=path_landuse_polygon,\n","        path_soil_polygon=path_soil_polygon,\n","        path_other_polygon_1=path_other_polygon_1,\n","        path_other_polygon_2=path_to_aspect,\n","        path_landuse_info=path_landuse_info,\n","        path_soil_info=path_soil_info,\n","        path_veg_info=path_veg_info,\n","        path_to_dem=path_to_dem,\n","        area_ratio_thresholds=area_ratio_thresholds,\n","        gis_platform=\"purepy\",\n","        projected_epsg_code='EPSG:3161',\n","        pixel_size=pixel_size\n","    )\n","\n","    end = time.time()\n","    print(\"This section took \", end - start, \" seconds\\n\")\n","\n","    # Visualize HRUs\n","    hru_polygon = gpd.read_file(os.path.join(HRU_output_folder, \"finalcat_hru_info.shp\"))\n","\n","    # Define paths for connected and non-connected lake polygons\n","    path_connect_lake_polygon_dir = Path(os.path.join(routing_dir, f\"sl_connected_lake_{version_number}.shp\"))\n","    path_non_connect_lake_polygon_dir = Path(os.path.join(routing_dir, f\"sl_non_connected_lake_{version_number}.shp\"))\n","\n","    ax = hru_polygon.plot(linewidth=1, edgecolor='black', facecolor=\"none\", zorder=0, figsize=(11, 12))\n","\n","    # Plot connected lake polygons if exists\n","    if path_connect_lake_polygon_dir.exists():\n","        sl_lake_ply = gpd.read_file(path_connect_lake_polygon_dir).to_crs(hru_polygon.crs)\n","        sl_lake_ply.plot(ax=ax, linewidth=0.00001, edgecolor='black', alpha=0.6, zorder=1)\n","\n","    # Plot non-connected lake polygons if exists\n","    if path_non_connect_lake_polygon_dir.exists():\n","        nsl_lake_ply = gpd.read_file(path_non_connect_lake_polygon_dir).to_crs(hru_polygon.crs)\n","        nsl_lake_ply.plot(ax=ax, linewidth=0.00001, edgecolor='black', alpha=0.6, zorder=1)\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"VavTmTrygdqu"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Select which layers to include in discretization** <br>\n","landcover = True #@param {type:\"boolean\"}\n","soil = False #@param {type:\"boolean\"}\n","elevation_bands = True #@param {type:\"boolean\"}\n","dem = True #@param {type:\"boolean\"}\n","aspect = False #@param {type:\"boolean\"}\n","\n","#@markdown <font color=#5559AB> **Define area ratio thresholds** <br></font>\n","#@markdown Use [0.1,0.2, 0.2] to get the default HRU map, be sure to include square brackets around values <br>\n","#@markdown Values smaller than 0, ex: [0.1, 0.2, 0.3], provide a finer delineation </font><br>\n","\n","area_ratio_thresholds = [0.1,0.2,0.2] #@param\n","\n","#@markdown <font color=#5559AB> **Define pixel size** <br></font>\n","#@markdown User-defined grid size in m. It is recommend to use 30 m for OLRRP and 90 m for NA. The unit follows the coordinate system of the routing network polygons. <br>\n","\n","pixel_size = 30 #@param\n","\n","#@markdown <font color=#5559AB> **Define the product name** </font><br>\n","#@markdown 'OLRP'  to use the Ontario Lake-River Routing Product(version 1.0)<br>\n","#@markdown 'NALRP' to use the North American Lake-River Routing Product (version 2.1)</font><br>\n","\n","product_name = \"NALRP\"  #@param [\"NALRP\", \"OLRRP\"]\n","\n","# Define version number\n","version_number = None\n","\n","if product_name == \"NALRP\":\n","  version_number = 'v2-1'\n","elif product_name == \"OLRRP\":\n","  version_number = 'v2-0'\n","\n","define_hrus(landcover,soil,elevation_bands,dem,aspect,\n","                area_ratio_thresholds,pixel_size,version_number)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WNv1ut8WoqLW"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","# Store results in a dictionary\n","result_dict = {\n","    \"dem_res\": \"yes\" if dem else \"no\",\n","    \"elevation_bands_res\": \"yes\" if elevation_bands else \"no\",\n","    \"aspect_res\": \"yes\" if aspect else \"no\",\"landcover_res\": \"yes\" if landcover else \"no\",\n","    \"soil_res\": \"yes\" if soil else \"no\"\n","}\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file == True:\n","  # Include these results in the configuration file data\n","  data = {\n","      \"_comment\": \"------------ 2b) BASINMAKER DERIVE HRUS ---------------\",\n","      \"upload_HRU_files\": \"yes\",\n","      **result_dict,  # Using dictionary unpacking for a cleaner approach\n","      \"area_ratio_thresholds\": area_ratio_thresholds,\n","      \"pixel_size\": pixel_size,\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path, \"2b_derive_hrus.json\")\n","\n","  # Writing data to the JSON file\n","  if write_to_configuration_file:\n","      with open(file_path, 'w') as json_file:\n","          json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"QlxKTaYO3QpN"},"source":["### <font color=#5559AB>2c) RVH and RVP Files"]},{"cell_type":"markdown","metadata":{"id":"HhBcwpRwpkYi"},"source":["#### <font color=grey> **Upload RVH and/or RVP File(s)** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Uqu4onO80r_x"},"outputs":[],"source":["#@markdown <font color=grey>**Upload Raven RVP and RVH input files**</font>\n","\n","# upload RVH file\n","rvp_rvh_temp_dir = os.path.join(main_dir,'workflow_outputs','RavenInput')\n","if not os.path.exists(rvp_rvh_temp_dir):\n","  os.makedirs(rvp_rvh_temp_dir)\n","print('\\n-----------------------------------------------------------------------------------------------')\n","print('( ) Upload RVP and RVH files')\n","print('-----------------------------------------------------------------------------------------------')\n","print(f'drag-and-drop routing product files into following folder: {rvp_rvh_temp_dir}')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"JIxpy7_yphBX"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","data = {\n","    \"_comment\": \"------------ 2c) BASINMAKER GENERATED RVP AND RVH FILES ---------------\",\n","\n","    \"upload_RVP_RVH\": \"yes\",\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path,\"2c_rvp_rvh.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional"]},{"cell_type":"markdown","metadata":{"id":"6oETbxup13pe"},"source":["#### <font color=grey> **Generate RVH and/or RVP File(s)** </font>"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EFnSiuEQgUXf"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"geopy\", \"folium\",\"simpledbf\",\"branca\",\n","                      \"time\",\"ipyleaflet\",\"ipywidgets\",\"pathlib\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","!python -m pip install https://github.com/dustming/basinmaker/archive/master.zip\n","\n","import time\n","import shutil\n","import simpledbf\n","from pathlib import Path\n","from IPython.display import display\n","from basinmaker import basinmaker\n","import matplotlib.pyplot as plt ## only needed to plot figures\n","\n","#@markdown <font color=grey> **Use BasinMaker to Generate RVH File and RVP Template**</font> <br>\n","\n","def rvp_rvh_generate(model_name,main_dir,temporary_dir):\n","  # Generate Raven RVP and RVH input files\n","  # save final HRU shapefile to Geojson format for RavenView\n","  HRU_output_folder = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'maps')\n","  hru_polygon = gpd.read_file(os.path.join(HRU_output_folder, \"finalcat_hru_info.shp\"))\n","  hru_polygon.to_file(os.path.join(main_dir, 'shapefile', 'myshpfile.geojson'), driver='GeoJSON')\n","\n","  # generate Raven files\n","  bm = basinmaker.postprocess()\n","\n","  bm.Generate_Raven_Model_Inputs(\n","      path_hru_polygon         = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'maps', 'finalcat_hru_info.shp'),\n","      model_name            = model_name,                         # This is used for naming the output files.\n","      subbasingroup_names_channel   =[\"Allsubbasins\"],                        # A subbasin group will be created in the rvh file for simultaneous manipulation in Raven modeling.\n","      subbasingroup_length_channel   =[-1],\n","      subbasingroup_name_lake      =[\"AllLakesubbasins\"],\n","      subbasingroup_area_lake      =[-1],\n","      path_output_folder         = os.path.join(temporary_dir), # define temporary folder\n","      aspect_from_gis          = 'purepy',\n","  )\n","\n","  # save to drive\n","  for file in glob(os.path.join(temporary_dir, 'RavenInput','*')):\n","      shutil.move(file, os.path.join(main_dir, 'workflow_outputs', 'RavenInput'))\n","\n","# remove temporary data\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))\n","\n","# generate RVH file\n","rvp_rvh_generate(model_name,main_dir,temporary_dir)\n","\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ncvvUWY6qN_5"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file:\n","  data = {\n","      \"_comment\": \"------------ 2c) BASINMAKER GENERATED RVP AND RVH FILES ---------------\",\n","\n","      \"upload_RVP_RVH\": \"no\",\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path,\"2c_rvp_rvh.json\")\n","\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional"]},{"cell_type":"markdown","metadata":{"id":"yVPGofXigFQU"},"source":["**References**\n","\n","Han, M., H. Shen, B. A. Tolson, J. R. Craig, J. Mai, S. Lin, N. B. Basu, F. Awol. (2021). BasinMaker 3.0: a GIS toolbox for distributed watershed delineation of complex lake-river routing networks. Environmental Modelling and Software.\n","\n","Ming Han, Hongren Shen, Bryan A. Tolson, & Robert A. Metcalfe. (2022). Ontario Lake-River Routing Product version 1.0 (v1.0) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.6536085\n","\n","Ming Han, Hongren Shen, Bryan A. Tolson, James R. Craig, Juliane Mai, Simon Lin, Nandita Basu, & Frezer Awol. (2020). North American Lake-River Routing Product v 2.1, derived by BasinMaker GIS Toolbox (v2.1) [Data set]. Zenodo. https://doi.org/10.5281/zenodo.4728185"]},{"cell_type":"markdown","metadata":{"id":"uIfM9eWvvqi6"},"source":["# **3.0 Forcing Data**\n","\n","Raven supports gridded or station-based forcing inputs exclusively in NetCDF format (*.nc files). This subsection offers users to download four different data downloads:\n","\n","* 4.1 CaSPAr Data\n","* 4.2 Gridded Weights Generator\n","* 4.3 DayMet Data\n","* 4.4 Environment Canada Climate Data\n","* 4.5 Format Uploaded Observational Data\n","* 4.6 Hydrometric data (HYDAT)\n","\n","Once downloaded the data is then formatted into a Raven time series/forcing function file (RVT file).\n","\n","For section 4.5, users can upload their own CSV climatic data that is then formatted into a Raven RVT file.\n","\n","Please keep in mind, a Raven model does not require data from all of these sources. The user must determine which type of forcing data best complements their model objectives."]},{"cell_type":"markdown","metadata":{"id":"--y7kezUGgXk"},"source":["### <font color=#5559AB> 3a) CaSPAr Data </font>\n","\n","\n","\n","> <font color=red>Temporarily unavailable\n","\n","\n","\n","\n","\n","\n","The Canadian Surface Prediction Archive (CaSPAr), developed by Mai et al. (2020), is an archive of numerical weather predictions issued by Environment and Climate Change Canada. More information about the CaSPAr dataset is available [here](https://github.com/julemai/CaSPAr)\n","\n","This subsection of the Magpie workflow assists in walking users through how to download CaSPAr data and format it to be interpretable by Raven."]},{"cell_type":"markdown","metadata":{"id":"_7PCOuftojzF"},"source":["**References**\n","\n","Mai et al. (2020).\n","The Canadian Surface Prediction Archive (CaSPAr): A Platform to Enhance Environmental Modeling in Canada and Globally.\n","Bulletin of the American Meteorological Society, https://doi.org/10.1175/BAMS-D-19-0143.1\n","\n","**Acknowledgements**\n","\n","Dr. Jonathan Cuellar, Qiutong Yu, and others at the University of Waterloo assisted with the temporal adjustment/aggregation script."]},{"cell_type":"markdown","metadata":{"id":"takgaE7gubg7"},"source":["### <font color=#5559AB> 3b) Gridded Weights Generator </font>\n","\n","The grid weights generator for Raven, developed by Mai (2022), generates weights of grid cells contributing to individual shapes. Grid weights can be used to spatially aggregate a gridded model output to an irregularly shaped domain or, more generally, to map grid cells to a polygon shape. More information can be found [here](https://github.com/julemai/GridWeightsGenerator)\n","\n","Layout example for general use:\n","\n","<font color=grey> **python derive_grid_weights.py -i [your-nc-file] -d [dim-names] -v [var-names] -r [tlbx-shp-file] -s [subbasin-id] -b [gauge-id] -f [shp-attr-name] -o [weights-file]**\t</font>\n","\n","A <font color=red>NetCDF file</font> and <font color=red>shapfile of HRUs</font> are required.\n","\n","Check out the short video [4.3 Gridded Weights Generator - Magpie Workflow](https://youtu.be/zskW_9qczdk) for more information.\n","\n","If there is an error loading the netCDF4 library go to \"Runtime\" -> \"Disconnet and delete runtime\", run the \"Mandatory Workflow Set up\", and re-run this section."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5G1n8YLLyLBF"},"outputs":[],"source":["#@markdown <font color=#5559AB>**Define NetCDF File Path**</font>\n","\n","#@markdown <font color=grey> **-i [your-nc-file]**<br>\n","#@markdown filename of NetCDF file or shapefile that contains how you discretized your model\n","forcing_var = \"/content/google_drive/MyDrive/Magpie_Workflow/workflow_outputs/RavenInput/input/mean_prcp_daily.nc\" #@param {type:\"string\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"bNy5ZgfyyDmM"},"outputs":[],"source":["#@markdown <font color=grey> **Overview of CaSPAr data** </font>\n","\n","check_casp = xr.open_dataset(os.path.join(forcing_var))\n","display(check_casp)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-ZtcE9QZyyoh"},"outputs":[],"source":["#@markdown <font color=#5559AB>**Define Dimension and Variable Names**</font>\n","\n","#@markdown <font color=grey> **-d [dim-names]**<br>\n","#@markdown names of NetCDF dimensions of longitude (x) and latitude (y) in this order, e.g. \"rlon,rlat\"<br>\n","#@markdown if you are using RDRS data, input: \"rlon,rlat\"\n","dim_names = \"rlon,rlat\" #@param {type:\"string\"}\n","\n","#@markdown <font color=grey> **-v [var-names]**<br>\n","#@markdown names of 2D NetCDF variables containing longitudes and latitudes (in this order) of centroids of grid cells, e.g. \"lon,lat\"<br>\n","#@markdown if you are using RDRS data, input: \"lon,lat\"\n","var_names = \"lon,lat\" #@param {type:\"string\"}\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cqcMbFWozAKG"},"outputs":[],"source":["#@markdown <font color=#5559AB>**Define HRU Shapefile Path**</font>\n","\n","#@markdown <font color=grey> **-r [tlbx-shp-file]**<br>\n","#@markdown name of shapefile routing toolbox provides; shapefile contains shapes of all land and lake HRUs,e.g. \"HRUs.shp\"<br>\n","boundary_shp = \"/content/google_drive/MyDrive/Magpie_Workflow/workflow_outputs/RavenInput/maps/finalcat_hru_info.shp\" #@param {type:\"string\"}\n","\n","\n","#@markdown located in \"RavenInput\"->\"maps\" in Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hVCnDp0NWEwr"},"outputs":[],"source":["#@markdown <font color=grey>**OPTIONAL - Determine the Subbasin ID that the Flow Gauge is Located In**</font>\n","\n","#@markdown determin the flow gauge ID to determine the subbasin it is located in, copy and paste the ID into gauge RVT file\n","\n","#@markdown only needed is the user would rather utilize the subbasin ID rather than the flow gauge, althought the flow gauge is encouraged\n","\n","flow_gauge_ID = '02GA024' #@param {type:\"string\"}\n","\n","gauge_csv = pd.read_csv(os.path.join(drive_dir, 'extras', 'subbasin_plots', 'obs_gauges_NA_v2-1.csv'))\n","select_row = gauge_csv.loc[gauge_csv['Obs_NM'] == flow_gauge_ID]\n","subbasin_ID = select_row['SubId'].values\n","print(f'Subbasin ID: {subbasin_ID[0]}')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cmfkCP6TxlfN"},"outputs":[],"source":["#@markdown <font color=#5559AB>**Define Subbasin ID OR Gauge Flow ID**</font>\n","\n","#@markdown <font color=grey> **-s [subbasin-id]**<br>\n","#@markdown <font color=red> (**either** -s [subbasin-id] or -b [gauge-id] must be set)</font> ID of subbasin  most downstream (likely a subbasin\n","#@markdown that contains a streamflow gauge station but can be any subbasin ID); script will include all subbasins upstream of the given subbasin automatically; according\n","#@markdown attribute in [tlbx-shp-file] is called \"SubId\"; e.g. \"7202\"<br>\n","#@markdown Refer to 1.1_Shapefile.ipynb to identify subbasin\n","subId = \"3086525\" #@param {type:\"string\"}\n","#@markdown _if you rather define -b [gauge-id], input subID as NA_\n","\n","if subId == \"NA\":\n","  subId = None\n","\n","#@markdown <font color=grey> **-b [gauge-id]**<br>\n","#@markdown <font color=red> (**either** -b [gauge-id] or -s [subbasin-id] must be set)</font> ID of streamflow gauging station; according attribute in [tlbx-shp-file] is called \"Obs_NM\"; e.g. \"02LE024\n","gaugeId = \"NA\" #@param {type:\"string\"}\n","#@markdown _if you rather define -b [gauge-id], input subID as NA_\n","\n","if gaugeId == \"NA\":\n","  gaugeId = None"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"0wBodztKzP63"},"outputs":[],"source":["#@markdown <font color=#5559AB>**Define Output Name**</font>\n","\n","#@markdown <font color=grey> **-o [output-name]**<br>\n","#@markdown define output filename <br>\n","output_name = \"GridWeights.txt\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"C0tprX5OEgNR"},"outputs":[],"source":["#@markdown <font color=grey>**Run Gridded Weight Generator**</font>\n","\n","#!/usr/bin/env python\n","from __future__ import print_function\n","\n","# Copyright 2016-2020 Juliane Mai - juliane.mai(at)uwaterloo.ca\n","\n","# read netCDF files\n","import netCDF4 as nc4\n","\n","# command line arguments\n","import argparse\n","\n","# checking file paths and file extensions\n","from pathlib import Path\n","\n","# to perform numerics\n","import numpy as np\n","\n","# read shapefiles and convert to GeoJSON and WKT\n","import geopandas as gpd\n","\n","# get equal-area projection and derive overlay\n","from   osgeo   import ogr\n","from   osgeo   import osr\n","from   osgeo   import __version__ as osgeo_version\n","\n","\n","\n","input_file           = forcing_var\n","dimname              = dim_names\n","varname              = var_names\n","routinginfo          = boundary_shp\n","basin                = gaugeId  # e.g. \"02LE024\"\n","SubId                = subId  # e.g. 7202\n","output_file          = \"GriddedForcings.txt\"\n","doall                = False\n","key_colname          = \"HRU_ID\"\n","key_colname_model    = None\n","area_error_threshold = 0.05\n","dojson               = False\n","\n","parser      = argparse.ArgumentParser(formatter_class=argparse.RawTextHelpFormatter,\n","              description='''Convert files from ArcGIS raster format into NetDF file usable in CaSPAr.''')\n","parser.add_argument('-i', '--input_file', action='store',\n","                    default=input_file, dest='input_file', metavar='input_file',\n","                    help='Either (A) Example NetCDF file containing at least 1D or 2D latitudes and 1D or 2D longitudes where grid needs to be representative of model outputs that are then required to be routed. Or (B) a shapefile that contains shapes of subbasins and one attribute that is defining its index in the NetCDF model output file (numbering needs to be [0 ... N-1]).')\n","parser.add_argument('-d', '--dimname', action='store',\n","                    default=dimname, dest='dimname', metavar='dimname',\n","                    help='Dimension names of longitude (x) and latitude (y) (in this order). Example: \"rlon,rlat\", or \"x,y\"')\n","parser.add_argument('-v', '--varname', action='store',\n","                    default=varname, dest='varname', metavar='varname',\n","                    help='Variable name of 2D longitude and latitude variables in NetCDF (in this order). Example: \"lon,lat\".')\n","parser.add_argument('-r', '--routinginfo', action='store',\n","                    default=routinginfo, dest='routinginfo', metavar='routinginfo',\n","                    help='Shapefile that contains all information for the catchment of interest (and maybe some more catchments).')\n","parser.add_argument('-b', '--basin', action='store',\n","                    default=basin, dest='basin', metavar='basin',\n","                    help='Basin of interest (corresponds to \"Gauge_ID\" in shapefile given with -r). Either this or SubId ID (-s) needs to be given. Can be a comma-separated list of basins, e.g., \"02LB005,02LB008\".')\n","parser.add_argument('-s', '--SubId', action='store',\n","                    default=SubId, dest='SubId', metavar='SubId',\n","                    help='SubId of most downstream subbasin (containing usually a gauge station) (corresponds to \"SubId\" in shapefile given with -r). Either this or basin ID (-b) needs to be given. Can be a comma-separated list of SubIds, e.g., \"7399,7400\".')\n","parser.add_argument('-o', '--output_file', action='store',\n","                    default=output_file, dest='output_file', metavar='output_file',\n","                    help='File that will contain grid weights for Raven.')\n","parser.add_argument('-a', '--doall', action='store_true',\n","                    default=doall, dest='doall',\n","                    help='If given, all HRUs found in shapefile are processed. Overwrites settings of \"-b\" and \"-s\". Default: not set (False).')\n","parser.add_argument('-c', '--key_colname', action='store',\n","                    default=key_colname, dest='key_colname', metavar='key_colname',\n","                    help='Name of column in shapefile containing unique key for each dataset. This key will be used in output file. This setting is only used if \"-a\" option is used. \"Default: \"HRU_ID\".')\n","parser.add_argument('-f', '--key_colname_model', action='store',\n","                    default=key_colname_model, dest='key_colname_model', metavar='key_colname_model',\n","                    help='Attribute name in input_file shapefile (option -i) that defines the index of the shape in NetCDF model output file (numbering needs to be [0 ... N-1]). Example: \"NetCDF_col\".')\n","parser.add_argument('-e', '--area_error_threshold', action='store',\n","                    default=area_error_threshold, dest='area_error_threshold', metavar='area_error_threshold',\n","                    help='Threshold (as fraction) of allowed mismatch in areas between subbasins from shapefile (-r) and overlay with grid-cells or subbasins (-i). If error is smaller than this threshold the weights will be adjusted such that they sum up to exactly 1. Raven will exit gracefully in case weights do not sum up to at least 0.95. Default: 0.05.')\n","parser.add_argument('-j', '--dojson', action='store_true',\n","                    default=dojson, dest='dojson',\n","                    help='If given, the GeoJSON of grid cells contributing to at least one HRU are dumped into  GeoJSON. Default: False.')\n","\n","args                 = parser.parse_args()\n","input_file           = args.input_file\n","dimname              = np.array(args.dimname.split(','))\n","varname              = np.array(args.varname.split(','))\n","routinginfo          = args.routinginfo\n","basin                = args.basin\n","SubId                = args.SubId\n","output_file          = args.output_file\n","doall                = args.doall\n","key_colname          = args.key_colname\n","key_colname_model    = args.key_colname_model\n","area_error_threshold = float(args.area_error_threshold)\n","dojson               = args.dojson\n","\n","if dojson:\n","    # write geoJSON files (eventually)\n","    import geojson as gjs\n","\n","if not(SubId is None):\n","\n","    SubId = [ np.int(ss.strip()) for ss in SubId.split(',') ]\n","\n","if (SubId is None) and (basin is None) and not(doall):\n","    raise ValueError(\"Either gauge ID (option -b; e.g., 02AB003) or SubId ID (option -s; e.g., 7173) specified in shapefile needs to be given. You specified none. This basin will be the most downstream gridweights of all upstream subbasins will be added automatically.\")\n","\n","if ( not(SubId is None) ) and ( not(basin is None) ) and not(doall):\n","    raise ValueError(\"Either gauge ID (option -b; e.g., 02AB003) or SubId ID (option -s; e.g., 7173) specified in shapefile needs to be specified. You specified both. This basin will be the most downstream gridweights of all upstream subbasins will be added automatically.\")\n","\n","if not(doall):\n","    key_colname = \"HRU_ID\"    # overwrite any settimg made for this column name in case doall is not set\n","\n","del parser, args\n","\n","\n","# better dont chnage that ever\n","crs_lldeg = 4326        # EPSG id of lat/lon (deg) coordinate referenence system (CRS)\n","crs_caea  = 3573        # EPSG id of equal-area    coordinate referenence system (CRS)\n","\n","\n","def create_gridcells_from_centers(lat, lon):\n","\n","    # create array of edges where (x,y) are always center cells\n","    nlon = np.shape(lon)[1]\n","    nlat = np.shape(lat)[0]\n","    lonh = np.empty((nlat+1,nlon+1), dtype=float)\n","    lath = np.empty((nlat+1,nlon+1), dtype=float)\n","    tmp1 = [ [ (lat[ii+1,jj+1]-lat[ii,jj])/2 for jj in range(nlon-1) ] + [ (lat[ii+1,nlon-1]-lat[ii,nlon-2])/2 ] for ii in range(nlat-1) ]\n","    tmp2 = [ [ (lon[ii+1,jj+1]-lon[ii,jj])/2 for jj in range(nlon-1) ] + [ (lon[ii+1,nlon-1]-lon[ii,nlon-2])/2 ] for ii in range(nlat-1) ]\n","    dlat = np.array(tmp1 + [ tmp1[-1] ])\n","    dlon = np.array(tmp2 + [ tmp2[-1] ])\n","    lonh[0:nlat,0:nlon] = lon - dlon\n","    lath[0:nlat,0:nlon] = lat - dlat\n","\n","    # make lat and lon one column and row wider such that all\n","    lonh[nlat,0:nlon] = lonh[nlat-1,0:nlon] + (lonh[nlat-1,0:nlon] - lonh[nlat-2,0:nlon])\n","    lath[nlat,0:nlon] = lath[nlat-1,0:nlon] + (lath[nlat-1,0:nlon] - lath[nlat-2,0:nlon])\n","    lonh[0:nlat,nlon] = lonh[0:nlat,nlon-1] + (lonh[0:nlat,nlon-1] - lonh[0:nlat,nlon-2])\n","    lath[0:nlat,nlon] = lath[0:nlat,nlon-1] + (lath[0:nlat,nlon-1] - lath[0:nlat,nlon-2])\n","    lonh[nlat,nlon]   = lonh[nlat-1,nlon-1] + (lonh[nlat-1,nlon-1] - lonh[nlat-2,nlon-2])\n","    lath[nlat,nlon]   = lath[nlat-1,nlon-1] + (lath[nlat-1,nlon-1] - lath[nlat-2,nlon-2])\n","\n","    return [lath,lonh]\n","\n","def shape_to_geometry(shape_from_jsonfile,epsg=None):\n","\n","    # converts shape read from shapefile to geometry\n","    # epsg :: integer EPSG code\n","\n","    ring_shape = ogr.Geometry(ogr.wkbLinearRing)\n","\n","    for ii in shape_from_jsonfile:\n","        ring_shape.AddPoint_2D(ii[0],ii[1])\n","    # close ring\n","    ring_shape.AddPoint_2D(shape_from_jsonfile[0][0],shape_from_jsonfile[0][1])\n","\n","    poly_shape = ogr.Geometry(ogr.wkbPolygon)\n","    poly_shape.AddGeometry(ring_shape)\n","\n","    if not( epsg is None):\n","        source = osr.SpatialReference()\n","        source.ImportFromEPSG(crs_lldeg)       # usual lat/lon projection\n","\n","        target = osr.SpatialReference()\n","        target.ImportFromEPSG(epsg)       # any projection to convert to\n","\n","        transform = osr.CoordinateTransformation(source, target)\n","        poly_shape.Transform(transform)\n","\n","    return poly_shape\n","\n","def check_proximity_of_envelops(gridcell_envelop, shape_envelop):\n","\n","    # checks if two envelops are in proximity (intersect)\n","\n","    # minX  --> env[0]\n","    # maxX  --> env[1]\n","    # minY  --> env[2]\n","    # maxY  --> env[3]\n","\n","    if  ((gridcell_envelop[0] <= shape_envelop[1]) and (gridcell_envelop[1] >= shape_envelop[0]) and\n","         (gridcell_envelop[2] <= shape_envelop[3]) and (gridcell_envelop[3] >= shape_envelop[2])):\n","\n","        grid_is_close = True\n","\n","    else:\n","\n","        grid_is_close = False\n","\n","    return grid_is_close\n","\n","def check_gridcell_in_proximity_of_shape(gridcell_edges, shape_from_jsonfile):\n","\n","    # checks if a grid cell falls into the bounding box of the shape\n","    # does not mean it intersects but it is a quick and cheap way to\n","    # determine cells that might intersect\n","\n","    # gridcell_edges = [(lon1,lat1),(lon2,lat2),(lon3,lat3),(lon4,lat4)]\n","    # shape_from_jsonfile\n","\n","    min_lat_cell  = np.min([ii[1] for ii in gridcell_edges])\n","    max_lat_cell  = np.max([ii[1] for ii in gridcell_edges])\n","    min_lon_cell  = np.min([ii[0] for ii in gridcell_edges])\n","    max_lon_cell  = np.max([ii[0] for ii in gridcell_edges])\n","\n","    lat_shape = np.array([ icoord[1] for icoord in shape_from_jsonfile ])     # is it lat???\n","    lon_shape = np.array([ icoord[0] for icoord in shape_from_jsonfile ])     # is it lon???\n","\n","    min_lat_shape = np.min(lat_shape)\n","    max_lat_shape = np.max(lat_shape)\n","    min_lon_shape = np.min(lon_shape)\n","    max_lon_shape = np.max(lon_shape)\n","\n","    if  ((min_lat_cell <= max_lat_shape) and (max_lat_cell >= min_lat_shape) and\n","         (min_lon_cell <= max_lon_shape) and (max_lon_cell >= min_lon_shape)):\n","\n","        grid_is_close = True\n","\n","    else:\n","\n","        grid_is_close = False\n","\n","    return grid_is_close\n","\n","\n","def derive_2D_coordinates(lat_1D, lon_1D):\n","\n","    # nlat = np.shape(lat_1D)[0]\n","    # nlon = np.shape(lon_1D)[0]\n","\n","    # lon_2D =              np.array([ lon_1D for ilat in range(nlat) ], dtype=float32)\n","    # lat_2D = np.transpose(np.array([ lat_1D for ilon in range(nlon) ], dtype=float32))\n","\n","    lon_2D =              np.tile(lon_1D, (lat_1D.size, 1))\n","    lat_2D = np.transpose(np.tile(lat_1D, (lon_1D.size, 1)))\n","\n","    return lat_2D, lon_2D\n","\n","\n","if ( Path(input_file).suffix == '.nc'):\n","\n","    # -------------------------------\n","    # Read NetCDF\n","    # -------------------------------\n","    print(' ')\n","    print('   (1) Reading NetCDF (grid) data ...')\n","\n","    nc_in = nc4.Dataset(input_file, \"r\")\n","    lon      = nc_in.variables[varname[0]][:]\n","    lon_dims = nc_in.variables[varname[0]].dimensions\n","    lat      = nc_in.variables[varname[1]][:]\n","    lat_dims = nc_in.variables[varname[1]].dimensions\n","    nc_in.close()\n","\n","\n","    if len(lon_dims) == 1 and len(lat_dims) == 1:\n","\n","        # in case coordinates are only 1D (regular grid), derive 2D variables\n","\n","        print('   >>> Generate 2D lat and lon fields. Given ones are 1D.')\n","\n","        lat, lon = derive_2D_coordinates(lat,lon)\n","        lon_dims_2D = lat_dims + lon_dims\n","        lat_dims_2D = lat_dims + lon_dims\n","        lon_dims = lon_dims_2D\n","        lat_dims = lat_dims_2D\n","\n","    elif len(lon_dims) == 2 and len(lat_dims) == 2:\n","\n","        # Raven numbering is (numbering starts with 0 though):\n","        #\n","        #      [      1      2      3   ...     1*nlon\n","        #        nlon+1 nlon+2 nlon+3   ...     2*nlon\n","        #           ...    ...    ...   ...     ...\n","        #           ...    ...    ...   ...  nlat*nlon ]\n","        #\n","        # --> Making sure shape of lat/lon fields is like that\n","        #\n","\n","        if np.all(np.array(lon_dims) == dimname[::1]):\n","            lon = np.transpose(lon)\n","            print('   >>> switched order of dimensions for variable \"{0}\"'.format(varname[0]))\n","        elif np.all(np.array(lon_dims) == dimname[::-1]):\n","            print('   >>> order of dimensions correct for variable \"{0}\"'.format(varname[0]))\n","        else:\n","            print('   >>> Dimensions found {0} does not match the dimension names specified with (-d): {1}'.format(lon_dims,dimname))\n","            raise ValueError('STOP')\n","\n","        if np.all(np.array(lat_dims) == dimname[::1]):\n","            lat = np.transpose(lat)\n","            print('   >>> switched order of dimensions for variable \"{0}\"'.format(varname[1]))\n","        elif np.all(np.array(lat_dims) == dimname[::-1]):\n","            print('   >>> order of dimensions correct for variable \"{0}\"'.format(varname[1]))\n","        else:\n","            print('   >>> Dimensions found {0} does not match the dimension names specified with (-d): {1}'.format(lat_dims,dimname))\n","            raise ValueError('STOP')\n","\n","    else:\n","\n","        raise ValueError(\n","            \"The coord variables must have the same number of dimensions (either 1 or 2)\"\n","        )\n","\n","    lath, lonh    = create_gridcells_from_centers(lat, lon)\n","\n","    nlon       = np.shape(lon)[1]\n","    nlat       = np.shape(lat)[0]\n","    nshapes    = nlon * nlat\n","\n","elif ( Path(input_file).suffix == '.shp'):\n","\n","    # -------------------------------\n","    # Read Shapefile\n","    # -------------------------------\n","    print(' ')\n","    print('   (1) Reading Shapefile (grid) data ...')\n","\n","    model_grid_shp     = gpd.read_file(input_file)\n","    model_grid_shp     = model_grid_shp.to_crs(epsg=crs_caea)           # WGS 84 / North Pole LAEA Canada\n","\n","    nshapes    = model_grid_shp.geometry.count()    # number of shapes in model \"discretization\" shapefile (i.e. model grid-cells; not basin-discretization shapefile)\n","    nlon       = 1        # only for consistency\n","    nlat       = nshapes  # only for consistency\n","\n","else:\n","\n","    print(\"File extension found: {}\".format(input_file.split('.')[-1]))\n","    raise ValueError('Input file needs to be either NetCDF (*.nc) or a Shapefile (*.shp).')\n","\n","\n","# -------------------------------\n","# Read Basin shapes and all subbasin-shapes (from toolbox)\n","# -------------------------------\n","print(' ')\n","print('   (2) Reading shapefile data ...')\n","\n","shape     = gpd.read_file(routinginfo)\n","# shape     = shape.to_crs(epsg=crs_lldeg)        # this is lat/lon in degree\n","shape     = shape.to_crs(epsg=crs_caea)           # WGS 84 / North Pole LAEA Canada\n","\n","# check that key column contains only unique values\n","keys = np.array(list(shape[key_colname]))\n","# keys_uniq = np.unique(keys)\n","# if len(keys_uniq) != len(keys):\n","#     raise ValueError(\"The attribute of the shapefile set to contain only unique identifiers ('{}') does contain duplicate keys. Please specify another column (option -c '<col_name>') and use the option to process all records contained in the shapefile (-a).\".format(key_colname))\n","\n","\n","# select only relevant basins/sub-basins\n","if not(doall):\n","\n","    if not(basin is None):    # if gauge ID is given\n","\n","        basins     = [ bb.strip() for bb in basin.split(',') ]\n","        idx_basins = [ list(np.where(shape['Obs_NM']==bb)[0]) for bb in basins ]\n","\n","        # find corresponding SubId\n","        SubId = [np.int(shape.loc[idx_basin].SubId) for idx_basin in idx_basins]\n","        print(\"   >>> found gauge at SubId = \",SubId)\n","\n","    if not(SubId is None): # if basin ID is given\n","\n","        old_SubIds = []\n","        for SI in SubId:\n","\n","            old_SubId     = []\n","            new_SubId     = [ SI ]\n","\n","            while len(new_SubId) > 0:\n","\n","                old_SubId.append(new_SubId)\n","                new_SubId = [ list(shape.loc[(np.where(shape['DowSubId']==ii))[0]].SubId) for ii in new_SubId ]  # find all upstream catchments of these new basins\n","                new_SubId = list(np.unique([item for sublist in new_SubId for item in sublist])) # flatten list and make entries unique\n","\n","            old_SubId   = np.array([item for sublist in old_SubId for item in sublist],dtype=np.int)  # flatten list\n","            old_SubIds += list(old_SubId)\n","\n","        old_SubIds = list( np.sort(np.unique(old_SubIds)) )\n","\n","        idx_basins = [ list(np.where(shape['SubId']==oo)[0]) for oo in old_SubIds ]\n","        idx_basins = [ item for sublist in idx_basins for item in sublist ]  # flatten list\n","        idx_basins = list(np.unique(idx_basins))                             # getting only unique list indexes\n","\n","else: # all HRUs to be processed\n","\n","    idx_basins = list(np.arange(0,len(shape)))\n","\n","\n","# make sure HRUs are only once in this list\n","hrus = np.array( shape.loc[idx_basins][key_colname] ) #[sort_idx]\n","\n","idx_basins_unique = []\n","hrus_unique       = []\n","for ihru,hru in enumerate(hrus):\n","\n","    if not( hru in hrus_unique ):\n","\n","        hrus_unique.append(hrus[ihru])\n","        idx_basins_unique.append(idx_basins[ihru])\n","\n","idx_basins = idx_basins_unique\n","hrus       = hrus_unique\n","\n","\n","# order according to values in \"key_colname\"; just to make sure outputs will be sorted in the end\n","sort_idx = np.argsort(shape.loc[idx_basins][key_colname])\n","print('   >>> HRU_IDs found = ',list(np.array( shape.loc[idx_basins][key_colname] )[sort_idx]),'  (total: ',len(idx_basins),')')\n","\n","# reduce the shapefile dataset now to only what we will need\n","shape     = shape.loc[np.array(idx_basins)[sort_idx]]\n","\n","# indexes of all lines in df\n","keys       = shape.index\n","nsubbasins = len(keys)\n","\n","# initialize\n","coord_catch_wkt = {}\n","\n","# loop over all subbasins and transform coordinates into equal-area projection\n","for kk in keys:\n","\n","    ibasin = shape.loc[kk]\n","\n","    poly                = ibasin.geometry\n","    try:\n","        coord_catch_wkt[kk] = ogr.CreateGeometryFromWkt(poly.to_wkt())\n","    except:\n","        coord_catch_wkt[kk] = ogr.CreateGeometryFromWkt(poly.wkt)\n","\n","# -------------------------------\n","# construct all grid cell polygons\n","# -------------------------------\n","if ( Path(input_file).suffix == '.nc'):\n","\n","    print(' ')\n","    print('   (3) Generate shapes for NetCDF grid cells ...')\n","\n","    grid_cell_geom_gpd_wkt_ea = [ [ [] for ilon in range(nlon) ] for ilat in range(nlat) ]\n","    if dojson:\n","        grid_cell_geom_gpd_wkt_ll = [ [ [] for ilon in range(nlon) ] for ilat in range(nlat) ]\n","    for ilat in range(nlat):\n","        if ilat%10 == 0:\n","            print('   >>> Latitudes done: {0} of {1}'.format(ilat,nlat))\n","\n","        for ilon in range(nlon):\n","\n","            # -------------------------\n","            # EPSG:3035   needs a swap before and after transform ...\n","            # -------------------------\n","            # gridcell_edges = [ [lath[ilat,ilon]    , lonh[ilat,  ilon]    ],            # for some reason need to switch lat/lon that transform works\n","            #                    [lath[ilat+1,ilon]  , lonh[ilat+1,ilon]    ],\n","            #                    [lath[ilat+1,ilon+1], lonh[ilat+1,ilon+1]  ],\n","            #                    [lath[ilat,ilon+1]  , lonh[ilat,  ilon+1]  ]]\n","\n","            # tmp = shape_to_geometry(gridcell_edges, epsg=crs_caea)\n","            # tmp.SwapXY()              # switch lat/lon back\n","            # grid_cell_geom_gpd_wkt_ea[ilat][ilon] = tmp\n","\n","            # -------------------------\n","            # EPSG:3573   does not need a swap after transform ... and is much faster than transform with EPSG:3035\n","            # -------------------------\n","            #\n","            # Windows            Python 3.8.5 GDAL 3.1.3 --> lat/lon (Ming)\n","            # MacOS 10.15.6      Python 3.8.5 GDAL 3.1.3 --> lat/lon (Julie)\n","            # Graham             Python 3.8.2 GDAL 3.0.4 --> lat/lon (Julie)\n","            # Graham             Python 3.6.3 GDAL 2.2.1 --> lon/lat (Julie)\n","            # Ubuntu 18.04.2 LTS Python 3.6.8 GDAL 2.2.3 --> lon/lat (Etienne)\n","            #\n","            if osgeo_version < '3.0':\n","                gridcell_edges = [ [lonh[ilat,  ilon]   , lath[ilat,ilon]      ],            # for some reason need to switch lat/lon that transform works\n","                                   [lonh[ilat+1,ilon]   , lath[ilat+1,ilon]    ],\n","                                   [lonh[ilat+1,ilon+1] , lath[ilat+1,ilon+1]  ],\n","                                   [lonh[ilat,  ilon+1] , lath[ilat,ilon+1]    ]]\n","            else:\n","                gridcell_edges = [ [lath[ilat,ilon]     , lonh[ilat,  ilon]    ],            # for some reason lat/lon order works\n","                                   [lath[ilat+1,ilon]   , lonh[ilat+1,ilon]    ],\n","                                   [lath[ilat+1,ilon+1] , lonh[ilat+1,ilon+1]  ],\n","                                   [lath[ilat,ilon+1]   , lonh[ilat,  ilon+1]  ]]\n","\n","            tmp = shape_to_geometry(gridcell_edges, epsg=crs_caea)\n","            grid_cell_geom_gpd_wkt_ea[ilat][ilon] = tmp\n","\n","            if dojson:\n","                tmp = shape_to_geometry(gridcell_edges)\n","                grid_cell_geom_gpd_wkt_ll[ilat][ilon] = tmp\n","\n","elif ( Path(input_file).suffix == '.shp'):\n","\n","    # -------------------------------\n","    # Grid-cells are actually polygons in a shapefile\n","    # -------------------------------\n","    print(' ')\n","    print('   (3) Extract shapes from shapefile ...')\n","\n","    grid_cell_geom_gpd_wkt_ea = [ [ [] for ilon in range(nlon) ] for ilat in range(nlat) ]   # nlat = nshapes, nlon = 1\n","    for ishape in range(nshapes):\n","\n","        idx = np.where(model_grid_shp[key_colname_model] == ishape)[0]\n","        if len(idx) == 0:\n","            print(\"Polygon ID = {} not found in '{}'. Numbering of shapefile attribute '{}' needs to be [0 ... {}-1].\".format(ishape,input_file,key_colname_model,nshapes))\n","            raise ValueError('Polygon ID not found.')\n","        if len(idx) > 1:\n","            print(\"Polygon ID = {} found multiple times in '{}' but needs to be unique. Numbering of shapefile attribute '{}' needs to be [0 ... {}-1].\".format(ishape,input_file,key_colname_model,nshapes))\n","            raise ValueError('Polygon ID not unique.')\n","        idx  = idx[0]\n","        poly = model_grid_shp.loc[idx].geometry\n","        try:\n","            grid_cell_geom_gpd_wkt_ea[ishape][0] = ogr.CreateGeometryFromWkt(poly.to_wkt())\n","        except:\n","            grid_cell_geom_gpd_wkt_ea[ishape][0] = ogr.CreateGeometryFromWkt(poly.wkt)\n","\n","else:\n","\n","    print(\"File extension found: {}\".format(input_file.split('.')[-1]))\n","    raise ValueError('Input file needs to be either NetCDF (*.nc) or a Shapefile (*.shp).')\n","\n","# -------------------------------\n","# Derive overlay and calculate weights\n","# -------------------------------\n","print(' ')\n","print('   (4) Deriving weights ...')\n","\n","filename = output_file\n","ff       = open(filename,'w')\n","ff.write(':GridWeights                     \\n')\n","ff.write('   #                                \\n')\n","ff.write('   # [# HRUs]                       \\n')\n","ff.write('   :NumberHRUs       {0}            \\n'.format(nsubbasins))\n","ff.write('   :NumberGridCells  {0}            \\n'.format(nshapes))\n","ff.write('   #                                \\n')\n","ff.write('   # [HRU ID] [Cell #] [w_kl]       \\n')\n","\n","if dojson:\n","    cells_to_write_to_geojson = []\n","    geojson = []\n","\n","error_dict = {}\n","for ikk,kk in enumerate(keys):\n","\n","    ibasin = shape.loc[kk]\n","\n","    area_basin = coord_catch_wkt[kk].Area()\n","    enve_basin = coord_catch_wkt[kk].GetEnvelope()   # bounding box around basin (for easy check of proximity)\n","\n","    area_all = 0.0\n","    ncells   = 0\n","\n","    data_to_write = []\n","    for ilat in range(nlat):\n","        for ilon in range(nlon):\n","\n","            enve_gridcell  = grid_cell_geom_gpd_wkt_ea[ilat][ilon].GetEnvelope()   # bounding box around grid-cell (for easy check of proximity)\n","            grid_is_close  = check_proximity_of_envelops(enve_gridcell, enve_basin)\n","\n","            if grid_is_close: # this check decreases runtime DRASTICALLY (from ~6h to ~1min)\n","\n","                grid_cell_area = grid_cell_geom_gpd_wkt_ea[ilat][ilon].Area()\n","\n","                inter = (grid_cell_geom_gpd_wkt_ea[ilat][ilon].Buffer(0.0)).Intersection(coord_catch_wkt[kk].Buffer(0.0)) # \"fake\" buffer to avoid invalid polygons and weirdos dumped by ArcGIS\n","                area_intersect = inter.Area()\n","\n","                area_all += area_intersect\n","                if area_intersect > 0:\n","\n","                    ncells += 1\n","                    cell_ID = ilat*nlon+ilon\n","                    data_to_write.append( [int(ibasin[key_colname]),ilat,ilon,cell_ID,area_intersect/area_basin] )\n","\n","                    if dojson:\n","                        if cell_ID not in cells_to_write_to_geojson:\n","                            cells_to_write_to_geojson.append( cell_ID )\n","                            tmp = grid_cell_geom_gpd_wkt_ll[ilat][ilon].Buffer(0.0).ExportToJson()\n","                            geojson.append({\"type\":\"Feature\",\"geometry\":gjs.loads(tmp),\"properties\":{\"CellId\":cell_ID,\"Area\":grid_cell_area}})\n","\n","    # mismatch between area of subbasin (shapefile) and sum of all contributions of grid cells (model output)\n","    error = (area_basin - area_all)/area_basin\n","\n","\n","    if abs(error) > area_error_threshold and area_basin > 500000.:\n","        # record all basins with errors larger 5% (if basin is larger than 0.5 km2)\n","        error_dict[int(ibasin[key_colname])] = [ error, area_basin ]\n","        for idata in data_to_write:\n","            print(\"   >>> {0},{1},{2},{3},{4}\".format(idata[0],idata[1],idata[2],idata[3],idata[4]))\n","            ff.write(\"   {0}   {1}   {2}\\n\".format(idata[0],idata[3],idata[4]))\n","\n","    else:\n","        # adjust such that weights sum up to 1.0\n","        for idata in data_to_write:\n","            corrected = idata[4] * 1.0/(1.0-error)\n","            print(\"   >>> {0},{1},{2},{3},{4}  (corrected to {5})\".format(idata[0],idata[1],idata[2],idata[3],idata[4],corrected))\n","            ff.write(\"   {0}   {1}   {2}\\n\".format(idata[0],idata[3],corrected))\n","\n","        if error < 1.0:\n","            area_all *= 1.0/(1.0-error)\n","        error    = 0.0\n","\n","    print('   >>> (Sub-)Basin: {0} ({1} of {2})'.format(int(ibasin[key_colname]),ikk+1,nsubbasins))\n","    print('   >>> Derived area of {0}  cells: {1}'.format(ncells,area_all))\n","    print('   >>> Read area from shapefile:   {0}'.format(area_basin))\n","    print('   >>> error:                      {0}%'.format(error*100.))\n","    print('   ')\n","\n","ff.write(':EndGridWeights \\n')\n","ff.close()\n","\n","# write geoson\n","if dojson:\n","\n","    json_file = '.'.join(filename.split('.')[0:-1])+\".json\"\n","\n","    geojson = {\"type\":\"FeatureCollection\",\"features\":geojson}\n","    with open(json_file, 'w') as outfile:\n","      gjs.dump(geojson, outfile)\n","\n","\n","\n","# print out all subbasins that have large errors\n","if (error_dict != {}):\n","    print('')\n","    print('WARNING :: The following (sub-)basins show large mismatches between provided model')\n","    print('           grid and domains spefied in the shapefile. It seems that your model')\n","    print('           output is not covering the entire domain!')\n","    print(\"           { <basin-ID>: <error>, ... } = \")\n","    for attribute, value in error_dict.items():\n","        print('               {0} : {1:6.2f} % of {2:8.1f} km2 basin'.format(attribute, value[0]*100., value[1]/1000./1000.))\n","    print('')\n","\n","print('')\n","print('Wrote: ',filename)\n","if dojson:\n","    print('Wrote: ',json_file)\n","print('')"]},{"cell_type":"markdown","metadata":{"id":"yZvDbrPiETqU"},"source":["**References**\n","\n","Han, M., Mai, J., Tolson, B. A., Craig, J. R., Gaborit, É., Liu, H., and Lee, K. (2020a):\n","Subwatershed-based lake and river routing products for hydrologic and land surface models applied over Canada\n","Canadian Water Resources Journal, 0, 1-15. (publication)\n","\n","Han, M. et al. (2020b):\n","An automated GIS toolbox for watershed delineation with lakes\n","In preparation.\n","\n","Han, M., Mai, J., Tolson, B. A., Craig, J. R., Gaborit, É., Liu, H., and Lee, K. (2020c):\n","A catchment-based lake and river routing product for hydrologic and land surface models in Canada (Dataset)\n","Zenodo. (dataset)"]},{"cell_type":"markdown","metadata":{"id":"7QrfZEcv4oOV"},"source":["### <font color=#5559AB> 3c) DayMet Data</font>\n","\n","This section downloads Daymet data for the years of interest, merges the years together, and formats the RVT file for the Raven model.\n","\n","Daymet provides long-term, continuous, gridded estimates of daily weather and climatology variables produced on a 1 km x 1 km gridded surface by interpolating and extrapolating ground-based observations through statistical modeling techniques.\n","\n","Only a <font color=red>shapefile of the study area</font> is required.\n","\n","Areas included in the dataset: <font color=blue>North America</font>\n","\n","Check out the short video [4.1 DayMet Data - Magpie Workflow](https://youtu.be/wi6lUwNJ2d0) for more information"]},{"cell_type":"markdown","metadata":{"id":"O-JItRdg95Xj"},"source":["**Daymet Variables**\n","\n","Parameter | Abbr | Units | Description\n","--- | --- | --- | ---\n","Precipitation | prcp | mm | Daily total precipitation in millimeters. Sum of all forms of precipitation converted to a water-equivalent depth.\n","Shortwave radiation | srad | W/m2 | Incident shortwave radiation flux density in watts per square meter, taken as an average over the daylight period of the day.\n","Maximum air temperature | tmax | degrees C | Daily maximum 2 m air temperature in degrees Celsius.\n","Minimum air temperature | tmin | degrees C | Daily minimum 2 m air temperature in degrees Celsius."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_mBhL6dnD1Ta"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"rasterstats==0.19.0\", \"requests\",\"earthengine-api\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import ee\n","import requests\n","import rasterstats as rs\n","from shapely.geometry import Point\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","# Trigger the authentication flow.\n","service_account = 'magpie-developer@magpie-id-409519.iam.gserviceaccount.com'\n","credentials = ee.ServiceAccountCredentials(service_account, os.path.join(main_dir,'extras','magpie-key.json'))\n","\n","# Initialize the library.\n","ee.Initialize(credentials)\n","\n","def read_and_sample_data(main_dir, day_indiv_dir, point_distance, buffer_distance):\n","    # Find name of shapefile\n","    shp_file_path = os.path.join(main_dir, 'shapefile')\n","    shp_file_name = next((f for f in os.listdir(shp_file_path) if f.endswith(\".shp\")), None)\n","    if not shp_file_name:\n","        raise FileNotFoundError(\"No shapefile found in the specified directory.\")\n","\n","    # Read the original shapefile\n","    shapefile_gdf = gpd.read_file(os.path.join(shp_file_path, shp_file_name))\n","    print(\"Original CRS:\", shapefile_gdf.crs)\n","\n","    # Reproject shapefile to EPSG:4326\n","    shapefile_gdf = shapefile_gdf.to_crs(epsg=4326)\n","    print(\"Reprojected CRS:\", shapefile_gdf.crs)\n","\n","    # Create a buffer around the shapefile\n","    buffered_shapefile = shapefile_gdf.geometry.buffer(buffer_distance)\n","\n","    # Get the bounding box of the buffered shapefile\n","    minx, miny, maxx, maxy = buffered_shapefile.total_bounds\n","\n","    # Generate a grid of points within the bounding box\n","    x_coords = np.arange(minx, maxx, point_distance)\n","    y_coords = np.arange(miny, maxy, point_distance)\n","    points = [Point(x, y) for x in x_coords for y in y_coords]\n","\n","    grid_gdf = gpd.GeoDataFrame(geometry=points, crs=\"EPSG:4326\")\n","    grid_gdf = grid_gdf[grid_gdf.within(buffered_shapefile.unary_union)]\n","\n","    # Extract lat and lon from geometry\n","    grid_gdf['lon'] = grid_gdf.geometry.x\n","    grid_gdf['lat'] = grid_gdf.geometry.y\n","\n","    # Save selected points to CSV\n","    grid_gdf[['lon', 'lat']].to_csv(os.path.join(day_indiv_dir, 'selected_lat_lon.csv'), index=False)\n","\n","    # Plotting\n","    fig, ax = plt.subplots()\n","    shapefile_gdf.plot(ax=ax, color='gray', edgecolor='black', alpha=0.5, label='Original Shapefile')\n","    gpd.GeoDataFrame(geometry=buffered_shapefile, crs=\"EPSG:4326\").plot(ax=ax, color='blue', edgecolor='black', alpha=0.3, label='Buffered Shapefile')\n","    grid_gdf.plot(ax=ax, color='red', markersize=10, label='Selected Points (Grid)')\n","    plt.title('Generated Grid Points over Shapefile (Clipped)')\n","    plt.legend()\n","    plt.show()\n","\n","def download_daymet_data(lat, lon, variables, start_date, end_date, output_dir):\n","    base_url = 'https://daymet.ornl.gov/single-pixel/api/data'\n","\n","    # Prepare parameters for the request\n","    params = {\n","        'lat': lat,\n","        'lon': lon,\n","        'vars': ','.join(variables),\n","        'start': start_date,\n","        'end': end_date\n","    }\n","\n","    # Construct the URL\n","    url = f\"{base_url}?{'&'.join([f'{key}={value}' for key, value in params.items()])}\"\n","\n","    # Create output directory if it doesn't exist\n","    os.makedirs(output_dir, exist_ok=True)\n","\n","    # Download data using wget\n","    command = f\"wget --content-disposition '{url}' -P {output_dir}\"\n","    os.system(command)\n","\n","\n","def process_csv_files(csv_files_dir, min_year_val, max_year_val, params, units_lst, daymet_ind_dir):\n","    count = -1\n","\n","    for csv_file in os.listdir(csv_files_dir):\n","        if csv_file.endswith(\".csv\"):  # Assuming only CSV files are processed; adjust if needed\n","            count += 1\n","\n","            # Load the CSV file\n","            forc_input = pd.read_csv(os.path.join(csv_files_dir, csv_file), skiprows=6)\n","\n","            # Create a complete date range\n","            forc_input['date'] = pd.to_datetime(forc_input['year'].astype(str) + forc_input['yday'].astype(str), format='%Y%j')\n","            full_date_range = pd.date_range(start=f\"{min_year_val}-01-01\", end=f\"{max_year_val}-12-31\", freq='D')\n","\n","            # Reindex to include all dates\n","            forc_input = forc_input.set_index('date').reindex(full_date_range).reset_index()\n","            forc_input.rename(columns={'index': 'date'}, inplace=True)\n","\n","            # Fill missing year and yday columns\n","            forc_input['year'] = forc_input['date'].dt.year\n","            forc_input['yday'] = forc_input['date'].dt.dayofyear\n","\n","            # Handle missing data with defaults or interpolations\n","            forc_input.fillna(method='ffill', inplace=True)  # Forward-fill as default (adjust as needed)\n","\n","            if 'TEMP_DAILY_MAX' in params:\n","              forc_input['tmax (deg c)'] = forc_input['tmax (deg c)'].replace(-0.0, 0.0)\n","            if 'TEMP_DAILY_MIN' in params:\n","              forc_input['tmin (deg c)'] = forc_input['tmin (deg c)'].replace(-0.0, 0.0)\n","            if 'SW_RADIA' in params:\n","              forc_input['srad (W/m^2)'] = forc_input['srad (W/m^2)'].multiply(0.0864).round(2)\n","            if 'REL_HUMIDITY' in params:\n","              if 'tmax (deg c)' in forc_input and 'tmin (deg c)' in forc_input:\n","                # Calculate mean temperature\n","                forc_input['meanT'] = (forc_input['tmax (deg c)'] + forc_input['tmin (deg c)']) / 2\n","\n","                # Calculate saturation vapor pressure and convert it to Pa\n","                forc_input['SatVap'] = 6.11 * np.exp((17.27 * forc_input['meanT']) / (237.3 + forc_input['meanT'])) * 100\n","\n","                # Calculate relative humidity\n","                forc_input['vp (Pa)'] = round((forc_input['vp (Pa)'] / forc_input['SatVap']),2)\n","\n","                # Adjust relative humidity values that are exactly 0 or exceed 100\n","                forc_input['vp (Pa)'] = np.where((forc_input['vp (Pa)'] == 0) | (forc_input['vp (Pa)'] > 100), 100, forc_input['vp (Pa)'])\n","\n","                # Drop the 'meanT' column as it is no longer needed\n","                forc_input.drop('meanT', axis=1, inplace=True)\n","                # Drop the 'meanT' column as it is no longer needed\n","                forc_input.drop('SatVap', axis=1, inplace=True)\n","              else:\n","                print('Min and max temperature is required to compute relative humidity')\n","\n","            rm_col_df = forc_input.drop(['year', 'yday', 'date'], axis=1)\n","            t3 = '\\n'.join(rm_col_df.astype(str).apply(lambda x: ', '.join(x), axis=1))\n","\n","            output_file_path = os.path.join(daymet_ind_dir, f\"File{count}.rvt\")\n","            with open(output_file_path, \"a\") as f:\n","                print(\":MultiData\", file=f)\n","                print(f\"{min_year_val}-01-01 00:00:00 1.0 {len(forc_input.iloc[:, 0])}\", file=f)\n","                print(f\":Parameters,{params}\", file=f)\n","                print(f\":Units,{units_lst}\", file=f)\n","                print(f\"{t3}\", file=f)\n","                print(\":EndMultiData\", file=f)\n","\n","            print(f'Formatted DayMet data for Raven input saved to {output_file_path}')\n","\n","def check_projection(shp_file_path, main_dir, temporary_dir):\n","    # Print header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Check projection of shapefile')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find name of shapefile\n","    for shp_file in os.listdir(os.path.join(shp_file_path)):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Check if a shapefile was found\n","    if not shp_file_name:\n","        print('No shapefile found in the specified directory.')\n","        return\n","\n","    # Print shapefile information\n","    print('Shapefile name: ', shp_file_name)\n","    shp_file_full_path = os.path.join(main_dir, \"shapefile\", shp_file_name)\n","    print('Shapefile path: ', shp_file_full_path)\n","\n","    # Read shapefile\n","    shp_lyr_check = gpd.read_file(shp_file_full_path)\n","    print('Shapefile CRS: ', shp_lyr_check.crs)\n","\n","    # Check if CRS is different from EPSG:4326\n","    if shp_lyr_check.crs != 'EPSG:4326':\n","        # Reproject to EPSG:4326\n","        shp_lyr_crs = shp_lyr_check.to_crs(epsg=4326)\n","        shp_lyr_crs.to_file(shp_file_full_path)\n","        print('Shapefile layer has been reprojected to match EPSG:4326')\n","    else:\n","        shp_lyr_crs = shp_lyr_check\n","        print('Coordinate systems match!')\n","\n","def download_DEM(shp_file_path, data_source, band_name, scale, temporary_dir):\n","    # Print header\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Download DEM')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Authenticate and initialize Earth Engine (uncomment if needed)\n","    # ee.Authenticate()\n","    # ee.Initialize()\n","\n","    # Define buffer size\n","    buffer_size = 0.01\n","\n","    # Find name of shapefile\n","    shp_file_path = os.path.join(main_dir, 'shapefile')\n","    for shp_file in os.listdir(shp_file_path):\n","        if shp_file.endswith(\".shp\"):\n","            shp_file_name = shp_file\n","\n","    # Determine the boundary of the provided shapefile\n","    bounds = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name)).bounds\n","    west, south, east, north = bounds = bounds.loc[0]\n","    west -= buffer_size * (east - west)\n","    east += buffer_size * (east - west)\n","    south -= buffer_size * (north - south)\n","    north += buffer_size * (north - south)\n","\n","    # Create Earth Engine Image and define region\n","    img = ee.Image(data_source)\n","    region = ee.Geometry.BBox(west, south, east, north)\n","\n","    # Get download URL for the specified region and parameters\n","    url = img.getDownloadUrl({\n","        'bands': [band_name],\n","        'region': region,\n","        'scale': scale,\n","        'format': 'GEO_TIFF'\n","    })\n","\n","    # Define output directory for downloaded DEM\n","    dem_dir = os.path.join(temporary_dir, 'DEM')\n","    if not os.path.exists(dem_dir):\n","        os.makedirs(dem_dir)\n","\n","    # Download the DEM and save it locally\n","    response = requests.get(url)\n","    with open(os.path.join(dem_dir, 'dem.tif'), 'wb') as fd:\n","        fd.write(response.content)\n","\n","def extract_elev_data(temporary_dir,day_indiv_dir):\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Extract elevation data')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    # Find the DEM file name\n","    dem_files = [file for file in os.listdir(os.path.join(temporary_dir, 'DEM')) if file.endswith(\".tif\")]\n","    if not dem_files:\n","        raise FileNotFoundError(\"No DEM files found in the specified directory.\")\n","\n","    dem_file_name = dem_files[0]  # Assuming only one DEM file for simplicity\n","\n","    # Define path/open raster\n","    dem_path = os.path.join(temporary_dir, 'DEM', dem_file_name)\n","    print(dem_path)\n","    dem_lyr = rxr.open_rasterio(dem_path, masked=True).squeeze()\n","\n","    # Isolate lat and lon variables\n","    df_coords = pd.read_csv(os.path.join(day_indiv_dir, 'selected_lat_lon.csv'))\n","    df_coords1 = pd.DataFrame({\"lon\": df_coords.lon, \"lat\": df_coords.lat}).reset_index(drop=True)\n","\n","    # Generate points shapefile of lats and lon\n","    gdf = gpd.GeoDataFrame(\n","        df_coords1, geometry=gpd.points_from_xy(df_coords1.lon, df_coords1.lat), crs=\"EPSG:4326\")\n","\n","    # Create a buffered polygon layer from your plot location points\n","    plots_poly = gdf.copy()\n","\n","    # Buffer each point using a 20-meter circle radius\n","    # and replace the point geometry with the new buffered geometry\n","    plots_poly[\"geometry\"] = gdf.buffer(0.2)  # Buffer each point with a 5-meter radius\n","    plots_poly.head()\n","\n","    # Export the buffered point layer as a shapefile to use in zonal stats\n","    plot_buffer_path = os.path.join(temporary_dir, 'DEM', 'plot_buffer.shp')\n","    plots_poly.to_file(plot_buffer_path)\n","\n","    # Extract zonal stats\n","    elev_zonal = rs.zonal_stats(plot_buffer_path,\n","                                  dem_lyr.values,\n","                                  nodata=-999,\n","                                  affine=dem_lyr.rio.transform(),\n","                                  geojson_out=True,\n","                                  copy_properties=True,\n","                                  stats=\"mean\")\n","\n","    # Turn extracted data into a pandas geodataframe\n","    elev_zonal_df = gpd.GeoDataFrame.from_features(elev_zonal)\n","    df_elev = pd.DataFrame(elev_zonal_df['mean'])\n","    return df_elev, df_coords1\n","\n","def generate_RVT_gauges(df_elev, df_coords1,model_name):\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Generate RVT for gauges')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    with open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\") as f:\n","        print(f\"#------------------------------------------------------------------------\", file=f)\n","        print(f\"# Climate Stations List\", file=f)\n","        print(f\"#------------------------------------------------------------------------\\n#\", file=f)\n","\n","    lst_vals = list(range(0, len(df_coords1['lon'])))\n","\n","    for n in lst_vals:\n","        #n1 = forc_input[(forc_input.lon == df_coords1.lon[n])&(forc_input.lat == df_coords1.lat[n])].reset_index(drop=True)\n","        #print(n1)\n","        f = open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\")\n","        print(f\":Gauge File{n}\", file=f)\n","        print(f\":Latitude {df_coords1['lat'][n]}\", file=f)\n","        print(f\":Longitude {df_coords1['lon'][n]}\", file=f)\n","        print(f\":Elevation {df_elev['mean'][n]}\", file=f)\n","        print(f\":RedirectToFile DayMet/File{n}.rvt\", file=f)\n","        print(f\":EndGauge\", file=f)\n","        print(f\"#\", file=f)\n","        f.close()\n","\n","    f = open(os.path.join(main_dir, 'workflow_outputs','RavenInput','awslist.txt'), \"a\")\n","    print(f\"#------------------------------------------------------------------------\", file=f)\n","    print(f\"# Climate Stations List\", file=f)\n","    print(f\"#------------------------------------------------------------------------\\n#\", file=f)\n","    f.close()\n","\n","    lst_vals = list(range(0, len(df_coords1['lon'])))\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"913x9t8ND4a8"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define years of interest** </font>\n","\n","#@markdown Input the start year (min_year) and the end year (max_year)\n","\n","min_year = \"2000\" #@param {type:\"string\"}\n","max_year = \"2005\" #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Define number of points of interest** </font>\n","\n","#@markdown <font color=grey> for example, 0.01\n","\n","point_distance = 0.01 #@param\n","\n","#@markdown <font color=#5559AB> **Define buffer size around study area** </font>\n","\n","#@markdown <font color=grey> for example, 0.05\n","\n","buffer_distance = 0.01 #@param\n","\n","# back-up output directory\n","output_forc_dir = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data', 'forcing_backups')\n","os.makedirs(output_forc_dir, exist_ok=True)\n","print(\"created folder:\", output_forc_dir) if not os.path.isdir(output_forc_dir) else None\n","\n","day_indiv_dir = os.path.join(temporary_dir, 'DayMet_individual_files')\n","os.makedirs(day_indiv_dir, exist_ok=True)\n","print(\"created folder:\", day_indiv_dir) if not os.path.isdir(day_indiv_dir) else None\n","\n","# download DEM layer\n","read_and_sample_data(main_dir, day_indiv_dir, point_distance, buffer_distance)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"WjW2K-2LD9xY"},"outputs":[],"source":["# Lists to store information about variables of interest\n","variables_of_interest = []\n","params = []\n","units = []\n","\n","#@markdown <font color=#5559AB> **Select variables** </font>\n","\n","# Daymet variables configuration\n","prcp_dayMet = True #@param {type:\"boolean\"}\n","srad_dayMet = False #@param {type:\"boolean\"}\n","tmax_dayMet = True #@param {type:\"boolean\"}\n","tmin_dayMet = True #@param {type:\"boolean\"}\n","vp_dayMet = False #@param {type:\"boolean\"}\n","\n","\n","# Check which Daymet variables to include and populate lists\n","if prcp_dayMet:\n","    variables_of_interest.append('prcp')\n","    params.append('PRECIP')\n","    units.append('mm/d')\n","if srad_dayMet:\n","    variables_of_interest.append('srad')\n","    params.append('SW_RADIA')\n","    units.append('MJ/m^2/d')\n","if tmax_dayMet:\n","    variables_of_interest.append('tmax')\n","    params.append('TEMP_DAILY_MAX')\n","    units.append('degC')\n","if tmin_dayMet:\n","    variables_of_interest.append('tmin')\n","    params.append('TEMP_DAILY_MIN')\n","    units.append('degC')\n","if vp_dayMet:\n","    variables_of_interest.append('vp')\n","    params.append('REL_HUMIDITY')\n","    units.append('0..1')\n","\n","# Create a directory for temporary files\n","day_indiv_dir = os.path.join(temporary_dir, 'DayMet_individual_files')\n","os.makedirs(day_indiv_dir, exist_ok=True)\n","\n","# Read latitude and longitude data from a CSV file\n","df_lat_lon = pd.read_csv(os.path.join(day_indiv_dir, 'selected_lat_lon.csv'))\n","latitudes = df_lat_lon['lat'].tolist()\n","longitudes = df_lat_lon['lon'].tolist()\n","\n","# Specify Daymet download parameters\n","start_date = f\"{min_year}-01-01\"\n","end_date = f\"{max_year}-12-31\"\n","\n","# Create a directory to store downloaded Daymet data\n","output_directory = os.path.join(temporary_dir, 'csv_files')\n","os.makedirs(output_directory, exist_ok=True)\n","\n","# Loop through each location and download Daymet data\n","for lat, lon in zip(latitudes, longitudes):\n","    print(f\"Downloading data for Latitude: {lat}, Longitude: {lon}\")\n","    download_daymet_data(lat, lon, variables_of_interest, start_date, end_date, output_directory)\n","\n","print(\"Data download complete.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ZaD1WFc5EQWT"},"outputs":[],"source":["#@markdown <font color=grey> **Generate RVT files for each of the points** </font>\n","\n","# Example usage\n","csv_files_directory = output_directory\n","parameters = ', '.join(params)  # Replace with your actual parameters\n","units_lst = ', '.join(units) # Replace with your actual units\n","daymet_output_directory = os.path.join(main_dir, 'workflow_outputs','RavenInput','DayMet')\n","os.makedirs(daymet_output_directory, exist_ok=True)\n","\n","process_csv_files(csv_files_directory, min_year, max_year, parameters, units_lst, daymet_output_directory)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ynSYC7IdEXNi"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Select Elevation Layer** </font>\n","\n","#@markdown either use the exsisting DEM layer saved in the 1_HRU_data folder, download a MERIT DEM using Google Earth Engine, or upload your own DEM layer\n","\n","select_elev_layer = 'use_existing' #@param [\"use_existing\",\"download\", \"upload\"] {type:\"string\"}\n","\n","# generate elevation data\n","if select_elev_layer == 'use_existing':\n","  elev_path = os.path.join(main_dir, 'workflow_outputs', '1_HRU_data')\n","  df_elev, df_coords1 = extract_elev_data(elev_path,day_indiv_dir)\n","elif select_elev_layer == 'download':\n","  # shapefile path\n","  shp_file_path = os.path.join(main_dir, 'shapefile')\n","  data_source = \"MERIT/DEM/v1_0_3\"\n","  band_name = \"dem\"\n","  scale = 90\n","  check_projection(shp_file_path, main_dir,temporary_dir)\n","  download_DEM(shp_file_path,data_source, band_name, scale, temporary_dir)\n","  df_elev, df_coords1 = extract_elev_data(temporary_dir,day_indiv_dir)\n","elif select_elev_layer == 'upload':\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Upload DEM')\n","  print('-----------------------------------------------------------------------------------------------')\n","  # Define temporary directory for DEM\n","  dem_temp_dir = os.path.join(temporary_dir, 'DEM')\n","  os.makedirs(dem_temp_dir, exist_ok=True)\n","  print(\"Created folder:\", dem_temp_dir) if not os.path.isdir(dem_temp_dir) else None\n","  print('\\n')\n","  print(f'drag-and-drop DEM (.tif) file into following folder: {dem_temp_dir}')\n","  response = input(\"Have you uploaded the DEM file (yes or no): \")\n","  if response == \"yes\":\n","    df_elev, df_coords1 = extract_elev_data(temporary_dir,day_indiv_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YAqK-9H1EaNr","cellView":"form"},"outputs":[],"source":["#@markdown <font color=grey> **Generate Raven RVT input files for DayMet data** </font></br>\n","\n","generate_RVT_gauges(df_elev, df_coords1,model_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BwZ1VQ2mEdPA"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"FpyOFxvOEg7D"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","# Define boolean variables based on user inputs\n","prcp_dayMet_res = 'yes' if prcp_dayMet else 'no'\n","srad_dayMet_res = 'yes' if srad_dayMet else 'no'\n","tmax_dayMet_res = 'yes' if tmax_dayMet else 'no'\n","tmin_dayMet_res = 'yes' if tmin_dayMet else 'no'\n","vp_dayMet_res = 'yes' if vp_dayMet else 'no'\n","\n","# Define years of interest\n","min_year = \"2000\"\n","max_year = \"2001\"\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False  # @param {type:\"boolean\"}\n","\n","# Create a dictionary to store configuration data\n","data = {\n","    \"_comment\": \"------------ 3c) DAYMET ---------------\",\n","\n","    \"prcp_dayMet\": prcp_dayMet_res,\n","    \"tmax_dayMet\": tmax_dayMet_res,\n","    \"tmin_dayMet\": tmin_dayMet_res,\n","    \"vp_dayMet\": vp_dayMet_res,\n","    \"srad_dayMet\": srad_dayMet_res,\n","\n","    \"point_distance\": point_distance,\n","    \"buffer_distance\": buffer_distance,\n","\n","    \"min_year_dayMet\": min_year,\n","    \"max_year_dayMet\": max_year,\n","\n","    \"select_elev_layer\": select_elev_layer,\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path, \"3c_daymet.json\")\n","\n","# Writing data to the JSON file\n","with open(file_path, 'w') as json_file:\n","    json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)\n"]},{"cell_type":"markdown","metadata":{"id":"VvTTjB1-o922"},"source":["**References**\n","\n","Thornton, P. E., R. Shrestha, M. Thornton, S.-C. Kao, Y. Wei, and B. E. Wilson. (2022). Gridded daily weather data for North America with comprehensive uncertainty quantification. Scientific Data 8."]},{"cell_type":"markdown","metadata":{"id":"uGPmVyCjMjXX"},"source":["### <font color=#5559AB> 3d) Environment Canada Climate Data </font>\n","\n","This Python subsection downloads and formats Environment Canada climate station records into RVT format file usable in Raven. This subsection will be replaced with \"weathercan\" at a later date when it is available on CRAN.\n","\n","Areas included in the dataset: <font color=blue>Canada</font>\n","\n","Check out the short video [Downloading and Formatting Environment Canada Climate Data for Raven: Magpie Tutorial](https://youtu.be/N1Cw4OgI7bo) for more information."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UKLUXvbU1vmQ"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"folium\", \"branca\",\"requests\",\n","                      \"re\", \"plotly\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import folium\n","import branca\n","from IPython.display import display\n","import requests\n","from bs4 import BeautifulSoup\n","import re\n","import plotly.express as px\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def fancy_html(row):\n","    climName = climate_stations_updated['Name'].iloc[row]\n","    lat_info = climate_stations_updated['Latitude'].iloc[row]\n","    lon_info = climate_stations_updated['Longitude'].iloc[row]\n","\n","    html = f\"\"\"<!DOCTYPE html>\n","        <html>\n","        <p>Climate Station Name: {climName}</td></p>\n","        <p>Lat: {lat_info}</td></p>\n","        <p>Lon: {lon_info}</td></p>\n","        </html>\n","    \"\"\"\n","    return html\n","\n","def interactive_map_climate_stations(main_dir,climate_stations_updated):\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Interactive map')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    shp_file_path = os.path.join(main_dir, 'shapefile')\n","\n","    # Generate map with rectangular guide to assist in identifying the ideal subbasin/gauges to use\n","    shp_file_name = next((file for file in os.listdir(shp_file_path) if file.endswith(\".shp\")), None)\n","    if shp_file_name:\n","        shp_boundary = gpd.read_file(os.path.join(main_dir, 'shapefile', shp_file_name))\n","        # Reproject the GeoDataFrame to EPSG:4326 (WGS84)\n","        shp_boundary = shp_boundary.to_crs(epsg=4326)\n","\n","        # Get the bounds and extract the center for map initialization\n","        bounds = shp_boundary.bounds.loc[0]\n","        shp_bounds = [bounds['miny'], bounds['minx']]\n","        # Create a Folium map centered at the reprojected bounds\n","        map = folium.Map(location=shp_bounds, zoom_start=10)\n","        # Add the GeoJSON representation of the geometry to the map\n","        folium.GeoJson(data=shp_boundary[\"geometry\"]).add_to(map)\n","\n","        for i in range(len(climate_stations_updated)):\n","            html = fancy_html(i)\n","            iframe = branca.element.IFrame(html=html, width=200, height=200)\n","            popup = folium.Popup(iframe, parse_html=True)\n","            folium.Marker([climate_stations_updated['Latitude'].iloc[i], climate_stations_updated['Longitude'].iloc[i]],\n","                          popup=popup).add_to(map)\n","\n","        display(map)\n","\n","\n","def station_look_up(province,start_year,temporary_dir):\n","    \"\"\"\n","    Perform a station look-up for climate data and save the results to a CSV file.\n","\n","    This function uses the Environment Canada website to look up climate stations based on province and start year.\n","\n","    Parameters:\n","    - variable['province']: Province abbreviation (e.g., 'ON' for Ontario)\n","    - variable['climate_start_yr']: Start year for climate data retrieval\n","\n","    Outputs:\n","    - A CSV file containing station information (StationID, Name, Intervals, Year Start, Year End)\n","    \"\"\"\n","\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Station look-up')\n","    print('-----------------------------------------------------------------------------------------------')\n","\n","    max_pages = 10  # Number of maximum pages to parse (EC's limit is 100 rows per page)\n","\n","    # Store each page in a list and parse them later\n","    soup_frames = []\n","\n","    # Download and parse each page\n","    for i in range(max_pages):\n","        start_row = 1 + i * 100\n","        print('Downloading Page: ', i)\n","\n","        base_url = \"http://climate.weather.gc.ca/historical_data/search_historic_data_stations_e.html?\"\n","        query_province = \"searchType=stnProv&timeframe=1&lstProvince={}&optLimit=yearRange&\".format(province)\n","        query_year = \"StartYear={}&EndYear=2025&Year=2025&Month=5&Day=29&selRowPerPage=100&txtCentralLatMin=0&txtCentralLatSec=0&txtCentralLongMin=0&txtCentralLongSec=0&\".format(start_year)\n","        query_start_row = \"startRow={}\".format(start_row)\n","\n","        response = requests.get(base_url + query_province + query_year + query_start_row)\n","        soup = BeautifulSoup(response.text, 'html.parser')\n","        soup_frames.append(soup)\n","\n","    # Empty list to store the station data\n","    station_data = []\n","\n","    # Parse each soup\n","    for soup in soup_frames:\n","        forms = soup.findAll(\"form\", {\"id\": re.compile('stnRequest*')})\n","        for form in forms:\n","            try:\n","                # Extract station information\n","                station_id = form.find(\"input\", {\"name\": \"StationID\"})['value']\n","                name = form.find(\"input\", {\"name\": \"lstProvince\"}).find_next_siblings(\"div\")[0].text\n","                timeframes = form.find(\"select\", {\"name\": \"timeframe\"}).findChildren()\n","                intervals = [t.text for t in timeframes]\n","                years = form.find(\"select\", {\"name\": \"Year\"}).findChildren()\n","                min_year = years[0].text\n","                max_year = years[-1].text\n","\n","                # Store the data in an array\n","                data = [station_id, name, intervals, min_year, max_year]\n","                station_data.append(data)\n","            except:\n","                pass\n","\n","    # create temporary dir\n","    station_temp_path = os.path.join(temporary_dir,'station_lookup_dir')\n","    os.makedirs(station_temp_path, exist_ok=True)\n","\n","    # Create a pandas dataframe using the collected data and give it the appropriate column names\n","    stations_df = pd.DataFrame(station_data, columns=['StationID', 'Name', 'Intervals', 'Year Start', 'Year End'])\n","    stations_df.to_csv(os.path.join(station_temp_path, f'station_lookup_{province}.csv'))\n","\n","def stationID_download(station_ID_lst, start_date, end_date, temporary_dir):\n","  all_files_merged = []\n","  for stationID in station_ID_lst:\n","      print('\\n-----------------------------------------------------------------------------------------------')\n","      print(f'( ) {stationID} station download')\n","      print('-----------------------------------------------------------------------------------------------')\n","      # Define time step\n","      time_step = \"daily\"\n","      time_step_val = {\"hourly\": 1, \"daily\": 2, \"monthly\": 3}[time_step]\n","\n","      # Define station folder\n","      station_dir = os.path.join(temporary_dir, 'weather_data', str(stationID))\n","      os.makedirs(station_dir, exist_ok=True)\n","\n","      # Split the date string\n","      start_year, start_month, start_day = start_date.split('-')\n","      end_year, end_month, end_day = end_date.split('-')\n","\n","      # Define month list\n","      month = [\"01\", \"02\", \"03\", \"04\", \"05\", \"06\", \"07\", \"08\", \"09\", \"10\", \"11\", \"12\"]\n","      # Collect years of interest and store as a list\n","      years_of_interest = [str(year) for year in range(int(start_year), int(end_year) + 1)]\n","      print('List of years:', years_of_interest)\n","\n","      # Download climate data for each month and year\n","      for yr in years_of_interest:\n","          for m in month:\n","              base_url = \"http://climate.weather.gc.ca/climate_data/bulk_data_e.html?\"\n","              query_url = f\"format=csv&stationID={stationID}&Year={yr}&Month={m}&timeframe={time_step_val}\"\n","              api_endpoint = base_url + query_url\n","              wget.download(api_endpoint, out=station_dir)\n","\n","      # Remove duplicate files\n","      file_list = glob(os.path.join(station_dir, \"*(1).csv\"))\n","      for f in file_list:\n","          os.remove(f)\n","\n","      # Merge files\n","      file_pattern = f'en_climate_{time_step}_*.csv'\n","      file_daily = glob(os.path.join(station_dir, file_pattern), recursive=True)\n","\n","      # Define merged folder\n","      merge_dir = os.path.join(temporary_dir, 'weather_data', 'merge')\n","      os.makedirs(merge_dir, exist_ok=True)\n","\n","      # Merge files\n","      climateIDs = pd.DataFrame()\n","      files_merged = pd.concat([pd.read_csv(f) for f in file_daily]).reset_index()\n","      files_merged.to_csv(os.path.join(merge_dir, f'stationID_{stationID}_merged.csv'))\n","\n","      # Convert the 'Date/Time' column to datetime format\n","      files_merged['Date/Time'] = pd.to_datetime(files_merged['Date/Time'], errors='coerce')\n","\n","      # Filter the DataFrame for the specified date range\n","      files_merged = files_merged[\n","          (files_merged['Date/Time'] >= start_date) &\n","          (files_merged['Date/Time'] <= end_date)\n","      ]\n","\n","      files_merged.to_csv(os.path.join(merge_dir, f'stationID_{stationID}_merged.csv'))\n","\n","      # Display dataset\n","      display(files_merged)\n","      all_files_merged.append(files_merged)\n","\n","def generate_rvt_files(station_ID_lst, col_name_lst, param_lst, unit_lst, start_date, main_dir, temporary_dir):\n","    # Create directory if it doesn't exist\n","    obs_dir = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'climate_obs')\n","    os.makedirs(obs_dir, exist_ok=True)\n","\n","    for stationID in station_ID_lst:\n","        print('\\n-----------------------------------------------------------------------------------------------')\n","        print(f'( ) Generate Station {stationID} RVT File')\n","        print('-----------------------------------------------------------------------------------------------')\n","        start_year, start_month, start_day = start_date.split('-')\n","        # Define Minimum Year, Month, and Day\n","        min_year = str(start_year)\n","        month = str(start_month)\n","        day = str(start_day)\n","\n","        # Glob pattern for finding merged files in the folder\n","        file_pattern = f'stationID_{stationID}_merged.csv'\n","        file_path = os.path.join(temporary_dir, 'weather_data', 'merge', file_pattern)\n","        print(file_path)\n","\n","        # Check if the file exists\n","        if os.path.exists(file_path):\n","            # Read the merged file\n","            files_merged_station = pd.read_csv(file_path)\n","\n","            # Check for NaN values in the DataFrame and replace them with -1.2345\n","            #files_merged_station.fillna(-1.2345, inplace=True)\n","\n","            # Define variables to incorporate into the Raven RVT file\n","            var_df = files_merged_station[col_name_lst]\n","            print(var_df)\n","\n","            for vars in col_name_lst:\n","                var_df[vars] = (var_df[vars].interpolate(method='linear',order=3, limit=None,limit_direction='both').ffill().bfill())\n","                plt.figure()  # Create a new figure for each column\n","                plt.plot(var_df[vars])\n","                plt.title(f'Line Plot for {vars}')\n","\n","            # Show all the plots\n","            plt.show()\n","\n","            # Generate RVT file for each station\n","            file_name = f'station_{stationID}'\n","\n","            # Write RVT file\n","            var_vals = '\\n'.join(var_df.astype(str).apply(lambda x: ', '.join(x), axis=1))\n","\n","            with open(os.path.join(obs_dir, f\"{file_name}.rvt\"), \"a\") as f:\n","                print(\":MultiData\", file=f)\n","                print(f\"{min_year}-{month}-{day} 00:00:00 1.0 {len(files_merged_station)}\", file=f)\n","                print(f\":Parameters,{param_lst}\", file=f)\n","                print(f\":Units,{unit_lst}\", file=f)\n","                print(f\"{var_vals}\", file=f)\n","                print(\":EndMultiData\", file=f)\n","        else:\n","            print(f\"File not found for station {stationID}: {file_pattern}\")\n","\n","def rvt_station_generator(station_ID_lst, model_name, main_dir):\n","  print('\\n-----------------------------------------------------------------------------------------------')\n","  print('( ) Generate station RVT files')\n","  print('-----------------------------------------------------------------------------------------------')\n","  # Create directory if it doesn't exist\n","  obs_dir = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'climate_obs')\n","  # generate list of station files in climate_obs\n","  file_lst = glob(os.path.join(obs_dir, \"*.rvt\"))\n","\n","  lst_gauge_data = []\n","  for f in file_lst:\n","    f_name = os.path.basename(f)\n","    lst_gauge_data.append(f_name)\n","\n","  print(\"Station files: \",lst_gauge_data)\n","\n","  # empty list to collect climate IDs\n","  climateIDs = pd.DataFrame()\n","  # read in csv with climate station information\n","  climate_stations = pd.read_csv(os.path.join(main_dir, 'extras','subbasin_plots','climate_station_inventory.csv'))\n","  climate_stations_updated = climate_stations.dropna(subset=['Latitude'])\n","\n","  with open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\") as f:\n","    print(f\"#########################################################################\", file=f)\n","    print(f\"# Climate Stations List\", file=f)\n","    print(f\"#------------------------------------------------------------------------\\n#\", file=f)\n","\n","  for n in range(0, (len(lst_gauge_data))):\n","    station_val = station_ID_lst[n]\n","    df_search = climate_stations_updated[climate_stations_updated['Station_ID'].astype(str).str.contains(str(station_val))]\n","    if df_search.empty:\n","      f = open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\")\n","      print(f\":Gauge {station_val}\", file=f)\n","      print(f\"\\t:Latitude \\t\\t\\t\\t[]\", file=f)\n","      print(f\"\\t:Longitude \\t\\t\\t\\t[]\", file=f)\n","      print(f\"\\t:Elevation \\t\\t\\t\\t[]\", file=f)\n","      print(f\"\\t:RedirectToFile \\tclimate_obs/{lst_gauge_data[n]}\", file=f)\n","      print(f\":EndGauge\", file=f)\n","      print(f\"#\", file=f)\n","      f.close()\n","      print(f'WARNING: The climate station inventory CSV file available in Magpie unfortunately does not have the latitude, longitude, and elevation for {lst_gauge_data[n]}. Please follow the directions below to fill in the missing information.')\n","    else:\n","      f = open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\")\n","      print(f\":Gauge {station_val}\", file=f)\n","      print(f\"\\t:Latitude \\t\\t\\t\\t{df_search['Latitude'].iloc[0]}\", file=f)\n","      print(f\"\\t:Longitude \\t\\t\\t\\t{df_search['Longitude'].iloc[0]}\", file=f)\n","      print(f\"\\t:Elevation \\t\\t\\t\\t{df_search['Elevation'].iloc[0]}\", file=f)\n","      print(f\"\\t:RedirectToFile \\tclimate_obs/{lst_gauge_data[n]}\", file=f)\n","      print(f\":EndGauge\", file=f)\n","      print(f\"#\", file=f)\n","      f.close()\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eS7u_0iwCViO"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Visualization of Potential Climate Stations in Study Area** </font>\n","\n","#@markdown This cell produces an interactive plot with the study area shapefile, where users can click on the points to identify the stations names, latitude, and longitude\n","\n","map_visualize = True #@param {type:\"boolean\"}\n","\n","# Read in CSV with climate station information\n","climate_stations = pd.read_csv(os.path.join(main_dir, 'extras', 'subbasin_plots', 'climate_station_inventory.csv'))\n","climate_stations_updated = climate_stations.dropna(subset=['Latitude'])\n","\n","# generate map with rectangular guide to assist in identifying the ideal subbasin/gauges to use\n","if map_visualize == True:\n","  interactive_map_response = 'yes'\n","  interactive_map_climate_stations(main_dir,climate_stations_updated)\n","else:\n","  interactive_map_response = 'no'\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"ExQvUrQJD9M-"},"outputs":[],"source":["#@markdown <font color=grey> **Station Look-up by Provice** </font>\n","\n","#@markdown <font color=#5559AB> **Define Provice** </font><br>\n","#@markdown <font color=grey> use province abbreviation, for example:  <br>\n","#@markdown <font color=grey> ON, for Ontario </font>\n","province = \"ON\"   #@param [\"BC\", \"AB\", \"SK\", \"MB\", \"ON\", \"QC\", \"NB\", \"NS\", \"PEI\", \"NL\", \"YT\", \"NT\", \"NU\"]\n","\n","#@markdown <font color=#5559AB> **Define Start Year** </font>\n","start_year = \"2015\" #@param {type:\"string\"}\n","\n","max_pages = 10        # Number of maximum pages to parse, EC's limit is 100 rows per page, there are about 500 stations in BC with data going back to 2006\n","\n","# run function\n","station_look_up(province,start_year,temporary_dir)"]},{"cell_type":"markdown","metadata":{"id":"i9SO81N124Pe"},"source":["\n","\n","This subsection provides hourly, daily, and/or monthly downloads for gauge stations of interest. The downloaded data is the visualized and formatted into a Raven RVT file that are stored in the \"RavenInput\" --> \"obs\" folder.\n","\n","If the user would like to download and incorporate multiple gauge stations into their Raven model, run this subsection for each station of interest then move onto subsection 4.5.2\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"gUXd32Hd3XPS"},"outputs":[],"source":["#@markdown <font color=grey> **Generate Daily Climatic Variables for Selected Station** </font>\n","\n","#@markdown <font color=#5559AB> **Define Station(s) of Interest** </font>\n","stations_of_interest = \"32232,47748\" #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Define Start Year** </font>\n","start_date = \"2016-10-01\" #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Define End Year** </font>\n","end_date = \"2019-09-30\" #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Define Station of Interest** </font><br>\n","#@markdown <font color=grey> **for hourly data** </font>\n","time_step = \"daily\" #@param [\"hourly\", \"daily\", \"monthly\"]\n","\n","# Split the string into a list of substrings using the commas\n","str_values = stations_of_interest.split(',')\n","# Convert each substring to an integer and create a list of integers\n","station_ID_lst = [int(value.strip()) for value in str_values]\n","\n","# download station data\n","files_merged = stationID_download(station_ID_lst, start_date, end_date, temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yBmcYFOEJjX6"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define variables to incorporate into the Raven RVT file** </font><br>\n","#@markdown ensure each variable is within quatotations, a comma seperates each variable, and that it is surrounded by square brackets <br>\n","#@markdown <font color=grey> for example,<br> ['Total Precip (mm)', 'Mean Temp (°C)']\n","\n","col_name_lst = ['Total Precip (mm)', 'Max Temp (°C)', 'Min Temp (°C)']   #@param\n","#@markdown <font color=grey> recommended that users copy and paste column names from the Preview Column Names cell\n","\n","#@markdown <font color=#5559AB> **Adjust Variable names for RVT file and define units** </font><br>\n","#@markdown ensure each variable follows the same order as presented above and a comma seperates each variable <br>\n","#@markdown <font color=grey> for example,<br> 'PRECIP, TEMP_AVE'\n","param_lst = 'PRECIP, TEMP_MAX, TEMP_MIN' #@param {type:\"string\"}\n","unit_lst = 'mm/d, C, C' #@param {type:\"string\"}\n","#@markdown <font color=grey> additional naming suggestions, TEMP_MIN, TEMP_MAX, SNOWFALL, RAINFALL, PET<br></font>\n","\n","#@markdown be sure to check the [Raven Manual](http://raven.uwaterloo.ca/files/v3.6/RavenManual_v3.6.pdf) for additional information on which variables can be included in RVT files, naming suggestions, and appropriate units\n","\n","generate_rvt_files(station_ID_lst, col_name_lst, param_lst, unit_lst, start_date, main_dir,temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"QdtHbFY6cs70"},"outputs":[],"source":["#@markdown <font color=grey> **Generate Raven RVT Input containing Station Information** </font></br>\n","\n","rvt_station_generator(station_ID_lst, model_name, main_dir)\n"]},{"cell_type":"markdown","metadata":{"id":"V6ZIYXZvkpiF"},"source":["<font color=grey>**If warning is present:**\n","\n","The user needs to add latitude, longitude, and elevation data to RVT file once the cell is run. Please utilize [Search by Station Name](https://climate.weather.gc.ca/historical_data/search_historic_data_e.html) to gather latitude, longitude, and elevation information. The RVT file can the be opened directly in the Magpie Workflow, remove the square brackets ('[  ]') and replace them with the missing information. Press Ctrl+S to save."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"pAM7cbW_sNSY"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"aJ9rUnwQdV2q"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","col_names = ', '.join(col_name_lst)\n","params = ', '.join(param_lst)\n","units = ', '.join(unit_lst)\n","\n","config_path = os.path.join(main_dir, \"configuration_files\")\n","os.makedirs(config_path, exist_ok=True)\n","\n","write_to_configuration_file = False  # @param {type:\"boolean\"}\n","\n","# Create a dictionary to store configuration data\n","data = {\n","    \"_comment\": \"------------ 3d) ENVIRONMENT CANADA CLIMATE DATA ---------------\",\n","\n","    \"interactive_map_climate_stations\": f\"{interactive_map_response}\",\n","    \"station_look_up\": \"yes\",\n","    \"province\": f\"{province}\",\n","    \"climate_start_yr\": f\"{start_date}\",\n","    \"climate_end_yr\": f\"{end_date}\",\n","\n","    \"stationID\": f\"{stations_of_interest}\",\n","    \"time_step\": f\"{time_step}\",\n","\n","    \"col_name_lst\": f\"{col_names}\",\n","    \"param_lst\": f\"{params}\",\n","    \"unit_lst\": f\"{units}\",\n","\n","}\n","\n","# Specify the file path where you want to save the JSON file\n","file_path = os.path.join(config_path, \"3d_environment_canada_climate_data.json\")\n","\n","if write_to_configuration_file:\n","  # Writing data to the JSON file\n","  with open(file_path, 'w') as json_file:\n","      json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)"]},{"cell_type":"markdown","metadata":{"id":"iaBnoA9cJSjn"},"source":["**References**\n","\n","Lim, S. (2017). Exploring Environment Canada Weather Data with Python and Jupyter Notebooks, https://github.com/csianglim/weather-gc-ca-python.git."]},{"cell_type":"markdown","metadata":{"id":"i_qewtTdzcRq"},"source":["### <font color=#5559AB> 3e) Format Uploaded Observational Data </font>\n","\n","This  section formats meteorological and flow observational data saved as a CSV into an RVT file applicable in Raven. Be sure to follow the formatting requirements discussed below.\n","\n","Check out the short video [4.5 Format Uploaded Observational Data - Magpie Workflow](https://youtu.be/m4n7MMvMFNs) for more information.\n"]},{"cell_type":"markdown","metadata":{"id":"BAaBzcorU9kx"},"source":["#### <font color=grey> **4.5.1 Format Uploaded Meteorological Data** </font>\n","\n","This section requires a <font color=red>CSV file</font> containing <font color=red> meteorological data, latitude, longitude, and elevation </font>formatted as follows:\n","\n","lat | lon | elev | precip | ... | temp |\n","--- | --- | --- | --- | --- | --- |\n","50.1 | -115.1 | 360.5 | 4.1 | ... | 5.4 |\n","50.1 | -115.1 | 360.5 | 6.0 | ... | 6.3 |\n","... | ... | ... | ... | ... | ... |\n","50.1 | -115.1 | 360.5 | 6.6 | ... | 5.2 |"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"USr3Fe6Avmtx"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"ipython\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","from IPython.display import display\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","def uploaded_climate_data_rvt_file(forc_dir,obs_dir,latitude_name,longitude_name,elevation_name,\n","                                   var_lst,params_list,units_list,min_year,month,day,model_name,main_dir):\n","  # overview of the first uploaded forcing file\n","  file_name = os.listdir(forc_dir)[0]\n","  view_file = pd.read_csv(os.path.join(forc_dir, file_name))\n","\n","  # define column names of latitude, longitude, and elevations variables from CSV\n","  # longitude input\n","  if longitude_name == \"NA\":\n","    longitude_name = input('Enter longitude column name: ')\n","  # latitude input\n","  if latitude_name == \"NA\":\n","    latitude_name = input('Enter latitude column name: ')\n","  # elevation column input\n","  if elevation_name == \"NA\":\n","    elevation_name = input('Enter elevation column name: ')\n","\n","  lat_lst = []\n","  lon_lst = []\n","  elev_lst = []\n","\n","  # define column names of forcing variables from CSV to incorporate into the Raven RVT file\n","  # variable list input\n","  if var_lst == \"NA\":\n","      var_lst_input = input('Enter variable list: ')\n","      var_lst_temp = [x for x in var_lst_input.split(',')]\n","      var_lst = [item.replace(\" \", \"\") for item in var_lst_temp]\n","      print('CSV Variable list: ', var_lst)\n","  else:\n","      var_lst_input = var_lst\n","      var_lst_temp = [x for x in var_lst_input.split(',')]\n","      var_lst = [item.replace(\" \", \"\") for item in var_lst_temp]\n","      print('CSV Variable list: ', var_lst)\n","\n","  # find files\n","  file_path = glob(os.path.join(forc_dir, \"*.csv\"))\n","\n","  for f in file_path:\n","    # open csv\n","    forc_df = pd.read_csv(f)\n","    # get list of variables from column names\n","    forc_vars = forc_df.columns.values\n","\n","    lat_lst.append(forc_df[latitude_name].unique())\n","    lon_lst.append(forc_df[longitude_name].unique())\n","    elev_lst.append(forc_df[elevation_name].unique())\n","\n","    # adjust Variable names for RVT file and define units\n","    # ensure the list of variables follows the same order as the columns, the variables are within quatotations, a comma seperates each variable, and square brackets are present on either end\n","\n","    # param list input\n","    if params_list == \"NA\":\n","      params_list_input = input('Enter variable list: ')\n","      params_list = [x for x in params_list_input.split(',')]\n","      print('Parameter list: ',params_list)\n","    else:\n","      params_list_input = params_list\n","      params_list = [x for x in params_list_input.split(',')]\n","      print('Parameter list: ',params_list)\n","\n","    # unit list input\n","    if units_list == \"NA\":\n","      units_list_input = input('Enter variable list: ')\n","      units_list = [x for x in units_list_input.split(',')]\n","      print('Unit list: ',units_list)\n","    else:\n","      units_list_input = units_list\n","      units_list = [x for x in units_list_input.split(',')]\n","      print('Unit list: ',units_list)\n","\n","      # additional naming suggestions: <br>\n","      # TEMP_MIN, TEMP_MAX, SNOWFALL, RAINFALL, RELHUM, WINDVEL, PET<br></font>\n","      # be sure to check the [Raven Manual](http://raven.uwaterloo.ca/files/v3.6/RavenManual_v3.6.pdf) for additional information on which variables can be included in RVT files and check that variables have the appropriate units\n","\n","    forc_variables_selected = forc_df[var_lst]\n","\n","    # get filename\n","    base = os.path.basename(f)\n","    split_base = os.path.splitext(base)\n","    file_name = os.path.splitext(base)[0]\n","\n","    # write rvt file\n","    var_vals ='\\n'.join(forc_variables_selected.astype(str).apply(lambda x: ', '.join(x), axis=1))\n","    params = ','.join(params_list)\n","    units = ','.join(units_list)\n","\n","    f = open(os.path.join(obs_dir, f\"{file_name}.rvt\"), \"a\")\n","    print(f\":MultiData\", file=f)\n","    print(f\"{min_year}-{month}-{day} 00:00:00 1.0 {len(forc_variables_selected.iloc[:, 0])}\",file=f)\n","    print(f\":Parameters,{params}\", file=f)\n","    print(f\":Units,{units}\", file=f)\n","    print(f\"{var_vals}\", file=f)\n","    print(f\":EndMultiData\", file=f)\n","    f.close()\n","\n","  file_lst = glob(os.path.join(obs_dir, \"*.rvt\"))\n","  print(file_lst)\n","\n","  lst_gauge_data = []\n","  for f in file_lst:\n","    f_name = os.path.basename(f)\n","    lst_gauge_data.append(f_name)\n","\n","  lat_lst_val = np.concatenate(lat_lst, axis=0)\n","  lon_lst_val = np.concatenate(lon_lst, axis=0)\n","  elev_lst_val = np.concatenate(elev_lst, axis=0)\n","\n","  # generate Raven RVT Input containting Station Information\n","  # define model name (for RVT file)\n","  with open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\") as f:\n","    print(f\"#########################################################################\", file=f)\n","    print(f\"# Climate Stations List\", file=f)\n","    print(f\"#------------------------------------------------------------------------\\n#\", file=f)\n","\n","  for n in range(len(file_lst)):\n","    base = os.path.basename(lst_gauge_data[n])\n","    split_base = os.path.splitext(base)\n","    station_val = os.path.splitext(base)[0]\n","    # define station vals\n","    lat_val = lat_lst_val[n]\n","    lon_val = lon_lst_val[n]\n","    elev_val = elev_lst_val[n]\n","    f = open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\")\n","    print(f\":Gauge \\t\\t\\t\\t\\t\\t{station_val}\", file=f)\n","    print(f\"\\t:Latitude \\t\\t\\t{lat_val}\", file=f)\n","    print(f\"\\t:Longitude \\t\\t\\t{lon_val}\", file=f)\n","    print(f\"\\t:Elevation \\t\\t\\t{elev_val}\", file=f)\n","    print(f\"\\t:RedirectToFile climate_obs/{lst_gauge_data[n]}\", file=f)\n","    print(f\":EndGauge\", file=f)\n","    print(f\"#\", file=f)\n","    f.close()\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"rZ0rLYN2sLUI"},"outputs":[],"source":["#@markdown <font color=grey> **Generate temporary \"climate_obs\" to upload data** </font>\n","\n","#@markdown Step 1) once the cell is run, go to \"data_temporary\" -> \"forcing_data\" -> \"climate_obs\"\n","\n","#@markdown Step 2) drag and drop climate data into the \"climate_obs\" folder\n","\n","# Define and create temporary directory for RavenInput\n","obs_dir = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'climate_obs')\n","os.makedirs(obs_dir, exist_ok=True)\n","print(\"created folder:\", obs_dir)\n","\n","# Define and create temporary directory for forcing data\n","forc_dir = os.path.join(temporary_dir, 'forcing_data', 'climate_obs')\n","os.makedirs(forc_dir, exist_ok=True)\n","print(\"created folder:\", forc_dir)\n","\n","print(f\"Please drag-and-drop meteorological file(s) into the following folder: {forc_dir}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"U4KymBbVrdgq"},"outputs":[],"source":["#@markdown <font color=grey> **Overview of the first uploaded forcing file** </font>\n","\n","# Select the first file in the directory\n","file_name = os.listdir(forc_dir)[0]\n","\n","# Preview the content of the selected file\n","view_file = pd.read_csv(os.path.join(forc_dir, file_name))\n","display(view_file.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"EsLY1tO8z-Ci"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define column names of latitude, longitude, and elevations variables from CSV** </font><br>\n","#@markdown ensure each variable is within quatotations, a comma seperates each variable, and square brackets are present on either end <br>\n","#@markdown <font color=grey> for example,<br> ['lon', 'lat', 'elev]\n","\n","latitude_name = 'lat' #@param {type:\"string\"}\n","longitude_name = 'lon' #@param {type:\"string\"}\n","elevation_name = 'elev' #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Define column names of forcing variables from CSV to incorporate into the Raven RVT file** </font><br>\n","#@markdown ensure each variable is within quatotations, a comma seperates each variable, and square brackets are present on either end <br>\n","#@markdown <font color=grey> for example,<br> 'Precip . Amount (mm)', 'Mean Temp (°C)']\n","\n","var_lst = 'precip, temp' #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Adjust Variable names for RVT file and define units** </font><br>\n","#@markdown ensure the list of variables follows the same order as the columns, the variables are within quatotations, a comma seperates each variable, and square brackets are present on either end <br>\n","#@markdown <font color=grey> for example,<br> ['PRECIP', 'TEMP']\n","params_list = 'PRECP, TEMP' #@param {type:\"string\"}\n","units_list = 'mm/d, C' #@param {type:\"string\"}\n","#@markdown <font color=grey> additional naming suggestions: <br>\n","#@markdown TEMP_MIN, TEMP_MAX, SNOWFALL, RAINFALL, RELHUM, WINDVEL, PET<br></font>\n","#@markdown be sure to check the [Raven Manual](http://raven.uwaterloo.ca/files/v3.6/RavenManual_v3.6.pdf) for additional information on which variables can be included in RVT files and check that variables have the appropriate units\n","\n","#@markdown <font color=#5559AB> **Define Minimum Year, Month, and Day** </font><br>\n","min_year = '2000' #@param {type:\"string\"}\n","month = '01' #@param {type:\"string\"}\n","day = '01' #@param {type:\"string\"}\n","\n","uploaded_climate_data_rvt_file(forc_dir,obs_dir,latitude_name,longitude_name,elevation_name,\n","                                   var_lst,params_list,units_list,min_year,month,day,model_name,main_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fTQSCONjz-G6"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UHZM3uZtwKs1"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","write_to_configuration_file = False  # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file:\n","\n","    # Create a dictionary to store configuration data\n","    data = {\n","        \"_comment\": \"------------ 3e) FORMAT UPLOADED OBSERVATIONAL DATA ---------------\",\n","\n","        \"_comment\": \"-- UPLOADED METEOROLOGICAL DATA\",\n","\n","        \"uploaded_climate_latitude\": f\"{latitude_name}\",\n","        \"uploaded_climate_longitude\": f\"{longitude_name}\",\n","        \"uploaded_climate_elevation\": f\"{elevation_name}\",\n","\n","        \"uploaded_climate_csv_var_list\": f\"{var_lst}\",\n","        \"uploaded_climate_parameter_list\": f\"{params_list}\",\n","        \"uploaded_climate_unit_list\": f\"{units_list}\",\n","\n","        \"uploaded_climate_min_year\": f\"{min_year}\",\n","        \"uploaded_climate_month\": f\"{month}\",\n","        \"uploaded_climate_day\": f\"{day}\",\n","\n","\n","    }\n","\n","    # Specify the file path where you want to save the JSON file\n","    file_path = os.path.join(config_path, \"3e_upload_meteorological_data.json\")\n","\n","    if write_to_configuration_file:\n","      # Writing data to the JSON file\n","      with open(file_path, 'w') as json_file:\n","          json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)"]},{"cell_type":"markdown","metadata":{"id":"MJvAf5pkWUQU"},"source":["#### <font color=grey> **4.5.2 Format Uploaded Flow Data** </font>\n","\n","This section requires a <font color=red>CSV file</font> containing <font color=red> the flow data of only one station </font>formatted as follows:\n","\n","CSV File 1\n","\n","station1 |\n","--- |\n","2.1 |\n","... |\n","3.2 |\n","\n","\n","<br>In the case multiple flow gauges need to be formatted, upload them all in seperate CSV files. Note, that the column name is used to name the file. Additionally, ensure the flow data is in m^3/s"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"mIOolwdMyFbu"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"ipython\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","from IPython.display import display\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","\n","def uploaded_flow_data_rvt_file(flow_forc_temp,forc_dir,subbasin_ID,min_year,month,day):\n","  # overview of the first uploaded forcing file\n","  flow_file_name = os.listdir(flow_forc_temp)[0]\n","  view_file = pd.read_csv(os.path.join(flow_forc_temp, flow_file_name))\n","\n","  if subbasin_ID == \"NA\":\n","    subbasin_ID_input = input('Enter subbasin ID for each station (ensure they are in the same order as the station files): ')\n","    subbasin_ID = [x for x in subbasin_ID_input.split(',')]\n","    print('Subbasin ID(s): ',subbasin_ID)\n","  else:\n","    subbasin_ID_input = subbasin_ID\n","    subbasin_ID = [x for x in subbasin_ID_input.split(',')]\n","    print('Subbasin ID(s): ',subbasin_ID)\n","\n","  # find files\n","  flow_file_path = glob(os.path.join(flow_forc_temp, \"*.csv\"))\n","\n","  for n in range(len(flow_file_path)):\n","      # define temporary directory\n","      flow_obs_dir = os.path.join(main_dir,'workflow_outputs','RavenInput','obs')\n","      flow_obs_path = os.path.isdir(flow_obs_dir)\n","      if not flow_obs_path:\n","        os.makedirs(flow_obs_dir)\n","\n","      # open csv\n","      forc_df = pd.read_csv(flow_file_path[n])\n","      forc_col_names = forc_df.columns.values\n","      # get list of variables from column names\n","      forc_vars = forc_df.to_string(index=False, header=False)\n","      indent = '  '\n","      indented_vars = indent + forc_vars.replace('\\n', '\\n' + indent)\n","      # determine the number of observations\n","      num_obs = len(forc_df.index)\n","\n","      f = open(os.path.join(flow_obs_dir, f\"{forc_col_names[0]}.rvt\"), \"a\")\n","      print(f\":ObservationData HYDROGRAPH {subbasin_ID[n]} m3/s\", file=f)\n","      print(f\"\\t{min_year}-{month}-{day} 00:00:00 1 {num_obs}\",file=f)\n","      print(f\"{indented_vars}\", file=f)\n","      print(f\":EndObservationData\", file=f)\n","      f.close()\n","\n","  # define model name\n","  file_names_lst = []\n","  files_input = glob(os.path.join(main_dir, 'workflow_outputs', 'RavenInput','obs',\"*\"))\n","\n","  for f_name in list(range(0, len(files_input))):\n","    base = os.path.basename(files_input[f_name])\n","    file_names_lst.append(base)\n","\n","  if not glob(os.path.join(main_dir, 'workflow_outputs', 'RavenInput',model_name+'.rvt')):\n","    with open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\") as f:\n","      print(f\"#------------------------------------------------------------------------\", file=f)\n","      print(f\"# Observed Discharge Data\", file=f)\n","      print(f\"#------------------------------------------------------------------------\\n#\", file=f)\n","\n","    lst_vals = list(range(0, len(file_names_lst)))\n","\n","    for n in lst_vals:\n","      f = open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\")\n","      print(f\":RedirectToFile \\t\\t obs/{file_names_lst[n]}\", file=f)\n","      f.close()\n","  else:\n","    with open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\") as f:\n","      print(f\"#------------------------------------------------------------------------\", file=f)\n","      print(f\"# Observed Discharge Data\", file=f)\n","      print(f\"#------------------------------------------------------------------------\\n#\", file=f)\n","\n","    lst_vals = list(range(0, len(file_names_lst)))\n","\n","    for n in lst_vals:\n","      f = open(os.path.join(main_dir, 'workflow_outputs','RavenInput',model_name+'.rvt'), \"a\")\n","      print(f\":RedirectToFile \\t\\t obs/{file_names_lst[n]}\", file=f)\n","      f.close()\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zzyUVQCnW1Ay"},"outputs":[],"source":["#@markdown <font color=grey> **Generate temporary \"flow_obs\" to upload data** </font>\n","\n","#@markdown Step 1) once the cell is run, go to \"data_temporary\" -> \"forcing_data\" -> \"flow_obs\"\n","\n","#@markdown Step 2) drag and drop climate data into the \"flow_obs\" folder\n","\n","# Define and create temporary directory for RavenInput\n","forc_dir = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'obs')\n","os.makedirs(forc_dir, exist_ok=True)\n","print(\"created folder:\", forc_dir)\n","\n","# Define and create temporary directory for forcing data\n","flow_forc_temp = os.path.join(temporary_dir, 'forcing_data', 'flow_obs')\n","os.makedirs(flow_forc_temp, exist_ok=True)\n","print(\"created folder:\", flow_forc_temp)\n","\n","print(f\"Please drag-and-drop flow station file(s) into the following folder: {flow_forc_temp}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"57-LIYEEW1LH"},"outputs":[],"source":["#@markdown <font color=grey> **Overview of the first uploaded forcing file** </font>\n","\n","flow_file_name = os.listdir(flow_forc_temp)[0]\n","view_file = pd.read_csv(os.path.join(flow_forc_temp, flow_file_name))\n","display(view_file.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"6bIG6SbAiJc8"},"outputs":[],"source":["#@markdown <font color=#5559AB>**Determine the Subbasin ID that the Flow Gauge is Located In**</font>\n","\n","subbasin_ID = '3048815' #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Define Minimum Year, Month, and Day** </font><br>\n","min_year = '2000' #@param {type:\"string\"}\n","month = '01' #@param {type:\"string\"}\n","day = '01' #@param {type:\"string\"}\n","\n","uploaded_flow_data_rvt_file(flow_forc_temp,forc_dir,subbasin_ID,min_year,month,day)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"2JL5r26iZTVT"},"outputs":[],"source":["#@markdown <font color=grey> **Remove temporary data** </font><br>\n","#@markdown remove temporary data to assist in saving space on drive\n","\n","#@markdown if users want to save any of the temporary data, they can right-click on the layer in the folder directory and select download\n","\n","\n","# delete temorary folder\n","remove_temp_data(temporary_dir)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cMcEzMKAxcIU"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","write_to_configuration_file = False  # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file:\n","\n","    # Create a dictionary to store configuration data\n","    data = {\n","        \"_comment\": \"------------ 3e) FORMAT UPLOADED OBSERVATIONAL DATA ---------------\",\n","\n","        \"_comment\": \"-- UPLOADED FLOW DATA\",\n","\n","        \"uploaded_flow_subbasin_ID\": f\"{flow_gauge_ID}\",\n","        \"uploaded_flow_min_yr\": f\"{min_year}\",\n","        \"uploaded_flow_month\": f\"{month}\",\n","        \"uploaded_flow_day\": f\"{day}\",\n","\n","    }\n","\n","    # Specify the file path where you want to save the JSON file\n","    file_path = os.path.join(config_path, \"3e_upload_flow_data.json\")\n","\n","    if write_to_configuration_file:\n","      # Writing data to the JSON file\n","      with open(file_path, 'w') as json_file:\n","          json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)"]},{"cell_type":"markdown","metadata":{"id":"EHHk1xjChbrz"},"source":["\n","### <font color=#5559AB> 3f) Hydrometric Data (HYDAT) </font>\n","\n","This section is run in R and utilizes the RavenR, developed by Chlumsky et al. (2022), command `rvn_rvt_tidyhydat` to convert Environment Canada historical streamgauge data, accessed via the tidyhydat package, into .rvt format files usable in Raven.\n","\n","Check out the short video [Downloading and Processing Hydrometric Data (HYDAT) for Raven](https://youtu.be/ySqDOxj-EYA) for more information.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"Ev3tYaHm4zu3"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"rpy2==3.5.1\",\"folium\",\"branca\", \"ipython\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","%load_ext rpy2.ipython\n","\n","from IPython.display import display\n","# # interactive map\n","import folium\n","import branca\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","# if \"interactive_map_response\": \"yes\"\n","# an interative map is generated to visualize which gauges are near the previously defined coords\n","def interactive_gauge_map(main_dir):\n","    print('\\n-----------------------------------------------------------------------------------------------')\n","    print('( ) Interactive gauge plot')\n","    print('-----------------------------------------------------------------------------------------------')\n","    print('Response: ', interactive_map_response)\n","    # read in csv with gauge information\n","    flow_stations = pd.read_csv(os.path.join(main_dir, 'extras','subbasin_plots','obs_gauges_NA_v2-1.csv'))\n","    flow_stations_updated = flow_stations.dropna(subset=['POINT_Y'])\n","\n","    # format fancy folium pop-up\n","    def fancy_html(row):\n","        i = row\n","        flowName = flow_stations_updated['Obs_NM'].iloc[i]\n","        pointX_info = flow_stations_updated['POINT_Y'].iloc[i]\n","        pointY_info = flow_stations_updated['POINT_X'].iloc[i]\n","        html = \"\"\"<!DOCTYPE html>\n","    <html>\n","    <p>Obs_NM: {}</td>\"\"\".format(flowName) + \"\"\"</p>\n","    <p>POINT_X: {}</td>\"\"\".format(pointX_info) + \"\"\"</p>\n","    <p>POINT_Y: {}</td>\"\"\".format(pointY_info) + \"\"\"</p>\n","    </html>\n","    \"\"\"\n","        return html\n","\n","    # generate map with rectangular guide to assist in identifying the ideal subbasin/gauges to use\n","    shp_boundary = gpd.read_file(os.path.join(main_dir, 'shapefile','studyArea_outline.shp'))\n","    shp_boundary = shp_boundary.to_crs(epsg=4326)\n","\n","    # determine the boundary of the provided shapefile\n","    bounds = shp_boundary.bounds\n","    west, south, east, north = bounds = bounds.loc[0]\n","    shp_bounds = [south,west]\n","\n","    map = folium.Map(location=shp_bounds, zoom_start=10)\n","    folium.GeoJson(data=shp_boundary[\"geometry\"]).add_to(map)\n","    #folium.LatLngPopup().add_to(map)\n","\n","    for i in range(0,len(flow_stations_updated)):\n","        html = fancy_html(i)\n","        iframe = branca.element.IFrame(html=html,width=200,height=200)\n","        popup = folium.Popup(iframe,parse_html=True)\n","        folium.Marker([flow_stations_updated['POINT_Y'].iloc[i],flow_stations_updated['POINT_X'].iloc[i]],popup=popup).add_to(map)\n","    # displays interactive map\n","    display(map)\n","\n","def generate_rvt_file(main_dir, model_name):\n","    file_names_lst = []\n","    files_input = glob(os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'obs', \"*\"))\n","\n","    for f_name in files_input:\n","        base = os.path.basename(f_name)\n","        file_names_lst.append(base)\n","\n","    rvt_file_path = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', f\"{model_name}.rvt\")\n","\n","    if not os.path.exists(rvt_file_path):\n","        with open(rvt_file_path, \"a\") as f:\n","            f.write(\"#------------------------------------------------------------------------\\n\")\n","            f.write(\"# Observed Discharge Data\\n\")\n","            f.write(\"#------------------------------------------------------------------------\\n#\\n\")\n","\n","    for file_name in file_names_lst:\n","        with open(rvt_file_path, \"a\") as f:\n","            f.write(f\":RedirectToFile \\t\\t obs/{file_name}\\n\")\n","\n","def get_subbasin_id(main_dir, flow_gauge_ID):\n","    # Assuming main_dir is a parameter passed to the function\n","    gauge_csv_path = os.path.join(main_dir, 'extras', 'subbasin_plots', 'obs_gauges_NA_v2-1.csv')\n","\n","    # Read the CSV file into a DataFrame\n","    gauge_csv = pd.read_csv(gauge_csv_path)\n","\n","    # Select the row where 'Obs_NM' is equal to flow_gauge_ID\n","    select_row = gauge_csv.loc[gauge_csv['Obs_NM'] == flow_gauge_ID]\n","\n","    # Get the 'SubId' values from the selected row\n","    subbasin_ID = select_row['SubId'].values\n","\n","    if len(subbasin_ID) > 0:\n","        return subbasin_ID[0]\n","    else:\n","        return None  # Return None or any other value indicating that the subbasin ID is not found"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"DrPSJ4LsbQ3A"},"outputs":[],"source":["#@markdown <font color=grey> **Visualization of Potential Flow Gauges in Study Area** </font>\n","\n","#@markdown This cell produces an interactive plot with the study area shapefile, where users can click on the points to identify the flow gauges names, latitude, and longitude\n","\n","#@markdown Please note this will take a minute or two to load\n","\n","interactive_map_response = False # @param {type:\"boolean\"}\n","\n","if interactive_map_response:\n","  interactive_gauge_map(main_dir)\n","  interactive_map_response_flow = 'yes'\n","else:\n","  interactive_map_response_flow = 'no'"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"PnttadJBaoSb"},"outputs":[],"source":["#@markdown <font color=grey> **Install and Load Related R Packages** </font>\n","\n","#@markdown This will take aproximately 2.5 minutes to run, good time for a coffee break ☕\n","\n","%%R\n","# Start measuring time\n","start_time <- Sys.time()\n","\n","cat('-----------------------------------------------------------------------------------------------\\n')\n","cat('( ) Installing libraries\\n')\n","cat('-----------------------------------------------------------------------------------------------\\n')\n","\n","install.packages(\"tidyhydat\")\n","install.packages(\"rjson\")\n","\n","cat('-----------------------------------------------------------------------------------------------\\n')\n","cat('( ) Loading libraries\\n')\n","cat('-----------------------------------------------------------------------------------------------\\n')\n","\n","library(tidyhydat)\n","library(ggplot2)\n","library(rjson)\n","library(RavenR, lib=\"/content/google_drive/MyDrive/R_Packages\")\n","\n","# End measuring time\n","end_time <- Sys.time()\n","\n","# Calculate the elapsed time in minutes\n","elapsed_time_minutes <- as.numeric(difftime(end_time, start_time, units = \"mins\"))\n","\n","cat(paste(\"Script execution time:\", elapsed_time_minutes, \"minutes\\n\"))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cH__5bsAofL0"},"outputs":[],"source":["#@markdown <font color=grey> **Download HYDAT Data** </font>\n","\n","#@markdown please note this will take a couple of minutes to run\n","\n","%%R\n","hy_dir()\n","\n","download_hydat(dl_hydat_here=\"/root/.local/share/tidyhydat\", ask=FALSE)\n","print(\"Download complete\")"]},{"cell_type":"markdown","metadata":{"id":"-Jyw3gBxANTx"},"source":["<font color=#5559AB>**Define Input Variables**</font>\n","\n","To identify which historical hydrometic station corresponds with the study area, please click [here](https://wateroffice.ec.gc.ca/search/historical_e.html)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"a3hAJDXVsIfb"},"outputs":[],"source":["#@markdown <font color=#5559AB>**List stations of interest**</font>\n","\n","# Define the folder path\n","folder_path = os.path.join(main_dir, \"workflow_outputs\", \"RavenInput\", \"obs\")\n","\n","# Check if the folder exists\n","if not os.path.exists(folder_path):\n","    # Create the folder\n","    os.makedirs(folder_path)\n","\n","#@markdown use comma to seperate stations if there are multiple\n","\n","stations_lst = '02KB001' #@param {type:\"string\"}\n","\n","#@markdown <font color=#5559AB> **Define start and end date** </font><br>\n","\n","#@markdown format date as follows, 2000-01-01\n","\n","start_date = '2016-10-01' #@param {type:\"string\"}\n","end_date = '2019-09-30' #@param {type:\"string\"}\n","\n","# File path for output\n","output_file = \"/content/variable_info.txt\"\n","\n","# Writing to a file\n","with open(output_file, \"w\") as file:\n","    file.write(\"main_dir = '{}'\\n\".format(main_dir))\n","    file.write(\"stations_lst = '{}'\\n\".format(stations_lst))\n","    file.write(\"start_date = '{}'\\n\".format(start_date))\n","    file.write(\"end_date = '{}'\\n\".format(end_date))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cWZcpVxKtv71"},"outputs":[],"source":["#@markdown <font color=grey> **Formatting values** </font>\n","\n","%%R\n","\n","# Define the file path\n","file_path <- \"/content/variable_info.txt\"\n","\n","# Read the file\n","lines <- readLines(file_path)\n","\n","# Initialize variables\n","main_dir <- NULL\n","stations_lst <- NULL\n","start_date <- NULL\n","end_date <- NULL\n","\n","# Extract values from the lines\n","for (line in lines) {\n","  if (grepl(\"main_dir\", line)) {\n","    main_dir <- gsub(\"main_dir = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"stations_lst\", line)) {\n","    stations_lst <- gsub(\"stations_lst = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"start_date\", line)) {\n","    start_date <- gsub(\"start_date = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"end_date\", line)) {\n","    end_date <- gsub(\"end_date = '(.*)'\", \"\\\\1\", line)\n","  }\n","}\n","\n","# Split the string by commas\n","stations_list <- strsplit(stations_lst, \",\")[[1]]\n","\n","cat('Values have been successfully formatted')"]},{"cell_type":"markdown","metadata":{"id":"w7MWEuTDno2E"},"source":["<font color=grey> **Gather station data/info using tidyhydat function** </font>\n","\n","This cell downloads daily flow data, however, users have the option of adjusting the script to download monthly data by subbing `hy_daily_flows` with `hy_monthly_flows` to gather monthly flow levels.\n","\n","For more informationon available tidhydat functions, please click [here](https://cran.r-project.org/web/packages/tidyhydat/tidyhydat.pdf)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kUq-1XjDnkbH"},"outputs":[],"source":["#@markdown <font color=grey> **Download available HYDAT data** </font>\n","\n","%%R\n","\n","hd <- tidyhydat::hy_daily_flows(station_number = stations_list, start_date = start_date, end_date = end_date)\n","print(hd)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"eiYbdeGKDgT3"},"outputs":[],"source":["#@markdown <font color=grey> **Plot Station HYDAT Data** </font>\n","\n","%%R\n","\n","ggplot(hd) +\n","geom_line(aes(x = Date, y = Value, colour = STATION_NUMBER)) +\n","labs(y = \"Mean daily Flow\") +\n","scale_colour_viridis_d(option = \"C\") +\n","theme_minimal() +\n","theme(legend.position = \"bottom\")"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"iMrlQh-RAVr6"},"outputs":[],"source":["#@markdown <font color=grey> **Generate RVT files for each of the available stations** </font>\n","\n","%%R\n","\n","# Extract unique values from the 'Value' column and save to a list\n","unique_values_list <- unique(hd$STATION_NUMBER)\n","\n","for (stationID in unique_values_list){\n","hd <- tidyhydat::hy_daily_flows(station_number = stationID, start_date = start_date, end_date = end_date)\n","\n","print('-- station data saved at: ')\n","tf1 <- file.path(main_dir,'workflow_outputs','RavenInput','obs',paste0(stationID,\".rvt\", sep=\"\"))\n","print(tf1)\n","\n","print('-- create RVT file for station')\n","# Create RVT files\n","rvn_rvt_tidyhydat(hd,\n","                  subIDs=c(3),\n","                  write_redirect = FALSE,\n","                  flip_number = FALSE,\n","                  filename=c(tf1)\n","                  )\n","\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"6fyMaeRllcm_"},"outputs":[],"source":["#@markdown <font color=grey>**Add station RVT information to exisiting RVT file or generate a new RVT file**</font>\n","\n","\n","generate_rvt_file(main_dir, model_name)\n","\n","# Define the file path\n","file_path = \"/content/variable_info.txt\"\n","\n","# Check if the file exists\n","if os.path.exists(file_path):\n","    # Delete the file\n","    os.remove(file_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1gXM7NoYGnYV"},"outputs":[],"source":["#@markdown <font color=red>**EXTRA STEP**</font>\n","\n","#@markdown <font color=#5559AB>**Determine the Subbasin ID that the Flow Gauge is Located In**</font>\n","\n","#@markdown if using NLRP product define the flow gauge ID to determine the subbasin it is located in, copy and paste the ID into gauge RVT file\n","\n","flow_gauge_ID = '02KB001' #@param {type:\"string\"}\n","\n","subbasin_ID = get_subbasin_id(main_dir, flow_gauge_ID)\n","\n","if subbasin_ID is not None:\n","    print(f'Subbasin ID: {subbasin_ID}')\n","else:\n","    print('Subbasin ID not found for the given flow gauge ID.')"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"cpB55odHyjUZ"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","write_to_configuration_file = False  # @param {type:\"boolean\"}\n","\n","if write_to_configuration_file:\n","\n","  # Create a dictionary to store configuration data\n","  data = {\n","      \"_comment\": \"------------ 3f) HYDROMETRIC DATA ---------------\",\n","\n","      \"interactive_map_response_flow\": f\"{interactive_map_response_flow}\",\n","      \"flow_stations\": f\"{stations_lst}\",\n","      \"start_date_flow\": f\"{start_date}\",\n","      \"end_date_flow\": f\"{end_date}\",\n","\n","  }\n","\n","  # Specify the file path where you want to save the JSON file\n","  file_path = os.path.join(config_path, \"3f_hydrometric_data.json\")\n","\n","  if write_to_configuration_file:\n","    # Writing data to the JSON file\n","    with open(file_path, 'w') as json_file:\n","        json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)"]},{"cell_type":"markdown","metadata":{"id":"buWg9LUEIVh-"},"source":["**References**\n","\n","Albers, S. J. (2017). tidyhydat: Extract and tidy Canadian hydrometric data. Journal of Open Source Software, 2(20), 511.\n","\n","Chlumsky, R., Craig, J. R., Lin, S. G., Grass, S., Scantlebury, L., Brown, G., & Arabzadeh, R. (2022). RavenR v2. 1.4: an open-source R package to support flexible hydrologic modelling. Geoscientific Model Development, 15(18), 7017-7030.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pmWXWj_chiEi"},"source":["# **4.0 Raven Input Files**\n","\n","This subsection focuses on generating the remaining Raven inputs (rvi, rvh, and rvc files) through RavenR, developed by Chlumsky et al. (2022). For more information on RavenR, please visit the [RavenR Manual](https://cran.r-project.org/web/packages/RavenR/RavenR.pdf)\n","\n","For the rvp file, BasinMakers <font color=red>.rvp_temp.rvp</font> template is required.\n","\n","Check out the short video [Creating Raven Input Files with RavenR: Magpie Tutorial](https://youtu.be/vA2ozwdblWA) for more information.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"hp_JNZcFVJrH"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"rpy2==3.5.1\",\"requests\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","%load_ext rpy2.ipython\n","\n","import requests\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","\n","def download_and_build_raven(url, temporary_dir, main_dir):\n","    \"\"\"\n","    Download a zip file from the given URL, unzip it, and build Raven.\n","\n","    Parameters:\n","    - url (str): The URL of the zip file.\n","    - temporary_dir (str): The temporary directory to store files.\n","    - main_dir (str): The main directory for the build.\n","\n","    Returns:\n","    - None\n","    \"\"\"\n","    zip_file_path = os.path.join(temporary_dir, 'RavenSource')\n","    zip_file_unpacked = os.path.join(temporary_dir, 'unpacked')\n","    source_dir = os.path.join(temporary_dir, 'unpacked')\n","    build_dir = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'packages')\n","\n","    # Download the zip file\n","    response = requests.get(url, stream=True)\n","    zip_file_path_full = os.path.join(zip_file_path, 'RavenSource_v3.8.zip')\n","    os.makedirs(zip_file_path, exist_ok=True)\n","\n","    with open(zip_file_path_full, 'wb') as zip_file:\n","        shutil.copyfileobj(response.raw, zip_file)\n","\n","    # Check if the download was successful\n","    print(f\"Downloaded {url} to {zip_file_path_full}\") if response.status_code == 200 else print(f\"Failed to download {url} (Status code: {response.status_code})\")\n","\n","    # Unzip the source code\n","    subprocess.run(['unzip', zip_file_path_full, '-d', zip_file_unpacked], check=True)\n","\n","    # Create the build directory and change to it\n","    os.makedirs(build_dir, exist_ok=True)\n","    os.chdir(build_dir)\n","\n","    # Run CMake and Make\n","    subprocess.run(['cmake', source_dir], check=True)\n","    subprocess.run(['make'], check=True)\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"8NE6YvL-yOr4"},"outputs":[],"source":["#@markdown <font color=grey> **Install and Setup Raven Executable** </font>\n","\n","# define output directory path\n","raven_exe_dir = os.path.join(main_dir,'workflow_outputs', 'RavenInput', 'packages')\n","output_path = os.path.isdir(raven_exe_dir)\n","if not output_path:\n","  os.makedirs(raven_exe_dir)\n","  print(\"created  folder: \", raven_exe_dir)\n","else:\n","  print(raven_exe_dir, \"folder already exists\")\n","\n","src = os.path.join(main_dir,'extras', 'netcdf_library')\n","\n","src_files = os.listdir(src)\n","for file_name in src_files:\n","    full_file_name = os.path.join(src, file_name)\n","    if os.path.isfile(full_file_name):\n","        shutil.copy(full_file_name, raven_exe_dir)\n","\n","# Define the file path to check\n","file_path = os.path.join(main_dir, 'workflow_outputs','RavenInput','packages','Raven')\n","\n","# Check if the file exists\n","if not os.path.exists(file_path):\n","    # download Raven executable\n","    url = \"https://raven.uwaterloo.ca/files/v3.8/RavenSource_v3.8.zip\"\n","    download_and_build_raven(url, temporary_dir, main_dir)\n","    remove_temp_data(temporary_dir)\n","else:\n","    os.remove(file_path)\n","    # download Raven executable\n","    url = \"https://raven.uwaterloo.ca/files/v3.8/RavenSource_v3.8.zip\"\n","    download_and_build_raven(url, temporary_dir, main_dir)\n","    remove_temp_data(temporary_dir)\n","\n","print(f\"\\n\\n-----------------------------------------------------------------------\")\n","print(f\"Final output folder previously defined as: {final_output_folder}\")\n","print(f\"-----------------------------------------------------------------------\")\n","\n","# File path for output\n","output_file = \"/content/variable_info.txt\"\n","\n","# Writing to a file\n","with open(output_file, \"w\") as file:\n","    file.write(\"main_dir = '{}'\\n\".format(main_dir))\n","    file.write(\"model_name = '{}'\\n\".format(model_name))\n"]},{"cell_type":"markdown","metadata":{"id":"Cl3P12Oujxbj"},"source":["<font color=#5559AB> **Define model name** </font>\n","\n","Be sure to keep the model naming consistent between the rvi, rvh, rvp, rvc, and rvt files"]},{"cell_type":"markdown","metadata":{"id":"W5pYtutpsr_X"},"source":["<font color=red>**Note</font> - If the folder directory name, \"Magpie_Workflow\", has been changed, the following file paths will need to be adjusted"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"BHFugY88kRSZ"},"outputs":[],"source":["#@markdown <font color=grey> **Load path information** </font>\n","\n","%%R\n","\n","# Define the file path\n","file_path <- \"/content/variable_info.txt\"\n","\n","# Read the file\n","lines <- readLines(file_path)\n","\n","# Initialize variables\n","main_dir <- NULL\n","model_name <- NULL\n","\n","# Extract values from the lines\n","for (line in lines) {\n","  if (grepl(\"main_dir\", line)) {\n","    main_dir <- gsub(\"main_dir = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"model_name\", line)) {\n","    model_name <- gsub(\"model_name = '(.*)'\", \"\\\\1\", line)\n","  }\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"yVdAd2m9NlRa"},"outputs":[],"source":["#@markdown <font color=grey> **Install and Load Related R Packages** </font>\n","\n","#@markdown This will take a couple of minutes, good time for a coffee break ☕\n","\n","%%R\n","\n","library(RavenR, lib=\"/content/google_drive/MyDrive/R_Packages\")"]},{"cell_type":"markdown","metadata":{"id":"BnQ8gfp_gOVC"},"source":["### <font color=grey>**Generate RVI File**</font>"]},{"cell_type":"markdown","metadata":{"id":"QRQjpXlykkDI"},"source":["\n","RavenR provides several templates that can be used; \"UBCWM\", \"HBV-EC\", \"HBV-Light\", \"GR4J\", \"CdnShield\", \"MOHYSE\", \"HMETS\", \"HYPR\", or \"HYMOD\". For more information on the model templates, check out the [Raven Manual](http://raven.uwaterloo.ca/files/v3.6/RavenManual_v3.6.pdf) OR in the cell below change view_rvn_rvi_write_template to \"yes\" and run to get more information"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIjIDwFvCUZS"},"outputs":[],"source":["%%R\n","\n","template_help <- \"no\"\n","\n","# runs funciton, if view_rvn_rvi_write_template value is yes\n","if (template_help == \"yes\") {\n","  help(rvn_rvi_write_template)\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"NdzA5mvYJFuh"},"outputs":[],"source":["#@markdown <font color=grey> **Set working directory** </font>\n","\n","#@markdown check that the set working directory path is correct\n","\n","%%R\n","# Use file.path to join paths\n","target_dir <- file.path(main_dir, \"workflow_outputs/RavenInput\")\n","\n","# Set the working directory\n","setwd(target_dir)\n","getwd()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"E68j6hbIGz73"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define template name** </font><br>\n","\n","#@markdown use drop-down menu to select RavenR RVI template\n","\n","define_template_name = 'CdnShield' #@param [\"CdnShield\", \"UBCWM\", \"HBV-EC\",\"HBV-Light\", \"GR4J\", \"MOHYSE\", \"HMETS\", \"HYPR\", \"HYMOD\"]\n","\n","#@markdown <font color=#5559AB> **Fill in author name and user description** </font><br>\n","\n","author_name = 'Magpie' #@param {type:\"string\"}\n","user_description = 'RVI file for Raven model created by RavenR' #@param {type:\"string\"}\n","\n","# File path for output\n","output_file = \"/content/variable_rvi_info.txt\"\n","\n","# Writing to a file\n","with open(output_file, \"w\") as file:\n","    file.write(\"define_template_name = '{}'\\n\".format(define_template_name))\n","    file.write(\"author_name = '{}'\\n\".format(author_name))\n","    file.write(\"user_description = '{}'\\n\".format(user_description))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"R0WwX06BiXSc"},"outputs":[],"source":["#@markdown <font color=grey> **Generate RVI file** </font>\n","\n","%%R\n","\n","# Define the file path\n","file_path <- \"/content/variable_rvi_info.txt\"\n","\n","# Read the file\n","lines <- readLines(file_path)\n","\n","# Initialize variables\n","define_template_name <- NULL\n","author_name <- NULL\n","user_description <- NULL\n","\n","# Extract values from the lines\n","for (line in lines) {\n","  if (grepl(\"define_template_name\", line)) {\n","    define_template_name <- gsub(\"define_template_name = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"author_name\", line)) {\n","    author_name <- gsub(\"author_name = '(.*)'\", \"\\\\1\", line)\n","  } else if (grepl(\"user_description\", line)) {\n","    user_description <- gsub(\"user_description = '(.*)'\", \"\\\\1\", line)\n","  }\n","}\n","\n","# create an rvi file and template file with Raven\n","rvn_rvi_write_template(template_name=define_template_name,\n","   filename=file.path(getwd(), paste(model_name,\"rvi\", sep=\".\")),\n","   author=author_name,\n","   description=user_description)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_2yV8PUPqfae"},"outputs":[],"source":["#@markdown <font color=grey> **Write Model Decisions to Configuration File**\n","\n","#@markdown To enhance model reproducibility, users can choose to document the model\n","#@markdown decisions made in the Magpie interface by writing them to a configuration\n","#@markdown file. Subsequently, this configuration file can be executed in Magpie\n","#@markdown Developer, facilitating the recreation of the same model with consistent results.\n","\n","# delete rvi variable text file\n","os.remove('/content/variable_rvi_info.txt')\n","\n","write_to_configuration_file = False  # @param {type:\"boolean\"}\n","\n","#@markdown users will need to redefine the following variables as the variables defined in R cannot be easily transferred to Python\n","\n","if write_to_configuration_file:\n","    # Create a dictionary to store configuration data\n","    data = {\n","        \"_comment\": \"------------ 4) RAVEN INPUT ---------------\",\n","\n","        \"define_template_name\": f\"{define_template_name}\",\n","        \"author_name\": f\"{author_name}\",\n","        \"user_description\": f\"{user_description}\",\n","\n","    }\n","\n","    # Specify the file path where you want to save the JSON file\n","    file_path = os.path.join(config_path, \"4_raven_input_files.json\")\n","\n","    # Writing data to the JSON file\n","    with open(file_path, 'w') as json_file:\n","        json.dump(data, json_file, indent=2)  # indent parameter for pretty formatting (optional)"]},{"cell_type":"markdown","metadata":{"id":"1h2Z329WgkUd"},"source":["### <font color=grey>**Generate RVP File**</font>"]},{"cell_type":"markdown","metadata":{"id":"trponIEOkxKR"},"source":["To edit the RVI file, find it in the RavenInput folder, double click and it will open in an editable window.\n","\n","To generate an RVP file template that can be filled in later on, copy and paste **:CreateRVPTemplate** into the RVI file, save, and run the next cell.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"kxqW75pFSXt0"},"outputs":[],"source":["#@markdown ### <font color=#5559AB> **Run Raven**</font>\n","\n","#@markdown run raven to generate RVP template to be filled in with RavenR\n","\n","# define paths\n","exe_path = os.path.join(main_dir,'workflow_outputs','RavenInput','packages','Raven')\n","model_path = os.path.join(main_dir,'workflow_outputs', 'RavenInput', model_name)\n","final_output_path = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', final_output_folder)\n","\n","# define bash command\n","bash_command = f'\"{exe_path}\" \"{model_path}\" -o \"{final_output_path}\"'\n","\n","# Run the Bash command and capture the output\n","result = subprocess.run(bash_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n","\n","# Display the output\n","print(\"Bash Command Output:\")\n","print(result.stdout)\n","\n","# Display the error, if any\n","if result.returncode != 0:\n","    print(\"Error (if any):\")\n","    print(result.stderr)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"fShAmppSgO-t"},"outputs":[],"source":["#@markdown <font color=grey> **Load Pre-Generated RVP Template**\n","\n","#@markdown within the Magpie workflow the \"rvp_temp.rvp\"  can be generated by BasinMaker or users can upload their own template directly to the \"RavenInput\" folder before running the following cell\n","\n","%%R\n","# load pre-generated template file and other model files\n","\n","model_template_file <- file.path(getwd(), paste(model_name,\"rvp_temp.rvp\", sep=\".\"))\n","rvi_file <- file.path(getwd(), paste(model_name,\"rvi\", sep=\".\"))\n","rvh_file <- file.path(getwd(), paste(model_name,\"rvh\", sep=\".\"))\n","rvp_out_file <- file.path(getwd(), paste(model_name,\"rvp\", sep=\".\"))"]},{"cell_type":"markdown","metadata":{"id":"T97IqFCKg3VJ"},"source":["\n","<font color=grey>**Fill in RVP File**\n","\n","users have the option to define the average annual runoff value before running the cell\n","\n","kindly be aware that the values provided are not mandatory; users are encouraged to tailor these parameters according to the specifics of the study area by referring to relevant information"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"1ewk-DU1QQhP"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define average annual runoff** </font><br>\n","\n","avg_annual_runoff = 123 #@param\n","\n","# File path for output\n","output_file = \"/content/variable_rvp_info.txt\"\n","\n","# Writing to a file\n","with open(output_file, \"w\") as file:\n","    file.write(\"avg_annual_runoff = '{}'\\n\".format(avg_annual_runoff))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"_I5JcwLqg09q"},"outputs":[],"source":["#@markdown <font color=grey> **Generate RVP file**\n","\n","%%R\n","\n","# Define the file path\n","file_path <- \"/content/variable_rvp_info.txt\"\n","\n","# Read the file\n","lines <- readLines(file_path)\n","\n","# Initialize variables\n","avg_annual_runoff <- NULL\n","\n","# Extract values from the lines\n","for (line in lines) {\n","  if (grepl(\"avg_annual_runoff\", line)) {\n","    avg_annual_runoff <- gsub(\"avg_annual_runoff = '(.*)'\", \"\\\\1\", line)\n","  }\n","}\n","\n","\n","# rewrite template with parameter values\n","rvn_rvp_fill_template(rvi_file=rvi_file,\n","                      rvh_file=rvh_file,\n","                      rvp_template_file=model_template_file,\n","                      avg_annual_runoff = as.double(avg_annual_runoff),\n","                      extra_commands=\":RedirectToFile  channel_properties.rvp\",\n","                      rvp_out=rvp_out_file)"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"UJTPrbOXg7N3"},"outputs":[],"source":["#@markdown <font color=grey> **Preview of parameter dataframe**\n","\n","%%R\n","\n","rvi_file %>%\n","  rvn_rvi_read() %>%\n","  rvn_rvi_getparams() %>%\n","  head()"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"YjXxYSqfhPMV"},"outputs":[],"source":["#@markdown <font color=grey> **Remove Unnecessary Files**\n","\n","# delete old rvp file\n","os.remove('/content/variable_rvp_info.txt')\n","\n","# delete old rvp file\n","os.remove(os.path.join(main_dir, 'workflow_outputs','RavenInput',f'{model_name}.rvp'))\n","\n","# rename RavenR generated rvp file\n","old_rvp_file = os.path.join(main_dir, 'workflow_outputs','RavenInput',f'{model_name}_ravenr_generated.rvp')\n","new_rvp_file =  os.path.join(main_dir, 'workflow_outputs','RavenInput',f'{model_name}.rvp')\n","\n","os.rename(old_rvp_file, new_rvp_file)\n","\n","# delete old rvp\n","os.remove( os.path.join(main_dir, 'workflow_outputs','RavenInput',f'{model_name}.rvp_temp.rvp'))"]},{"cell_type":"markdown","metadata":{"id":"qi6Mige-h8rm"},"source":["### <font color=grey>**Generate RVC File**</font>\n","\n","This provides a blank RVC file."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"zd-HM0Rah43N"},"outputs":[],"source":["#@markdown <font color=grey> **Create empty RVC file**\n","\n","%%R\n","\n","# Generate RVC file\n","#rvn_rvc_res(initial_percent = 0, output = file.path(getwd(),\"model_name.rvc\"))\n","\n","rvc_file <- file.path(getwd(), paste(model_name,\"rvc\", sep=\".\"))\n","writeLines(c(\"# ----------------------------------------------\",\"# Raven Input file\",\"# ----------------------------------------------\"), rvc_file)"]},{"cell_type":"markdown","metadata":{"id":"BQrb3JrpiB7D"},"source":["**References**\n","\n","Chlumsky, R., Craig, J. R., Lin, S. G., Grass, S., Scantlebury, L., Brown, G., & Arabzadeh, R. (2022). RavenR v2. 1.4: an open-source R package to support flexible hydrologic modelling. Geoscientific Model Development, 15(18), 7017-7030.\n","\n","Craig, J. R., Brown, G., Chlumsky, R., Jenkinson, R. W., Jost, G., Lee, K., ... & Tolson, B. A. (2020). Flexible watershed simulation with the Raven hydrological modelling framework. Environmental Modelling & Software, 129, 104728."]},{"cell_type":"markdown","metadata":{"id":"DzgowN8Ehoj2"},"source":["# **5.0 Run Raven**\n","\n","Raven, a robust and flexible hydrological modelling framework developed by Craig et al. (2020). This fully object-oriented code allows for complete flexibility in spatial discretization, interpolation, process representation, and the generation of forcing functions. Raven models range from a single watershed lumped model with only a few state variables to a full semi-distributed system model with physically-based infiltration, snowmelt, and routing.\n","\n","Check out the short video [Running Raven Model with Magpie Workflow](https://youtu.be/Rp3aAQjmXSA) for more information.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"5NpC341wzTtq"},"outputs":[],"source":["#@markdown <font color=grey> **Download Raven Input Files** </font>\n","\n","#@markdown This cell zips the Raven input files so that users can then download the zipped file and run the Raven model locally\n","\n","download_Raven_input_files = True #@param {type:\"boolean\"}\n","\n","if download_Raven_input_files == True:\n","  # zip folder\n","  directory_name = os.path.join(main_dir, 'Raven_model_inputs')\n","  zip_name = os.path.join(main_dir, 'workflow_outputs', 'RavenInput')\n","  print('Folder zipped')\n","\n","  # Create 'path\\to\\zip_file.zip'\n","  shutil.make_archive(directory_name, 'zip', zip_name)"]},{"cell_type":"markdown","metadata":{"id":"kIDbp1KcOOnb"},"source":["**Here is a quick overview of the required input files for Raven:**\n","\n","<font color=#5559AB> .rvi </font> - the primary model input file <font color=grey> (section 5.0 Raven Input Files) </font>\n","\n","<font color=#5559AB> .rvh </font> - the HRU / basin definition file <font color=grey> (section 3.0 Discretize Basin and 5.0 Raven Input Files) </font>\n","\n","<font color=#5559AB> .rvt </font> - the time series/forcing function file <font color=grey> (section 4.0 Forcing Data) </font>\n","\n","<font color=#5559AB> .rvp </font> - the class parameters file <font color=grey> (section 3.0 Discretize Basin) </font>\n","\n","<font color=#5559AB> .rvc </font> - the initial conditions file <font color=grey> (section 5.0 Raven Input Files) </font>\n","\n","\n","**Optional Raven Inputs:**\n","\n","A map of the final HRUs <font color=grey> (section 3.0 Discretize Basin) </font>\n","\n","Grid Weights for NetCDF data <font color=grey> (section 4.0 Forcing Data --> 4.3 Gridded Weights Generator) </font>\n","\n","More information about the files can be found in the [Raven Manual](http://raven.uwaterloo.ca/files/v3.6/RavenManual_v3.6.pdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXhVjhWHgBQ3","cellView":"form"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"requests\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","#@markdown <font color=#C41E3A> **Load Functions** </font> <br>\n","#@markdown Loading functions in Python involves loading them into your script or notebook using, making the functions available for use. </font> <br>\n","\n","import requests\n","\n","def download_and_build_raven(url, temporary_dir, main_dir):\n","    \"\"\"\n","    Download a zip file from the given URL, unzip it, and build Raven.\n","\n","    Parameters:\n","    - url (str): The URL of the zip file.\n","    - temporary_dir (str): The temporary directory to store files.\n","    - main_dir (str): The main directory for the build.\n","\n","    Returns:\n","    - None\n","    \"\"\"\n","    zip_file_path = os.path.join(temporary_dir, 'RavenSource')\n","    zip_file_unpacked = os.path.join(temporary_dir, 'unpacked')\n","    source_dir = os.path.join(temporary_dir, 'unpacked')\n","    build_dir = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', 'packages')\n","\n","    # Download the zip file\n","    response = requests.get(url, stream=True)\n","    zip_file_path_full = os.path.join(zip_file_path, 'RavenSource_v3.8.zip')\n","    os.makedirs(zip_file_path, exist_ok=True)\n","\n","    with open(zip_file_path_full, 'wb') as zip_file:\n","        shutil.copyfileobj(response.raw, zip_file)\n","\n","    # Check if the download was successful\n","    print(f\"Downloaded {url} to {zip_file_path_full}\") if response.status_code == 200 else print(f\"Failed to download {url} (Status code: {response.status_code})\")\n","\n","    # Unzip the source code\n","    subprocess.run(['unzip', zip_file_path_full, '-d', zip_file_unpacked], check=True)\n","\n","    # Create the build directory and change to it\n","    os.makedirs(build_dir, exist_ok=True)\n","    os.chdir(build_dir)\n","\n","    # Run CMake and Make\n","    subprocess.run(['cmake', source_dir], check=True)\n","    subprocess.run(['make'], check=True)\n","\n","\n","def remove_temp_data(temporary_dir):\n","  if os.path.exists(os.path.join(temporary_dir)):\n","    shutil.rmtree(os.path.join(temporary_dir))"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"-GZwRtkGgOR1"},"outputs":[],"source":["#@markdown <font color=grey> **Load Raven Executable** </font>\n","\n","# Define the file path to check\n","file_path = os.path.join(main_dir, 'workflow_outputs','RavenInput','packages','Raven')\n","\n","# Check if the file exists\n","if not os.path.exists(file_path):\n","    # download Raven executable\n","    url = \"https://raven.uwaterloo.ca/files/v3.8/RavenSource_v3.8.zip\"\n","    download_and_build_raven(url, temporary_dir, main_dir)\n","    remove_temp_data(temporary_dir)\n","else:\n","    os.remove(file_path)\n","    # download Raven executable\n","    url = \"https://raven.uwaterloo.ca/files/v3.8/RavenSource_v3.8.zip\"\n","    download_and_build_raven(url, temporary_dir, main_dir)\n","    remove_temp_data(temporary_dir)\n","\n","print(f\"\\n\\n-----------------------------------------------------------------------\")\n","print(f\"Final output folder previously defined as: {final_output_folder}\")\n","print(f\"-----------------------------------------------------------------------\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"61DYd2-qjqlF"},"outputs":[],"source":["#@markdown **Optional: change output foler name**\n","\n","#@markdown users have the option to change the name of their Raven output folder here\n","\n","final_output_folder = 'outputA' #@param {type:\"string\"}\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"6l2KM6-vm12z"},"outputs":[],"source":["#@markdown <font color=grey> **Run Raven** </font>\n","\n","# define paths\n","exe_path = os.path.join(main_dir,'workflow_outputs','RavenInput','packages','Raven')\n","model_path = os.path.join(main_dir,'workflow_outputs', 'RavenInput', model_name)\n","final_output_path = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', final_output_folder)\n","\n","# define bash command\n","bash_command = f'\"{exe_path}\" \"{model_path}\" -o \"{final_output_path}\"'\n","\n","# Run the Bash command and capture the output\n","result = subprocess.run(bash_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n","\n","# Display the output\n","print(\"Bash Command Output:\")\n","print(result.stdout)\n","\n","# Display the error, if any\n","if result.returncode != 0:\n","    print(\"Error (if any):\")\n","    print(result.stderr)"]},{"cell_type":"markdown","metadata":{"id":"DNwOw0rgd_-G"},"source":["**References**\n","\n","Craig, J. R., Brown, G., Chlumsky, R., Jenkinson, R. W., Jost, G., Lee, K., ... & Tolson, B. A. (2020). Flexible watershed simulation with the Raven hydrological modelling framework. Environmental Modelling & Software, 129, 104728."]},{"cell_type":"markdown","metadata":{"id":"M4iwUZ7cSgz0"},"source":["#**6.0 Raven Visualization**\n","\n","The Raven model outputs are formatted and visualized in this subsection. Please keep in mind that the data visualization options are dependent on the Raven model outputs and may need to be customized by the user.\n","\n","Check out the short video [7.0 Visualization of Raven Model Outputs - Magpie Workflow](https://youtu.be/NgbmsRu6Fm0) for more information."]},{"cell_type":"markdown","metadata":{"id":"SP6sRXHBrSEQ"},"source":["###<font color=grey> **HydroGlyph**\n","\n","An online Time Series Visualizer for Raven Hydroglyph which displays CSV output files from the Raven Hydrological Modelling Framework developed by the Raven Development Team at the University of Waterloo.\n","\n","Hydroglyph was developed by Leland Scantlebury."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"53f3UrbvqW-q"},"outputs":[],"source":["# check libraries\n","libraries_to_check = [\"requests\", \"zipfile\"]\n","check_and_install_libraries(libraries_to_check)\n","\n","import zipfile\n","import requests\n","\n","#@markdown <font color=#5559AB> **Define Raven output folder name** </font>\n","\n","final_output_folder = \"outputA\" #@param {type:\"string\"}\n","\n","#@markdown <font color=grey> **Zip and download HydroGlyph outputs** </font>\n","\n","#@markdown Right click on the zipped folder and select \"Download\" to save to your local computer.\n","\n","#@markdown Once downloaded, visit [HydroGlyph](http://raven.uwaterloo.ca/hydroglyph/) to visualize outputs\n","\n","\n","# define directory\n","zip_dir = os.path.join(temporary_dir)\n","os.makedirs(zip_dir, exist_ok=True)\n","\n","def zip_and_download(file_path, zip_name):\n","    if os.path.exists(file_path):\n","        # Zip the specified file\n","        with zipfile.ZipFile(zip_name, 'w') as zipf:\n","            zipf.write(file_path, os.path.basename(file_path))\n","        print(f\"Zipped and downloaded {file_path} to {zip_name}\")\n","    else:\n","        print(f'The file {file_path} does not exist.')\n","\n","# Example usage\n","folder_path = os.path.join(main_dir,'workflow_outputs','RavenInput',final_output_folder)\n","file_to_zip = \"Hydrographs.csv\"  # Replace with the actual file name\n","zip_file_name = os.path.join(\"HydroGlyph_files.zip\")\n","\n","# Construct the full path to the file\n","file_path = os.path.join(folder_path, file_to_zip)\n","\n","# Call the function to zip and download\n","zip_and_download(file_path, zip_file_name)"]},{"cell_type":"markdown","metadata":{"id":"BP1KKiYnemF5"},"source":["###<font color=grey> **Visualize Hydrographs with Magpie**"]},{"cell_type":"code","source":["import plotly.express as px\n","\n","#@markdown <font color=grey> **Check output folder path**\n","\n","print('Output folder name: ', final_output_folder)\n","print('Full output folder path: ', os.path.join(main_dir, 'workflow_outputs', 'RavenInput', final_output_folder))\n","\n","user_input = input(\"Replace output folder path? (yes/no): \").strip().lower()\n","if user_input == 'yes':\n","  new_path = input(\"Please enter the new output folder path: \").strip()\n","  final_output_folder = new_path\n","  print('Updated full output folder path: ', final_output_folder)\n","\n"],"metadata":{"cellView":"form","id":"phW1-9_K4AGh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Hydrograph**"],"metadata":{"id":"XRJPB0Fb6z1I"}},{"cell_type":"code","source":["#@markdown <font color=#5559AB> **Define hydrograph name** </font>\n","\n","# File path\n","hydrograph_filename = 'Hydrographs.csv' # @param {type:\"string\"}\n","\n","hydro_file = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', final_output_folder, hydrograph_filename)\n","\n","# Load the CSV file\n","hydrograph_vals = pd.read_csv(hydro_file)\n","display(hydrograph_vals.head())\n","\n","print('------------------------------------------------------------------------------------')\n","\n","print(\"Column Names: \", hydrograph_vals.columns.tolist())"],"metadata":{"id":"Psq4-Wp5uyxO","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown <font color=#5559AB> **Define simulated and observed column names** </font>\n","\n","simulated_column = \"sub26007397 (res. inflow) [m3/s]\" # @param {type:\"string\"}\n","\n","observed_column = \"sub26007397 (res. inflow) [m3/s]\" # @param {type:\"string\"}\n","\n","# Convert the \"date\" column to datetime\n","hydrograph_vals['date'] = pd.to_datetime(hydrograph_vals['date'])\n","\n","# Ensure the selected columns are numeric\n","columns_to_plot = [simulated_column, observed_column, 'precip [mm/day]']\n","for col in columns_to_plot:\n","    hydrograph_vals[col] = pd.to_numeric(hydrograph_vals[col], errors='coerce')\n","\n","# Create an interactive line plot with a custom color for precipitation\n","fig = px.line(\n","    hydrograph_vals,\n","    x='date',\n","    y=columns_to_plot,\n","    labels={\n","        \"value\": \"Values\",\n","        \"date\": \"Date\",\n","        \"variable\": \"Legend\"\n","    },\n","    title=\"Hydrograph: Precipitation and Discharge Over Time\",\n","    color_discrete_map={\n","        simulated_column: 'red',\n","        observed_column: 'blue',\n","        'precip [mm/day]': 'grey'  # Custom color for precipitation\n","    }\n",")\n","\n","# Show the plot\n","fig.show()\n"],"metadata":{"cellView":"form","id":"k5_dOPie2WNV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Forcing Function Outputs**\n","\n","> add *:WriteForcingFunctions* to RVI file, rerun to generate file"],"metadata":{"id":"ZEryPA0pvyJ-"}},{"cell_type":"code","source":["#@markdown <font color=#5559AB> **Define forcing function file name** </font>\n","\n","# File path\n","forcing_functions_filename = 'ForcingFunctions.csv' # @param {type:\"string\"}\n","\n","forcing_file = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', final_output_folder, forcing_functions_filename)\n","\n","# Read the CSV file into a pandas DataFrame\n","forcings = pd.read_csv(forcing_file)\n","\n","display(forcings.head())\n","\n","print('------------------------------------------------------------------------------------')\n","\n","print(\"Column Names: \", forcings.columns.tolist())"],"metadata":{"id":"h0lrVDTNunDu","cellView":"form"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown <font color=#5559AB> **Define forcing variables to compare** </font>\n","\n","col_val1 = \" temp_daily_min [C]\" # @param {type:\"string\"}\n","\n","col_val2 = \" temp_daily_max [C]\" # @param {type:\"string\"}\n","\n","\n","# Ensure the \"date\" column is in Date format\n","forcings['date'] = pd.to_datetime(forcings[' date'])\n","\n","# Create an interactive line chart using Plotly Express\n","fig = px.line(\n","    data_frame=forcings,\n","    x='date',\n","    y=[col_val1, col_val2],\n","    labels={'value': 'Value', 'date': 'Date', 'variable': 'Legend'},\n","    title=\"Daily Minimum and Maximum Temperatures\",\n",")\n","\n","# Customize the line colors\n","fig.update_traces(line=dict(width=2))\n","fig.update_layout(\n","    legend=dict(title=\"Legend\", orientation=\"h\", x=0.5, xanchor=\"center\"),\n","    title=dict(x=0.5),\n",")\n","\n","# Display the interactive plot\n","fig.show()"],"metadata":{"cellView":"form","id":"BqaDO_by7YtT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### **Watershed Storage Outputs**"],"metadata":{"id":"LJMrcXR98FKU"}},{"cell_type":"code","source":["#@markdown <font color=#5559AB> **Define forcing function file name** </font>\n","\n","# File path\n","watershed_filename = 'WatershedStorage.csv' # @param {type:\"string\"}\n","\n","wshed_file = os.path.join(main_dir, 'workflow_outputs', 'RavenInput', final_output_folder, watershed_filename)\n","\n","# Read the CSV file into a pandas DataFrame\n","wshed = pd.read_csv(wshed_file)\n","\n","display(wshed.head())\n","\n","print('------------------------------------------------------------------------------------')\n","\n","print(\"Column Names: \", wshed.columns.tolist())"],"metadata":{"cellView":"form","id":"iKi6csh0kITG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@markdown <font color=#5559AB> **Define forcing variables to compare** </font>\n","\n","col_of_interest = 'Snow [mm]' # @param {type:\"string\"}\n","\n","fig = px.line(\n","    wshed,\n","    y=col_of_interest,\n","    labels={'value': 'value', 'index': 'Time'},\n","    template='plotly'\n",")\n","\n","# Show the interactive plot\n","fig.show()"],"metadata":{"cellView":"form","id":"KKVoLGXZmClk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z2sHez4HNV7X"},"source":["###<font color=#5559AB> **RavenView**\n","\n","RavenView is an online visualization tool, developed by Craig (2022), for Raven models. This subsection generates a downloadable folder of Raven inputs and outputs that can be dragged-and-dropped into RavenView to examine the model further.\n","\n","RavenView is not only helpful in visualizing Raven model outputs but can be used beforehand to visualize Raven model input files (RVH, RVP)\n","\n","Check out the short video [8.0 RavenView - Magpie Workflow](https://youtu.be/2AltLpOWe1M) for more information."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"u00VAS-WmGGN"},"outputs":[],"source":["#@markdown <font color=#5559AB> **Define Raven output folder name** </font>\n","\n","output_name = \"outputA\" #@param {type:\"string\"}"]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"lEx4EyXWhI0w"},"outputs":[],"source":["#@markdown <font color=grey> **Zip and download Raven model outputs** </font>\n","\n","#@markdown Right click on the zipped folder and select \"Download\" to save to your local computer.\n","\n","#@markdown Once downloaded, visit [RavenView](http://raven.uwaterloo.ca/RavenView/RavenView.html) to visualize outputs\n","\n","\n","# Define source and destination directories\n","ravenview_output_name = output_name+'_RavenView'\n","src_dir = os.path.join(main_dir, 'workflow_outputs', 'RavenInput')\n","dst_dir = os.path.join(src_dir, ravenview_output_name)\n","\n","# Create the destination directory if it doesn't exist\n","os.makedirs(dst_dir, exist_ok=True)\n","\n","# Copy and move HRU file\n","src_files = glob(os.path.join(src_dir, 'maps', '*.geojson'))\n","if src_files:\n","    dst_file = os.path.join(dst_dir, 'hru_map.geojson')\n","    shutil.copyfile(src_files[0], dst_file)\n","\n","# Define a list of file extensions to copy and move\n","file_ext_lst = ('rvh', 'rvt', 'rvc', 'rvp', 'rvi')\n","\n","# Copy and move files with specified extensions\n","for f in file_ext_lst:\n","    src_files = glob(os.path.join(src_dir, f\"{model_name}.{f}\"))\n","    for src_file in src_files:\n","        dst_file = os.path.join(dst_dir, os.path.basename(src_file))\n","        shutil.copyfile(src_file, dst_file)\n","\n","# Copy and move subbasin file\n","subbasin_files = glob(os.path.join(main_dir, 'workflow_outputs', 'routing_product', 'finalcat_info_*.shp'))\n","if subbasin_files:\n","    subbasin_file = subbasin_files[0]\n","    subbasin_polygon = gpd.read_file(subbasin_file)\n","    subbasin_polygon.to_file(os.path.join(dst_dir, 'subbasin_map.geojson'), driver='GeoJSON')\n","\n","# Copy and move river map\n","river_files = glob(os.path.join(main_dir, 'workflow_outputs', 'routing_product', 'finalcat_info_riv_*.shp'))\n","if river_files:\n","    river_file = river_files[0]\n","    river_polygon = gpd.read_file(river_file)\n","    river_polygon.to_file(os.path.join(dst_dir, 'river_map.geojson'), driver='GeoJSON')\n","\n","# Zip the folder\n","shutil.make_archive(dst_dir, 'zip', dst_dir)\n","print('Folder zipped:', dst_dir + '.zip')\n"]},{"cell_type":"markdown","metadata":{"id":"pDz_IZUvEdt8"},"source":["#**7.0 Magpie Developer - Configuration Files**\n","\n","To enhance the reproducibility of the Magpie Workflow, users can use this section to complie all the configuration files generated during the workflow. The final configuration file can then be uploaded into Magpie Developer, allowing them to fully replicate their model.\n","\n","Magpie Developer is a Jupyter notebook that can operate within Google Colab, be executed locally, or run on an alternative server, as long as the necessary packages are properly installed. Moreover, Magpie Developer streamlines the model development process by eliminating the need for a point-and-click interface; instead, all model decisions are retrieved from the configuration file.\n","\n","If you're interested in trying out Magpie Developer for yourself, please visit Raven Utilities for more information."]},{"cell_type":"code","execution_count":null,"metadata":{"cellView":"form","id":"wkzm10AjEc76"},"outputs":[],"source":["#@markdown <font color=grey> **Merge Existing Configuration Files into One for Magpie Developer** </font>\n","\n","# 1. Sort JSON files in a specific folder by name\n","folder_path = os.path.join(main_dir, \"configuration_files\")\n","sorted_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.json')])\n","\n","# 2. Combine JSON files into one and save it to a specific location\n","combined_data = []\n","\n","for file_name in sorted_files:\n","    file_path = os.path.join(folder_path, file_name)\n","    with open(file_path, 'r') as file:\n","        json_data = json.load(file)\n","        combined_data.append(json_data)\n","\n","# Specify the location where you want to save the combined JSON file\n","output_file_path = '/content/configuration_file.json'\n","\n","with open(output_file_path, 'w') as output_file:\n","    json.dump(combined_data, output_file, indent=4)\n","\n","print(f'Combined JSON file saved to {output_file_path}')\n"]}],"metadata":{"colab":{"collapsed_sections":["XRLIfXS0JCmR","s7T7RQ9KMFmy","vC3vND72tm89","hcJK2E-dbkbC","PR63RNeUuiFu","x2fj5T3ZvyKp","5Kxs2qQ-2CFV","TKy81j-k2COG","_mO8mnrG2CUc","q5-PqtpEJX9j","YB9lnpYkvyVk","lTN57xbsSYu7","CGNwp4hc8GbL","-IB0C9cpaoGC","k8-TLJfja_9j","cTrn3D4maSQB","LsWa49EQdWEI","ASk0UnaqqUwE","YyVqr8g2WxFy","_vokMNBydCjm","s6xO_kvDhSjn","jPCK2juUG1tP","yc11UpIQ0oL2","dVIW4K3v0ohT","ddqQn6b1SOU8","JvmQG8S2D8Hg","8rb198U7hOC4","Q1ShqGk8eh2k","vANU4a9PH8B1","SQNKEE60WwYq","lEzzcRumhD4E","bVEZtNvBWepv","UR7hlhCHWetb","Iz-31jqhgyRV","41ytbQP-lEDa","iCAfcvarlPA5","nKvQuv0-lRhK","cvkBInUglhCQ","5wXu6zxQFjgj","Zp6jN05MloQ0","ag21BG_llvv8","oZCkPMOqkQeT","mYN9N5_scD8a","b7VUtWEWW5Y1","deJ3_kTzrP_6","Ullydtkc3DP-","6Gw9Ceoo7idq","T8yvPTxPr2mB","QlxKTaYO3QpN","HhBcwpRwpkYi","6oETbxup13pe","uIfM9eWvvqi6","--y7kezUGgXk","takgaE7gubg7","7QrfZEcv4oOV","uGPmVyCjMjXX","i_qewtTdzcRq","BAaBzcorU9kx","MJvAf5pkWUQU","EHHk1xjChbrz","pmWXWj_chiEi","BnQ8gfp_gOVC","1h2Z329WgkUd","qi6Mige-h8rm","DzgowN8Ehoj2","M4iwUZ7cSgz0","SP6sRXHBrSEQ","BP1KKiYnemF5","XRJPB0Fb6z1I","ZEryPA0pvyJ-","Z2sHez4HNV7X","pDz_IZUvEdt8"],"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}